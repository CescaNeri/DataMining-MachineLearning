const local_index = {"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Data Mining and Machine Learning Modules: Data Mining : Prof. Golfarelli - 36h introduction to data mining Knowledge discovery process Understanding and preparing data Data mining techniques Data understanding and validation Weka software Case studies analysis Machine Learning : Prof. Guido Borghi - 18h Introduction to AI Machine Learning and Deep Learning Data acquisition and Processing Model Training Metrics LIBRARIES: Scikit-learn (ML) Tensofrflow (DL) Assessment Method - EXAM The exam consists in an oral exam on all the subjects (80%) of the course and an elaborate (20% - agreed with the teacher). The elaborate must be carried out in the Machine Learning module, choosing between: - Study and algorithm among those in the literature - Analysis of a data set with mining techniques There are no fixed dates for the exam, it can be defined with teachers along the whole academic year. The two modules must be discussed within 15 days .","title":"Data Mining and Machine Learning"},{"location":"index.html#data-mining-and-machine-learning","text":"","title":"Data Mining and Machine Learning"},{"location":"index.html#modules","text":"Data Mining : Prof. Golfarelli - 36h introduction to data mining Knowledge discovery process Understanding and preparing data Data mining techniques Data understanding and validation Weka software Case studies analysis Machine Learning : Prof. Guido Borghi - 18h Introduction to AI Machine Learning and Deep Learning Data acquisition and Processing Model Training Metrics LIBRARIES: Scikit-learn (ML) Tensofrflow (DL)","title":"Modules:"},{"location":"index.html#assessment-method-exam","text":"The exam consists in an oral exam on all the subjects (80%) of the course and an elaborate (20% - agreed with the teacher). The elaborate must be carried out in the Machine Learning module, choosing between: - Study and algorithm among those in the literature - Analysis of a data set with mining techniques There are no fixed dates for the exam, it can be defined with teachers along the whole academic year. The two modules must be discussed within 15 days .","title":"Assessment Method - EXAM"},{"location":"SUMMARY.html","text":"Data Mining Data Understanding Decision Tree Rule-Based Classifier Machine Learning","title":"SUMMARY"},{"location":"data-mining/data-understanding/SUMMARY.html","text":"Data Understanding","title":"SUMMARY"},{"location":"data-mining/data-understanding/data-understanding.html","text":"Data Understanding & Preparation In data mining, data are composed of collections of objects described by a set of attributes (we refer to data that can be stored in a database). Attribute: property characteristic of an object Attribute types In order to perform meaningful analysis, the characteristics of the attributes must be known. The attribute type tells us what properties are reflected in the value we use as a measure. We can identify 4 types of attributes: Nominal -qualitative: different names of value (gender, zip code, ID) Ordinal -qualitative: values enables us to sort objects based on the value of attribute (grade) Interval -quantitative: the difference between the values has a meaning, with a unit of measurement (dates, temperature) Ratio -quantitative: the ratio of values has meaning (age, length, amount of money) Further classifications Binary, discrete and continuous Discrete: finite number of infinite countable set of values Continuous: real values Nominal and ordinal are typically discrete or binary, while interval and ratio attributes are continuous Asymmetric attribute: only instances that take non-zero values are relevant Documents and Texts: objects of the analysis described by a vector of terms Transactions Each record involves multiple items Items come from a finite set The number of items may vary from transaction to transaction Ordered data Explorative Analysis First step in business and ata understanding. It refers to the preliminary analysis of the data aimed at identify its main characteristics. It helps you choose the best tool for processing and analysis STATISTICS OVERVIEW Frequency The frequency of an attribute value is the percentage of times that value appears in the data set. Mode The mode of an attribute is the value that appears most frequently in the data set. Percentile Given an ordinal or continuous attribute x and a number p between 0 and 100, the p-th percentile is the value of xp of x such that p% of the observed values for x are lower than xp. Percentile visualization through boxplot enables the representation of a distribution of data. It can be used to compare multiple distributions when they have homogeneous magnitude. Mean The mean is the most common measure for locating a set of points. Subject to outliers It is preferred to use tee median or a 'controlled' mean Median The median is the term occupying the central place if the terms are odd; if the terms are even, the median is the arithmetic mean of the two central terms. Range Range is the difference between the minimum and maximum values taken by the attribute. Variance and Standard Deviation Variance and SD are the most common measures of dispersion of a data set. Sensitive to outliers since they are quadratically related to the concept of mean Data Quality The quality of the datasets profoundly affects the chances of finding meaningful patterns. The most frequent problems that deteriorate data quality are: Noise and outliers (objects with characteristics very different from all other objects in the data set) Missing values (not collecting the data is different from when the attribute is not applicable), how to handle them: Delete the objects that contain them Ignore missing values during analysis Manually/automatically fill the missing values ML can be applied to fill the missing values by inferring the other values of that attribute and calculate the most appropriate value Duplicated values (it may be necessary to introduce a data cleaning step in order to identify and eliminate redundancy) Dataset Preprocessing Rarely the dataset has the optimal characteristics to be best processed by machine learning algorithms. It is therefore necessary to put in place a series of actions to enable the algorithms of interest to function: Aggregation: combine two or more attributes into one attribute Sampling: main technique to select data Collecting and processing the entire dataset is too expensive and time consuming Simple Random Sampling (same probability of selecting each element) Stratified sampling (divides the data into multiple partitions and use simple random sampling on each partition) Before sampling a partitioning rule is applied (we inject knowledge about the domain) Allow the population to be balanced However, we are applying a distortion Sampling Cardinality: after choosing the sampling mode, it is necessary to fix the sample size in order to limit the loss of information Dimensionality reduction: the goal is to avoid the 'curse of dimensionality', reduce the amount of time and memory used by ML algorithms, simplify data visualization and eliminate irrelevant attributes and eliminate noise on data. Curse of dimensionality: as dimensionality increases, the data become progressively more sparse. Many clustering and classification algorithms deal with dimensionality and distances. All the elements become equi-distant from one another; the idea of selecting the right dimension to carry out analysis is crucial. The curve indicates that the more we increase the number of dimensionality, the smaller the ratio is. In the modeling phase, it is important reduce dimensionality. The goal is to reduce dimensionality and carry out analysis with the highest information amount. Principal Component Analysis: it is a projection method that transforms objects belonging to a p-dimensional space into a k-dimensional space in such way as to preserve maximum information in the initial dimension. Attribute creation: it is a way to reduce the dimensionality of data. The selection usually aims to eliminate redundant. We can use different attribute selection techniques: Exhaustive approaches Non-exhaustive approaches Feature engineering (create new features): we have raw data and we can extract useful KPIs by designing new attributes. Discretization and binarization: transformation of continuos-valued attributes to discrete-valued attributes. Discretization techniques can be unsupervised (do not exploit knowledge about the class to which elements belong) or supervised (exploit knowledge about the class to which the elements belong). Unsupervised: equi-width, equi-frequency, K-means Supervised: discretization intervals are positioned to maximize the 'purity' of the intervals Entropy and Information Gain: it is the measure of uncertainty about the outcome of an experiment that can be modeled by a random variable x. The entropy of a certain event is zero. The entropy of a discretization into n intervals depends on how pure each group. Binarization: we start with a discrete attribute but we need it to be binary. Attribute transformation: function that maps the entire set of values of an attribute to a new set such that each value in the starting set corresponds to a unique value in the ending set. Similarity and Dissimilarity These two concepts are central in Machine Learning, as it is important to group clusters based on similarity and dissimilarity. Some techniques are stronger with long distances while sometimes, by setting the wrong distance, we will incur in problems. Similarity: it is a numerical measure expressing the degree of similarity between two objects Takes values in the range [0, 1] Dissimilarity (distance): it is a numerical measure expressing the degree of difference between two objects Takes values in the range [0, 1] or [0, \u221e] Distance Distance Properties Given two objects p and q and a dissimilarity measure d(): d(p,q) = 0 only if p=q d(p,q) = d(q,p) -> Symmetry d(p,r) + d(p,q) + d(q,r) -> Triangular inequality Similarity Properties Given two objects p and q and a similarity measure s(): s(p,q) = 1 only if p=q s(p,q) = s(q,p) -> Symmetry Binary Vector Similarities It is common for attributes describing an object to contain only binary values. M01 = the number of attributes where p=0 and q=1 M10 = the number of attributes where p=1 and q=0 M00 = the number of attributes where p=0 and q=0 M11 = the number of attributes where p=1 and q=1 Cosine Similarity Like Jaccard's index. it does not consider 00 matches, but also allows non-binary vectors to be operated on. Similarity with Heterogeneous Attributes In the presence of heterogeneous attributes, it is necessary to compute the similarities separately and then combine them so that their result belongs to the range [0, 1] Correlation The correlation between pairs of objects described by attributes (binary or continuous) is a measure of the existence of a linear relationship between its attributes.","title":"Data Understanding"},{"location":"data-mining/data-understanding/data-understanding.html#data-understanding-preparation","text":"In data mining, data are composed of collections of objects described by a set of attributes (we refer to data that can be stored in a database). Attribute: property characteristic of an object","title":"Data Understanding &amp; Preparation"},{"location":"data-mining/data-understanding/data-understanding.html#attribute-types","text":"In order to perform meaningful analysis, the characteristics of the attributes must be known. The attribute type tells us what properties are reflected in the value we use as a measure. We can identify 4 types of attributes: Nominal -qualitative: different names of value (gender, zip code, ID) Ordinal -qualitative: values enables us to sort objects based on the value of attribute (grade) Interval -quantitative: the difference between the values has a meaning, with a unit of measurement (dates, temperature) Ratio -quantitative: the ratio of values has meaning (age, length, amount of money)","title":"Attribute types"},{"location":"data-mining/data-understanding/data-understanding.html#further-classifications","text":"Binary, discrete and continuous Discrete: finite number of infinite countable set of values Continuous: real values Nominal and ordinal are typically discrete or binary, while interval and ratio attributes are continuous Asymmetric attribute: only instances that take non-zero values are relevant Documents and Texts: objects of the analysis described by a vector of terms Transactions Each record involves multiple items Items come from a finite set The number of items may vary from transaction to transaction Ordered data","title":"Further classifications"},{"location":"data-mining/data-understanding/data-understanding.html#explorative-analysis","text":"First step in business and ata understanding. It refers to the preliminary analysis of the data aimed at identify its main characteristics. It helps you choose the best tool for processing and analysis","title":"Explorative Analysis"},{"location":"data-mining/data-understanding/data-understanding.html#statistics-overview","text":"","title":"STATISTICS OVERVIEW"},{"location":"data-mining/data-understanding/data-understanding.html#frequency","text":"The frequency of an attribute value is the percentage of times that value appears in the data set.","title":"Frequency"},{"location":"data-mining/data-understanding/data-understanding.html#mode","text":"The mode of an attribute is the value that appears most frequently in the data set.","title":"Mode"},{"location":"data-mining/data-understanding/data-understanding.html#percentile","text":"Given an ordinal or continuous attribute x and a number p between 0 and 100, the p-th percentile is the value of xp of x such that p% of the observed values for x are lower than xp. Percentile visualization through boxplot enables the representation of a distribution of data. It can be used to compare multiple distributions when they have homogeneous magnitude.","title":"Percentile"},{"location":"data-mining/data-understanding/data-understanding.html#mean","text":"The mean is the most common measure for locating a set of points. Subject to outliers It is preferred to use tee median or a 'controlled' mean","title":"Mean"},{"location":"data-mining/data-understanding/data-understanding.html#median","text":"The median is the term occupying the central place if the terms are odd; if the terms are even, the median is the arithmetic mean of the two central terms.","title":"Median"},{"location":"data-mining/data-understanding/data-understanding.html#range","text":"Range is the difference between the minimum and maximum values taken by the attribute.","title":"Range"},{"location":"data-mining/data-understanding/data-understanding.html#variance-and-standard-deviation","text":"Variance and SD are the most common measures of dispersion of a data set. Sensitive to outliers since they are quadratically related to the concept of mean","title":"Variance and Standard Deviation"},{"location":"data-mining/data-understanding/data-understanding.html#data-quality","text":"The quality of the datasets profoundly affects the chances of finding meaningful patterns. The most frequent problems that deteriorate data quality are: Noise and outliers (objects with characteristics very different from all other objects in the data set) Missing values (not collecting the data is different from when the attribute is not applicable), how to handle them: Delete the objects that contain them Ignore missing values during analysis Manually/automatically fill the missing values ML can be applied to fill the missing values by inferring the other values of that attribute and calculate the most appropriate value Duplicated values (it may be necessary to introduce a data cleaning step in order to identify and eliminate redundancy)","title":"Data Quality"},{"location":"data-mining/data-understanding/data-understanding.html#dataset-preprocessing","text":"Rarely the dataset has the optimal characteristics to be best processed by machine learning algorithms. It is therefore necessary to put in place a series of actions to enable the algorithms of interest to function: Aggregation: combine two or more attributes into one attribute Sampling: main technique to select data Collecting and processing the entire dataset is too expensive and time consuming Simple Random Sampling (same probability of selecting each element) Stratified sampling (divides the data into multiple partitions and use simple random sampling on each partition) Before sampling a partitioning rule is applied (we inject knowledge about the domain) Allow the population to be balanced However, we are applying a distortion Sampling Cardinality: after choosing the sampling mode, it is necessary to fix the sample size in order to limit the loss of information Dimensionality reduction: the goal is to avoid the 'curse of dimensionality', reduce the amount of time and memory used by ML algorithms, simplify data visualization and eliminate irrelevant attributes and eliminate noise on data. Curse of dimensionality: as dimensionality increases, the data become progressively more sparse. Many clustering and classification algorithms deal with dimensionality and distances. All the elements become equi-distant from one another; the idea of selecting the right dimension to carry out analysis is crucial. The curve indicates that the more we increase the number of dimensionality, the smaller the ratio is. In the modeling phase, it is important reduce dimensionality. The goal is to reduce dimensionality and carry out analysis with the highest information amount. Principal Component Analysis: it is a projection method that transforms objects belonging to a p-dimensional space into a k-dimensional space in such way as to preserve maximum information in the initial dimension. Attribute creation: it is a way to reduce the dimensionality of data. The selection usually aims to eliminate redundant. We can use different attribute selection techniques: Exhaustive approaches Non-exhaustive approaches Feature engineering (create new features): we have raw data and we can extract useful KPIs by designing new attributes. Discretization and binarization: transformation of continuos-valued attributes to discrete-valued attributes. Discretization techniques can be unsupervised (do not exploit knowledge about the class to which elements belong) or supervised (exploit knowledge about the class to which the elements belong). Unsupervised: equi-width, equi-frequency, K-means Supervised: discretization intervals are positioned to maximize the 'purity' of the intervals Entropy and Information Gain: it is the measure of uncertainty about the outcome of an experiment that can be modeled by a random variable x. The entropy of a certain event is zero. The entropy of a discretization into n intervals depends on how pure each group. Binarization: we start with a discrete attribute but we need it to be binary. Attribute transformation: function that maps the entire set of values of an attribute to a new set such that each value in the starting set corresponds to a unique value in the ending set.","title":"Dataset Preprocessing"},{"location":"data-mining/data-understanding/data-understanding.html#similarity-and-dissimilarity","text":"These two concepts are central in Machine Learning, as it is important to group clusters based on similarity and dissimilarity. Some techniques are stronger with long distances while sometimes, by setting the wrong distance, we will incur in problems. Similarity: it is a numerical measure expressing the degree of similarity between two objects Takes values in the range [0, 1] Dissimilarity (distance): it is a numerical measure expressing the degree of difference between two objects Takes values in the range [0, 1] or [0, \u221e]","title":"Similarity and Dissimilarity"},{"location":"data-mining/data-understanding/data-understanding.html#distance","text":"Distance Properties Given two objects p and q and a dissimilarity measure d(): d(p,q) = 0 only if p=q d(p,q) = d(q,p) -> Symmetry d(p,r) + d(p,q) + d(q,r) -> Triangular inequality Similarity Properties Given two objects p and q and a similarity measure s(): s(p,q) = 1 only if p=q s(p,q) = s(q,p) -> Symmetry Binary Vector Similarities It is common for attributes describing an object to contain only binary values. M01 = the number of attributes where p=0 and q=1 M10 = the number of attributes where p=1 and q=0 M00 = the number of attributes where p=0 and q=0 M11 = the number of attributes where p=1 and q=1 Cosine Similarity Like Jaccard's index. it does not consider 00 matches, but also allows non-binary vectors to be operated on. Similarity with Heterogeneous Attributes In the presence of heterogeneous attributes, it is necessary to compute the similarities separately and then combine them so that their result belongs to the range [0, 1]","title":"Distance"},{"location":"data-mining/data-understanding/data-understanding.html#correlation","text":"The correlation between pairs of objects described by attributes (binary or continuous) is a measure of the existence of a linear relationship between its attributes.","title":"Correlation"},{"location":"data-mining/decision-tree/SUMMARY.html","text":"Decision Tree Model","title":"SUMMARY"},{"location":"data-mining/decision-tree/model.html","text":"Decision Tree It is one of the most widely used classification techniques. It is simple, it can be trained with a limited number of examples, it is understandable and works well with categorical attributes. The usage of this model is characterized by a set of questions (yes/no), which build the tree. The idea is that the number of possible decision trees is exponential and we are looking for the best one (the one that creates the most accurate representation). All the classification algorithms are systems that work in a multidimensional space ans try to find some regions that have the same types of object (belonging to the same class). Learning the Model Many algorithms are available, but we will use C4.5 . The Haunt's Algorithm It is a recursive approach that progressively subdivides a set of Dt records into purely pure record sets. Procedure to follow: If Dt contains records belonging to the yj class only, then it is a lea node with label yj If Dt is an empty set, then t is a leaf node to which a parent node class is assigned If Dt contains records belonging to several classes, you choose an attribute and a split policy to partition the records into multiple subsets. Apply recursively the current procedure to each subset TreeGrowth(E,F) if StoppingCond(E,F) = TRUE then leaf = CreateNode() leaf.label = Classify(E) ; return leaf; else: root = CreateNode(); root.test cond = FindBestSplit(E,F) ; let V = {V | v is a possible outcome of root.test_cond} for each v \u2208 V do E = {e | root.test cond(e)=v and e \u2208 E} child = TreeGrowth(E,F); add child as descendant of root and label edge end for end if return root; end; Characteristic Feature Starting from the basic logic to completely define an algorithm for building decision trees, it is necessary to define: The split condition (depends on the type of attribute and on the number of splits) Nominal (N-ary split vs binary split) Ordinal (partitioning should not violate the order sorting) Continuous (the split condition can be expressed as a Boolean with N-ary split and as a binary comparison test with binary-split) Static (discretization takes place only once before applying the algorithm) DYnamic (discretization takes place at each recursion) The criterion defining the best split (it must allow you to determine more pure classes, using a measure of purity ) The criterion for interrupting splitting (AND conditions, if one applies, the splitting stops) When all its records belong to the same class When all its records have similar values on all attributes When the number of records in the node is below a certain threshold When the selected criterion would not be statistically relevant Methods for evaluating the goodness of a decision tree Metrics for Model Evaluation Confusion Matrix evaluates the ability of a classifier based on the following indicators: TP (true positive) FN (false negative) FP (false positive) TN (true negative) Accuracy is the most widely used metric to synthesize the information of a confusion matrix Accuracy Limitations Accuracy is not an appropriate metric if the classes contain a very different number of records. Precision and Recall are two metric used in applications where the correct classification of positive class records is more important Precision measures the fraction of record results actually among all those who were classified as such Recall measures the fraction of positive records correctly classified F-measure is a metric that summarizes precision and recall Cost-Based Evaluation Accuracy, precision, recall and F-measure classify an instance as positive if P(+,i) > P(-,i). They assume that FN and FP have the same weight, thus they are cost-intensive, but in many domains this is not true. ROC Space (Receiver Operator Characteristics) Roc graohs are two-dimensional graphs that depict relative tradeoffs between benefits (TP) and costs (FP) induced by a classifier. We distinguish between: Probabilistic classifiers return a score that is not necessarily a sensu strictu probability but represents the degree to which an object is a member of one particular class rather than another one - Discrete classifier predicts only the classes to which a test object belongs Classification Errors Training error: mistakes that are made on the training set Generalization error: errors made on the test set Underfitting: the model is too simple and does not allow a good classification or set training or test set Overfitting: the model is too complex, it allows a good classification of the training set, but a poor classification of the test set Due to noise (the boundaries of the areas are distorted) Due to the reduced size of the training set How to handle overfitting Pre-pruning: stop splitting before you reach a deep tree. A node can be split further if: Nodes does not contain instances All instances belong to the same class All attributes have the same values Post-pruning: run all possible splits to reduce the generalization error Post-pruning is more effective but involves more computational cost. It is based on the evidence of the result of a complete tree. Estimate Generalization Error A decision tree should minimize the error on the real data set, unfortunately during construction, only the training set is available. The methods for estimating the generalization error are: Optimistic approach Pessimistic approach Minimum Description Length (choose the model that minimizes the cost to describe a classification) Using the test set Building the Test Set Holdout: use 2/3 of training records and 1/3 for validation Random subsampling: repeated execution of the holdout method in which the training dataset is randomly selected Cross validation: partition the records into separate k subdivisions, run the training on k-1 divisions and test the reminder, repeat the test k times and calculate the average accuracy Bootstrap: The extracted records are replaced and records that are excluded form the validation set. This method does not create a new dataset with more information, but it can stabilize the obtained results of the available dataset. C4.5 (J48 on Weka) This algorithm exploits the GainRatio approach. It manages continuous attributes by determining a split point dividing the range of values into two. It manages data with missed values and run post pruning of the created tree.","title":"Decision Tree Model"},{"location":"data-mining/decision-tree/model.html#decision-tree","text":"It is one of the most widely used classification techniques. It is simple, it can be trained with a limited number of examples, it is understandable and works well with categorical attributes. The usage of this model is characterized by a set of questions (yes/no), which build the tree. The idea is that the number of possible decision trees is exponential and we are looking for the best one (the one that creates the most accurate representation). All the classification algorithms are systems that work in a multidimensional space ans try to find some regions that have the same types of object (belonging to the same class).","title":"Decision Tree"},{"location":"data-mining/decision-tree/model.html#learning-the-model","text":"Many algorithms are available, but we will use C4.5 . The Haunt's Algorithm It is a recursive approach that progressively subdivides a set of Dt records into purely pure record sets. Procedure to follow: If Dt contains records belonging to the yj class only, then it is a lea node with label yj If Dt is an empty set, then t is a leaf node to which a parent node class is assigned If Dt contains records belonging to several classes, you choose an attribute and a split policy to partition the records into multiple subsets. Apply recursively the current procedure to each subset TreeGrowth(E,F) if StoppingCond(E,F) = TRUE then leaf = CreateNode() leaf.label = Classify(E) ; return leaf; else: root = CreateNode(); root.test cond = FindBestSplit(E,F) ; let V = {V | v is a possible outcome of root.test_cond} for each v \u2208 V do E = {e | root.test cond(e)=v and e \u2208 E} child = TreeGrowth(E,F); add child as descendant of root and label edge end for end if return root; end;","title":"Learning the Model"},{"location":"data-mining/decision-tree/model.html#characteristic-feature","text":"Starting from the basic logic to completely define an algorithm for building decision trees, it is necessary to define: The split condition (depends on the type of attribute and on the number of splits) Nominal (N-ary split vs binary split) Ordinal (partitioning should not violate the order sorting) Continuous (the split condition can be expressed as a Boolean with N-ary split and as a binary comparison test with binary-split) Static (discretization takes place only once before applying the algorithm) DYnamic (discretization takes place at each recursion) The criterion defining the best split (it must allow you to determine more pure classes, using a measure of purity ) The criterion for interrupting splitting (AND conditions, if one applies, the splitting stops) When all its records belong to the same class When all its records have similar values on all attributes When the number of records in the node is below a certain threshold When the selected criterion would not be statistically relevant Methods for evaluating the goodness of a decision tree","title":"Characteristic Feature"},{"location":"data-mining/decision-tree/model.html#metrics-for-model-evaluation","text":"Confusion Matrix evaluates the ability of a classifier based on the following indicators: TP (true positive) FN (false negative) FP (false positive) TN (true negative) Accuracy is the most widely used metric to synthesize the information of a confusion matrix Accuracy Limitations Accuracy is not an appropriate metric if the classes contain a very different number of records. Precision and Recall are two metric used in applications where the correct classification of positive class records is more important Precision measures the fraction of record results actually among all those who were classified as such Recall measures the fraction of positive records correctly classified F-measure is a metric that summarizes precision and recall Cost-Based Evaluation Accuracy, precision, recall and F-measure classify an instance as positive if P(+,i) > P(-,i). They assume that FN and FP have the same weight, thus they are cost-intensive, but in many domains this is not true.","title":"Metrics for Model Evaluation"},{"location":"data-mining/decision-tree/model.html#roc-space-receiver-operator-characteristics","text":"Roc graohs are two-dimensional graphs that depict relative tradeoffs between benefits (TP) and costs (FP) induced by a classifier. We distinguish between: Probabilistic classifiers return a score that is not necessarily a sensu strictu probability but represents the degree to which an object is a member of one particular class rather than another one - Discrete classifier predicts only the classes to which a test object belongs","title":"ROC Space (Receiver Operator Characteristics)"},{"location":"data-mining/decision-tree/model.html#classification-errors","text":"Training error: mistakes that are made on the training set Generalization error: errors made on the test set Underfitting: the model is too simple and does not allow a good classification or set training or test set Overfitting: the model is too complex, it allows a good classification of the training set, but a poor classification of the test set Due to noise (the boundaries of the areas are distorted) Due to the reduced size of the training set How to handle overfitting Pre-pruning: stop splitting before you reach a deep tree. A node can be split further if: Nodes does not contain instances All instances belong to the same class All attributes have the same values Post-pruning: run all possible splits to reduce the generalization error Post-pruning is more effective but involves more computational cost. It is based on the evidence of the result of a complete tree.","title":"Classification Errors"},{"location":"data-mining/decision-tree/model.html#estimate-generalization-error","text":"A decision tree should minimize the error on the real data set, unfortunately during construction, only the training set is available. The methods for estimating the generalization error are: Optimistic approach Pessimistic approach Minimum Description Length (choose the model that minimizes the cost to describe a classification) Using the test set","title":"Estimate Generalization Error"},{"location":"data-mining/decision-tree/model.html#building-the-test-set","text":"Holdout: use 2/3 of training records and 1/3 for validation Random subsampling: repeated execution of the holdout method in which the training dataset is randomly selected Cross validation: partition the records into separate k subdivisions, run the training on k-1 divisions and test the reminder, repeat the test k times and calculate the average accuracy Bootstrap: The extracted records are replaced and records that are excluded form the validation set. This method does not create a new dataset with more information, but it can stabilize the obtained results of the available dataset.","title":"Building the Test Set"},{"location":"data-mining/decision-tree/model.html#c45-j48-on-weka","text":"This algorithm exploits the GainRatio approach. It manages continuous attributes by determining a split point dividing the range of values into two. It manages data with missed values and run post pruning of the created tree.","title":"C4.5 (J48 on Weka)"},{"location":"data-mining/introduction/SUMMARY.html","text":"Introduction Customer Retention Case","title":"SUMMARY"},{"location":"data-mining/introduction/customer-retention-case.html","text":"Customer Retention Customer retention, churn analysis, dropout analysis are synonyms for predictive analysis carried out by organizations and companies to avoid losing customers. The idea is to create a different profile for customers who stay and customers who drop-out. The Gym Case Study They discovered that customers who did not train well, eventually drop out from the gym. Therefore, the goal was to model customers' training sessions in order to predict those who did not train well and prevent them from dropping out. Steps: Customers have s list of exercises The system records the exercises (and repetition) did during the workout The system matches the exercises Train a classifier that is able to predict that someone is leaving the gym because he is unsatisfied The system update the profile each week Four weeks without training = dropout The idea of dropout needs to be defined properly (a customer who stops going to the gym in summer and comes back in summer is different from a customers who dropout and does not come back) Practitioner who is about to leave the gym is training poorly. How can characterize the user behaviors? How long does it last? Many KPIs can be adopted to assess the training session: in this case, two indicators were identified: Compliance (adherence of the performed workout) Regularity (regularity of the training sessions with reference to the prescribed one) We still have a problem of granularity: we can assess regularity by checking steps, repetition, physical activity, muscle or body part.","title":"Customer Retention Case"},{"location":"data-mining/introduction/customer-retention-case.html#customer-retention","text":"Customer retention, churn analysis, dropout analysis are synonyms for predictive analysis carried out by organizations and companies to avoid losing customers. The idea is to create a different profile for customers who stay and customers who drop-out. The Gym Case Study They discovered that customers who did not train well, eventually drop out from the gym. Therefore, the goal was to model customers' training sessions in order to predict those who did not train well and prevent them from dropping out. Steps: Customers have s list of exercises The system records the exercises (and repetition) did during the workout The system matches the exercises Train a classifier that is able to predict that someone is leaving the gym because he is unsatisfied The system update the profile each week Four weeks without training = dropout The idea of dropout needs to be defined properly (a customer who stops going to the gym in summer and comes back in summer is different from a customers who dropout and does not come back) Practitioner who is about to leave the gym is training poorly. How can characterize the user behaviors? How long does it last? Many KPIs can be adopted to assess the training session: in this case, two indicators were identified: Compliance (adherence of the performed workout) Regularity (regularity of the training sessions with reference to the prescribed one) We still have a problem of granularity: we can assess regularity by checking steps, repetition, physical activity, muscle or body part.","title":"Customer Retention"},{"location":"data-mining/introduction/data-mining.html","text":"Data Mining The amount of data stored on computer is constantly increasing, coming from: IoT data Social data Data on purchases Banking and credit card transaction The first step is to collect data in a data set. This step can be automated through artificial intelligence increasing the analytical power. From on side, data is more and more and on the other side, hardware becomes more powerful and cheaper each day. At the same time, managers are more and more willing to rely on data analysis for their business decisions. The information resource is a precious asset to overcoming competitors. Artificial Intelligence, Machine Learning and Data Mining Although strongly interrelates, the term machine learning is formally distinct from the term Data Mining which indicates the computational process of pattern discovery in large datasets using machine learning methods, artificial intelligence, statistics and databases. Data Mining - definition Complex extraction of implicit, previously unknown and potentially useful data from the information. Exploration and analysis, using automated and semi-automatic systems, of large amounts of data in order to find significant patterns through statistics. We do not just need to find results, but we need results to be USEFUL. Analytics ANalytics refers to software used to discover, understand and share relevant pattern in data. Analytics are based on the concurrent use of statistics, machine learning and operational research techniques, often exploiting visualization techniques. Prescriptive systems generale much value but it is extremely complex. Companies should start simple, adopting simple descriptive analytics solutions, and then move on. It is risky to skip intermediate steps. BI adoption path When we decide to digitalize a company, the adoption of BI solutions is incremental and rarely allows steps to be skipped. This is because it is risky, costly and useless to adopts advanced solutions before completely exploiting simple ones. The goal is to create a data-driven company , where managers are supported by data. Decisions are based on quantitative rather than qualitative knowledge. Process and knowledge are an asset of the company and are not lost if managers change The gap between a data-driven decision and a good decision is a good manager Adopting a data-driven mindset goes far beyond adopting a business intelligence solution and entails: Create a data culture Change the mindset of managers Change processes Improve the quality of all the data Digitalization is a journey that involves three main dimensions: Pattern A pattern is a synthetic representation rich in semantics of a set of data. It usually expresses a recurring pattern in data, but can also express an exceptional pattern. A pattern must be: Valid on data with a certain degree of confidence It can be understood from the syntax and semantic point of view, so that the user can interpret it Previously unknown and potentially useful, so that users can take actions accordingly When we distinguish between a manual technique (DW) and an automatic technique is the creation of a small subset of data which is rich in semantics. The process begins with a huge multi-dimension cube of data, then grouping and selection techniques are adopted, creating a pattern . Pattern types: Association rules (logical implications of the dataset) Classifiers (classify data according to a set of priori assigned classes) Decision trees (identify the causes that lead to an event, in order of importance) Clustering (group elements depending on their characteristics) Time series (detection of recurring or atypical patterns in complex data sequences) Data Mining Applications Predictive Systems Exploit some features to predict the unknown values of other features (classification and regression). Descriptive Systems Find user-readable patterns that can be understood by human users (clustering, association rules, sequential patter). Classification - Definition Given a record set, where each record is composed by a set of attributes (one of them represents the class of the record), find a model for the class attribute expressing the attribute value as a function of the remaining attributes. Given a feature (defined at priori), define weather a user belongs to that feature This model must work even when the record is not given. Unclassified record must be assigned to a class in the most accurate way. A test set is used to determine the model accuracy. Classification example Direct Marketing: The goal is to reduce the cost of email marketing by defining the set of customers that, with the highest probability, will buy a new product. Technique: Exploit the data collected during the launch of similar products We know which customers bought and which one did not { buy, not buy } = class attribute Collect all the available information about each customers Use such information as an input to train the model Churn Detection Predict customers who are willing to go to a competitor. Technique: Use the purchasing data of individual users to find the relevant attributes Label users as { loyal, not loyal } Find a pattern that defines loyalty Clustering example Given a set of points, each featuring set of attributes, and having a similarity measure between points, find subset of points such that: points belonging to a cluster are more similar to each other than those belonging to other clusters Marketing Segmentation The goal is to spit customers into distinct subsets to target specific marketing activities. Techniques: Gather information about customer lifestyle and geographic location Find clusters of similar customers Measure cluster quality by verifying whether the purchasing patterns of customers belonging to the same cluster are more similar to those of distinct clusters Association Rules example Given a set of records each consisting of multiple elements belonging to a given collection. It produces rules of dependence that predict the occurrence of one of the elements in the presence of others. Marketing Sales Promotion Suppose you have discovered this association rule: { Bagels,...} -> { Potato chips*} This information can be used to understand what actions to take to increase its sales. Data Mining Bets Scalability Multidimensionality of data set Complexity and heterogeneity of the data Data quality Data properties Privacy keeping Processing in real-time CRISP methodology A data mining project requires a structured approach in order to choose the best algorithm. CRISP-DM methodology is the most used technique. It is one of the most structures proposals to define the fundamental steps of a data mining project. The six stages of the life cycle are not strictly sequential, indeed, it is often necessary. Business understanding (understand the application domain): understanding project goals from users' point of view, translate the user's problem into a data mining problem and define a project plan. Get an idea about the business domain and the data mining approach to adopt. Data understanding : preliminary data collection aimed at identifying quality problems and conducting preliminary analysis to identify the salient characteristics. Data preparation : tasks needed to create the final dataset, selecting attributes and records, transforming and cleaning data. Prepare the data for ML tasks (clean, complete missing data, create new features) Model creation : data mining techniques are applied to the dataset in order to identify what makes the model more accurate. Evaluation of model and results : the model obtained from the previous phase are analyzed to verify that they are sufficiently precise and robust to respond adequately to the user's objectives. Deployment : the built-in model and acquired knowledge must be made available to users. Change the software and processes to include new AI functionalities Different classes of data mining use different algorithms so the evaluation changes accordingly.","title":"Introduction"},{"location":"data-mining/introduction/data-mining.html#data-mining","text":"The amount of data stored on computer is constantly increasing, coming from: IoT data Social data Data on purchases Banking and credit card transaction The first step is to collect data in a data set. This step can be automated through artificial intelligence increasing the analytical power. From on side, data is more and more and on the other side, hardware becomes more powerful and cheaper each day. At the same time, managers are more and more willing to rely on data analysis for their business decisions. The information resource is a precious asset to overcoming competitors.","title":"Data Mining"},{"location":"data-mining/introduction/data-mining.html#artificial-intelligence-machine-learning-and-data-mining","text":"Although strongly interrelates, the term machine learning is formally distinct from the term Data Mining which indicates the computational process of pattern discovery in large datasets using machine learning methods, artificial intelligence, statistics and databases.","title":"Artificial Intelligence, Machine Learning and Data Mining"},{"location":"data-mining/introduction/data-mining.html#data-mining-definition","text":"Complex extraction of implicit, previously unknown and potentially useful data from the information. Exploration and analysis, using automated and semi-automatic systems, of large amounts of data in order to find significant patterns through statistics. We do not just need to find results, but we need results to be USEFUL.","title":"Data Mining - definition"},{"location":"data-mining/introduction/data-mining.html#analytics","text":"ANalytics refers to software used to discover, understand and share relevant pattern in data. Analytics are based on the concurrent use of statistics, machine learning and operational research techniques, often exploiting visualization techniques. Prescriptive systems generale much value but it is extremely complex. Companies should start simple, adopting simple descriptive analytics solutions, and then move on. It is risky to skip intermediate steps.","title":"Analytics"},{"location":"data-mining/introduction/data-mining.html#bi-adoption-path","text":"When we decide to digitalize a company, the adoption of BI solutions is incremental and rarely allows steps to be skipped. This is because it is risky, costly and useless to adopts advanced solutions before completely exploiting simple ones. The goal is to create a data-driven company , where managers are supported by data. Decisions are based on quantitative rather than qualitative knowledge. Process and knowledge are an asset of the company and are not lost if managers change The gap between a data-driven decision and a good decision is a good manager Adopting a data-driven mindset goes far beyond adopting a business intelligence solution and entails: Create a data culture Change the mindset of managers Change processes Improve the quality of all the data Digitalization is a journey that involves three main dimensions:","title":"BI adoption path"},{"location":"data-mining/introduction/data-mining.html#pattern","text":"A pattern is a synthetic representation rich in semantics of a set of data. It usually expresses a recurring pattern in data, but can also express an exceptional pattern. A pattern must be: Valid on data with a certain degree of confidence It can be understood from the syntax and semantic point of view, so that the user can interpret it Previously unknown and potentially useful, so that users can take actions accordingly When we distinguish between a manual technique (DW) and an automatic technique is the creation of a small subset of data which is rich in semantics. The process begins with a huge multi-dimension cube of data, then grouping and selection techniques are adopted, creating a pattern . Pattern types: Association rules (logical implications of the dataset) Classifiers (classify data according to a set of priori assigned classes) Decision trees (identify the causes that lead to an event, in order of importance) Clustering (group elements depending on their characteristics) Time series (detection of recurring or atypical patterns in complex data sequences)","title":"Pattern"},{"location":"data-mining/introduction/data-mining.html#data-mining-applications","text":"Predictive Systems Exploit some features to predict the unknown values of other features (classification and regression). Descriptive Systems Find user-readable patterns that can be understood by human users (clustering, association rules, sequential patter).","title":"Data Mining Applications"},{"location":"data-mining/introduction/data-mining.html#classification-definition","text":"Given a record set, where each record is composed by a set of attributes (one of them represents the class of the record), find a model for the class attribute expressing the attribute value as a function of the remaining attributes. Given a feature (defined at priori), define weather a user belongs to that feature This model must work even when the record is not given. Unclassified record must be assigned to a class in the most accurate way. A test set is used to determine the model accuracy.","title":"Classification - Definition"},{"location":"data-mining/introduction/data-mining.html#classification-example","text":"Direct Marketing: The goal is to reduce the cost of email marketing by defining the set of customers that, with the highest probability, will buy a new product. Technique: Exploit the data collected during the launch of similar products We know which customers bought and which one did not { buy, not buy } = class attribute Collect all the available information about each customers Use such information as an input to train the model Churn Detection Predict customers who are willing to go to a competitor. Technique: Use the purchasing data of individual users to find the relevant attributes Label users as { loyal, not loyal } Find a pattern that defines loyalty","title":"Classification example"},{"location":"data-mining/introduction/data-mining.html#clustering-example","text":"Given a set of points, each featuring set of attributes, and having a similarity measure between points, find subset of points such that: points belonging to a cluster are more similar to each other than those belonging to other clusters Marketing Segmentation The goal is to spit customers into distinct subsets to target specific marketing activities. Techniques: Gather information about customer lifestyle and geographic location Find clusters of similar customers Measure cluster quality by verifying whether the purchasing patterns of customers belonging to the same cluster are more similar to those of distinct clusters","title":"Clustering example"},{"location":"data-mining/introduction/data-mining.html#association-rules-example","text":"Given a set of records each consisting of multiple elements belonging to a given collection. It produces rules of dependence that predict the occurrence of one of the elements in the presence of others. Marketing Sales Promotion Suppose you have discovered this association rule: { Bagels,...} -> { Potato chips*} This information can be used to understand what actions to take to increase its sales.","title":"Association Rules example"},{"location":"data-mining/introduction/data-mining.html#data-mining-bets","text":"Scalability Multidimensionality of data set Complexity and heterogeneity of the data Data quality Data properties Privacy keeping Processing in real-time","title":"Data Mining Bets"},{"location":"data-mining/introduction/data-mining.html#crisp-methodology","text":"A data mining project requires a structured approach in order to choose the best algorithm. CRISP-DM methodology is the most used technique. It is one of the most structures proposals to define the fundamental steps of a data mining project. The six stages of the life cycle are not strictly sequential, indeed, it is often necessary. Business understanding (understand the application domain): understanding project goals from users' point of view, translate the user's problem into a data mining problem and define a project plan. Get an idea about the business domain and the data mining approach to adopt. Data understanding : preliminary data collection aimed at identifying quality problems and conducting preliminary analysis to identify the salient characteristics. Data preparation : tasks needed to create the final dataset, selecting attributes and records, transforming and cleaning data. Prepare the data for ML tasks (clean, complete missing data, create new features) Model creation : data mining techniques are applied to the dataset in order to identify what makes the model more accurate. Evaluation of model and results : the model obtained from the previous phase are analyzed to verify that they are sufficiently precise and robust to respond adequately to the user's objectives. Deployment : the built-in model and acquired knowledge must be made available to users. Change the software and processes to include new AI functionalities Different classes of data mining use different algorithms so the evaluation changes accordingly.","title":"CRISP methodology"},{"location":"data-mining/rule-based-classifier/SUMMARY.html","text":"Rule-Based classifier Model","title":"SUMMARY"},{"location":"data-mining/rule-based-classifier/rule-classifier.html","text":"Rule-Based Classifier The basic idea is to classify records using rule sets of the type \" if .. then \". The condition used with 'if' is called the antecedent and predicted class of each rule is called the consequent. A rule has the form: (condition) -> y Building a model means identifying a set of rules Coverage and Accuracy Given a dataset D and a classification rule A -> y, we define: Coverage as the portion of records satisfying the antecedent of the rule Coverage = |A|/|D| Accuracy as the fraction that, by satisfying the antecedent, also satisfy the consequent Accuracy = |A \u2229 y|/|A| A set of rules R us said to be mutually exclusive if no pair of rules can be activated by the same record. A set of rules R has exhaustive coverage if there is one rule for each combination of attribute values. Properties It is nt always possible to determine an exhaustive and mutually exclusive set of rules Lack of mutual exclusivity Lack of exhaustiveness Rule Sorting Approach Rule-based sorting (individual rules are sort according to their quality) Class-based sorting (groups of rules that determine the same class appear consequently in the list) Sequential Covering set R = \u00d8 for each class y \u2208 Y 0 y k do stop=FALSE; while !stop do r = Learn One Rule(E,A,y) remove from E training records that are covered by r If Quality(r,E ) < Threshold then stop=TRUE; else R = R \u222a r // Add r at the bottom of the rule list end while end for R = R \u222a {{} -> y k } // Add the default rule at the bottom of the rule list PostPruning (R); Dropping instances from Training Set Deleting instances from the training set serves the purpose of: Properly classified instances: to avoid generating the same rule again and again, avoid overestimating the accuracy of the next rule Incorrectly classified instances: to avoid underestimating the accuracy of the next rule","title":"Rule-Based classifier Model"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#rule-based-classifier","text":"The basic idea is to classify records using rule sets of the type \" if .. then \". The condition used with 'if' is called the antecedent and predicted class of each rule is called the consequent. A rule has the form: (condition) -> y Building a model means identifying a set of rules","title":"Rule-Based Classifier"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#coverage-and-accuracy","text":"Given a dataset D and a classification rule A -> y, we define: Coverage as the portion of records satisfying the antecedent of the rule Coverage = |A|/|D| Accuracy as the fraction that, by satisfying the antecedent, also satisfy the consequent Accuracy = |A \u2229 y|/|A| A set of rules R us said to be mutually exclusive if no pair of rules can be activated by the same record. A set of rules R has exhaustive coverage if there is one rule for each combination of attribute values.","title":"Coverage and Accuracy"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#properties","text":"It is nt always possible to determine an exhaustive and mutually exclusive set of rules Lack of mutual exclusivity Lack of exhaustiveness","title":"Properties"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#rule-sorting-approach","text":"Rule-based sorting (individual rules are sort according to their quality) Class-based sorting (groups of rules that determine the same class appear consequently in the list)","title":"Rule Sorting Approach"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#sequential-covering","text":"set R = \u00d8 for each class y \u2208 Y 0 y k do stop=FALSE; while !stop do r = Learn One Rule(E,A,y) remove from E training records that are covered by r If Quality(r,E ) < Threshold then stop=TRUE; else R = R \u222a r // Add r at the bottom of the rule list end while end for R = R \u222a {{} -> y k } // Add the default rule at the bottom of the rule list PostPruning (R);","title":"Sequential Covering"},{"location":"data-mining/rule-based-classifier/rule-classifier.html#dropping-instances-from-training-set","text":"Deleting instances from the training set serves the purpose of: Properly classified instances: to avoid generating the same rule again and again, avoid overestimating the accuracy of the next rule Incorrectly classified instances: to avoid underestimating the accuracy of the next rule","title":"Dropping instances from Training Set"},{"location":"machine-learning/LAB/lab-notes.html","text":"Laboratory Programing done in Python We will use Colab as a programming tool We can use any IDE (Visual Studio, PyCharm) Guide Do not execute code without understanding it It is necessary to play a lot with the code to become familiar Bugs can be very informative Colab Free Google service Easy to use, the environment provided already has most of the resources installed The code runs in the browser (VM) Each assigned virtual machine has a variable hardware equipment CPU and RAM available GPU resources","title":"Laboratory"},{"location":"machine-learning/LAB/lab-notes.html#laboratory","text":"Programing done in Python We will use Colab as a programming tool We can use any IDE (Visual Studio, PyCharm)","title":"Laboratory"},{"location":"machine-learning/LAB/lab-notes.html#guide","text":"Do not execute code without understanding it It is necessary to play a lot with the code to become familiar Bugs can be very informative","title":"Guide"},{"location":"machine-learning/LAB/lab-notes.html#colab","text":"Free Google service Easy to use, the environment provided already has most of the resources installed The code runs in the browser (VM) Each assigned virtual machine has a variable hardware equipment CPU and RAM available GPU resources","title":"Colab"},{"location":"machine-learning/introduction/SUMMARY.html","text":"Introduction to Machine Learning Historical Evolution of AI","title":"SUMMARY"},{"location":"machine-learning/introduction/history.html","text":"Historical Evolution of AI To understand why AI is so important today, we have to analyze the past. In 1950 the enthusiasm for AI began: Turing Test : \"Can machines think?\" 1954: one of the main experiments in machine translation 1955: Arthur Samuel wrote a program that could play checkers very well 1957: Rosenblatt invented perceptrons, a type of neural network First AI Winter - promises of AI were exaggerated In 1980 the Boom times occurred: Commercialization of new AI Expert Systems capable of reproducing human-decision making, through \"if-then-else\" rules Financial planning, medical diagnosis, geological exploration, and microelectronic circuit design Second AI Winter - many tasks were too complicated for engineers In 2012 the Deep Learning revolution took place Solved mathematical problems New powerful Neural Networks Huge improvement with the computational power Introduction of GPUs Problem with data AI models need huge amount of training data Currently, we are able to: Acquire a lot of data (IoT) Store huge amount of data (improved storage) Today, the question is not if we are able to collect data, but if we are able to use them.","title":"Historical Evolution of AI"},{"location":"machine-learning/introduction/history.html#historical-evolution-of-ai","text":"To understand why AI is so important today, we have to analyze the past. In 1950 the enthusiasm for AI began: Turing Test : \"Can machines think?\" 1954: one of the main experiments in machine translation 1955: Arthur Samuel wrote a program that could play checkers very well 1957: Rosenblatt invented perceptrons, a type of neural network First AI Winter - promises of AI were exaggerated In 1980 the Boom times occurred: Commercialization of new AI Expert Systems capable of reproducing human-decision making, through \"if-then-else\" rules Financial planning, medical diagnosis, geological exploration, and microelectronic circuit design Second AI Winter - many tasks were too complicated for engineers In 2012 the Deep Learning revolution took place Solved mathematical problems New powerful Neural Networks Huge improvement with the computational power Introduction of GPUs Problem with data AI models need huge amount of training data Currently, we are able to: Acquire a lot of data (IoT) Store huge amount of data (improved storage) Today, the question is not if we are able to collect data, but if we are able to use them.","title":"Historical Evolution of AI"},{"location":"machine-learning/introduction/introduction.html","text":"Introduction to Machine Learning What are the main features of intelligence? Intelligence is a very general mental capability that, among the other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. Artificial Intelligence is a huge set of disciplines which also includes machine learning. With machine learning, we refer only to a small subset inside artificial intelligence. We, as humans, take many activities for granted that for machines would be very complex. Simulating human intelligence is extremely complex as our brain in an incredibly sophisticated machine, of which we still know few aspects. Impact of AI in our world For both graphs, we can not the exponential trend and the variety of continents covered. Information is the oil of 21st century, and analytics is the combustion engine The number of requests per position as data scientist is constantly increasing as companies need to extract knowledge from data to survive. The revolution introduced by AI is reflected also in companies. At least 5 of the top 10 world companies are directly related to AI. Also, at least 2 companies are directly related to the production of chips, key elements for AI. Nvidia It is a software and fabless company that design GPUs, which nowadays are essential to: Create AI models Perform High Performance Computing (HPC) Nvidia is the leading company in the sector and this is the reason why its shares has risen significantly in recent years. The General Paradigm of Machine Learning Machine Learning is a subset of the AI field that tries to develop systems able to automatically learn from specific examples ( training data ) amd to generalize the knowledge on new samples ( testing data ) of the same domain. From a practical point of view: We have some data which represents our application domain We implement an algorithm able to learn from the data (training phase) We use data to understand if the trained model has learned something -> model deployment The main steps for the development of intelligent systems based on ML: Data Acquisition Data is the founding element of any application related to ML. Acquiring large amounts of data is one of the main concerns for top-companies today. Data Processing All those techniques with which data are processed in order to adapt them to the best of the ML model that we plan to develop. Model This is the main core of AI systems. A model can be seen as a set of mathematical and statistical technique able to learn from a certain distribution of data provided in input and to generalize on new data. Prediction It can take many forms depending on the application developed. It is the output of the model and it is important to evaluate the effectiveness of the developed system.","title":"Introduction to Machine Learning"},{"location":"machine-learning/introduction/introduction.html#introduction-to-machine-learning","text":"What are the main features of intelligence? Intelligence is a very general mental capability that, among the other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience. Artificial Intelligence is a huge set of disciplines which also includes machine learning. With machine learning, we refer only to a small subset inside artificial intelligence. We, as humans, take many activities for granted that for machines would be very complex. Simulating human intelligence is extremely complex as our brain in an incredibly sophisticated machine, of which we still know few aspects.","title":"Introduction to Machine Learning"},{"location":"machine-learning/introduction/introduction.html#impact-of-ai-in-our-world","text":"For both graphs, we can not the exponential trend and the variety of continents covered. Information is the oil of 21st century, and analytics is the combustion engine The number of requests per position as data scientist is constantly increasing as companies need to extract knowledge from data to survive. The revolution introduced by AI is reflected also in companies. At least 5 of the top 10 world companies are directly related to AI. Also, at least 2 companies are directly related to the production of chips, key elements for AI. Nvidia It is a software and fabless company that design GPUs, which nowadays are essential to: Create AI models Perform High Performance Computing (HPC) Nvidia is the leading company in the sector and this is the reason why its shares has risen significantly in recent years.","title":"Impact of AI in our world"},{"location":"machine-learning/introduction/introduction.html#the-general-paradigm-of-machine-learning","text":"Machine Learning is a subset of the AI field that tries to develop systems able to automatically learn from specific examples ( training data ) amd to generalize the knowledge on new samples ( testing data ) of the same domain. From a practical point of view: We have some data which represents our application domain We implement an algorithm able to learn from the data (training phase) We use data to understand if the trained model has learned something -> model deployment The main steps for the development of intelligent systems based on ML: Data Acquisition Data is the founding element of any application related to ML. Acquiring large amounts of data is one of the main concerns for top-companies today. Data Processing All those techniques with which data are processed in order to adapt them to the best of the ML model that we plan to develop. Model This is the main core of AI systems. A model can be seen as a set of mathematical and statistical technique able to learn from a certain distribution of data provided in input and to generalize on new data. Prediction It can take many forms depending on the application developed. It is the output of the model and it is important to evaluate the effectiveness of the developed system.","title":"The General Paradigm of Machine Learning"}]}; var __search = { index: Promise.resolve(local_index) }