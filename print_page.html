
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-8.5.10">
    
    
      
        <title>Print as PDF - CescaNeri/DataMining-MachineLearning</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.472b142f.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.08040f6c.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="css/print-site.css">
    
      <link rel="stylesheet" href="css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="index.html" title="CescaNeri/DataMining-MachineLearning" class="md-header__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CescaNeri/DataMining-MachineLearning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print as PDF
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="index.html" title="CescaNeri/DataMining-MachineLearning" class="md-nav__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    CescaNeri/DataMining-MachineLearning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="index.html" class="md-nav__link">
        Data Mining and Machine Learning
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Data Mining
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Mining" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Data Mining
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/data-mining.html" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/customer-retention-case.html" class="md-nav__link">
        Customer Retention Case
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Data Understanding
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Understanding" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Data Understanding
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/data-understanding/data-understanding.html" class="md-nav__link">
        Data Understanding
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Decision Tree
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Decision Tree" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Decision Tree
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/decision-tree/model.html" class="md-nav__link">
        Decision Tree Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Classifier Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Classifier Models" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Classifier Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/rule-classifier.html" class="md-nav__link">
        Rule-Based classifier Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/instance-based.html" class="md-nav__link">
        Instance-Based Classifier Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/bayesian-classifier.html" class="md-nav__link">
        Bayesian Classifier
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/multiclassifier.html" class="md-nav__link">
        Multi-Classifier
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Association Rule
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Association Rule" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Association Rule
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/association-rule/association-rule.html" class="md-nav__link">
        Association Rules
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/association-rule/sequential-pattern.html" class="md-nav__link">
        Sequential Pattern
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/association-rule/outlier.html" class="md-nav__link">
        Outlier Detection
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Clustering
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Clustering" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Clustering
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/clustering/clustering.html" class="md-nav__link">
        Clustering
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/introduction/introduction.html" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/introduction/history.html" class="md-nav__link">
        Historical Evolution of AI
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_9">
          Data Acquisition and Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Acquisition and Processing" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          Data Acquisition and Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-acquisition.html" class="md-nav__link">
        Data Acquisition and Processing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-types.html" class="md-nav__link">
        Data Types
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-preparation.html" class="md-nav__link">
        Data Preparation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_10" type="checkbox" id="__nav_10" >
      
      
      
      
        <label class="md-nav__link" for="__nav_10">
          Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Model" data-md-level="1">
        <label class="md-nav__title" for="__nav_10">
          <span class="md-nav__icon md-icon"></span>
          Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/model/model.html" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/model/pattern.html" class="md-nav__link">
        Pattern Recognition
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_11" type="checkbox" id="__nav_11" >
      
      
      
      
        <label class="md-nav__link" for="__nav_11">
          Classification
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Classification" data-md-level="1">
        <label class="md-nav__title" for="__nav_11">
          <span class="md-nav__icon md-icon"></span>
          Classification
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/classification/classification.html" class="md-nav__link">
        Classification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/classification/metrics.html" class="md-nav__link">
        Metrics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/classification/deep-learning.html" class="md-nav__link">
        Deep Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/classification/neural-network.html" class="md-nav__link">
        Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/classification/machine-vs-deep.html" class="md-nav__link">
        Machine Learning vs Deep Learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_12" type="checkbox" id="__nav_12" >
      
      
      
      
        <label class="md-nav__link" for="__nav_12">
          DM - LAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="DM - LAB" data-md-level="1">
        <label class="md-nav__title" for="__nav_12">
          <span class="md-nav__icon md-icon"></span>
          DM - LAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/weka-lab.html" class="md-nav__link">
        Introduction to Weka
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/bank-data.html" class="md-nav__link">
        Bank Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/census-data.html" class="md-nav__link">
        Census Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/clustering.html" class="md-nav__link">
        Clustering
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_13" type="checkbox" id="__nav_13" >
      
      
      
      
        <label class="md-nav__link" for="__nav_13">
          ML - LAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML - LAB" data-md-level="1">
        <label class="md-nav__title" for="__nav_13">
          <span class="md-nav__icon md-icon"></span>
          ML - LAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/LAB/lab-notes.html" class="md-nav__link">
        Lab Notes
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14" type="checkbox" id="__nav_14" >
      
      
      
      
        <label class="md-nav__link" for="__nav_14">
          ML - Seminar
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML - Seminar" data-md-level="1">
        <label class="md-nav__title" for="__nav_14">
          <span class="md-nav__icon md-icon"></span>
          ML - Seminar
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/seminar/ai-business.html" class="md-nav__link">
        AI Solutions for real-world business
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/seminar/use-cases.html" class="md-nav__link">
        Inventory Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/seminar/marketing.html" class="md-nav__link">
        Marketing Management Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/seminar/nlp.html" class="md-nav__link">
        Natural Language Processing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/seminar/ethics.html" class="md-nav__link">
        AI Ethics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Print as PDF
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="print_page.html" class="md-nav__link md-nav__link--active">
        Print as PDF
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    1. Data Mining and Machine Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    2. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    3. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    4. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    5. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classifier-models" class="md-nav__link">
    IV. Classifier Models
  </a>
  
    <nav class="md-nav" aria-label="IV. Classifier Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-rule-classifier" class="md-nav__link">
    6. Rule-Based classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-instance-based" class="md-nav__link">
    7. Instance-Based Classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-bayesian-classifier" class="md-nav__link">
    8. Bayesian Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-multiclassifier" class="md-nav__link">
    9. Multi-Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-association-rule" class="md-nav__link">
    V. Association Rule
  </a>
  
    <nav class="md-nav" aria-label="V. Association Rule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-association-rule" class="md-nav__link">
    10. Association Rules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-sequential-pattern" class="md-nav__link">
    11. Sequential Pattern
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-outlier" class="md-nav__link">
    12. Outlier Detection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-clustering" class="md-nav__link">
    VI. Clustering
  </a>
  
    <nav class="md-nav" aria-label="VI. Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-clustering-clustering" class="md-nav__link">
    13. Clustering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-machine-learning" class="md-nav__link">
    VII. Machine Learning
  </a>
  
    <nav class="md-nav" aria-label="VII. Machine Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-introduction" class="md-nav__link">
    14. Introduction to Machine Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-history" class="md-nav__link">
    15. Historical Evolution of AI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-acquisition-and-processing" class="md-nav__link">
    VIII. Data Acquisition and Processing
  </a>
  
    <nav class="md-nav" aria-label="VIII. Data Acquisition and Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-acquisition" class="md-nav__link">
    16. Data Acquisition and Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-types" class="md-nav__link">
    17. Data Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-preparation" class="md-nav__link">
    18. Data Preparation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-model" class="md-nav__link">
    IX. Model
  </a>
  
    <nav class="md-nav" aria-label="IX. Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-model" class="md-nav__link">
    19. Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-pattern" class="md-nav__link">
    20. Pattern Recognition
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classification" class="md-nav__link">
    X. Classification
  </a>
  
    <nav class="md-nav" aria-label="X. Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-classification" class="md-nav__link">
    21. Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-metrics" class="md-nav__link">
    22. Metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-deep-learning" class="md-nav__link">
    23. Deep Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-neural-network" class="md-nav__link">
    24. Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-machine-vs-deep" class="md-nav__link">
    25. Machine Learning vs Deep Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-dm-lab" class="md-nav__link">
    XI. DM - LAB
  </a>
  
    <nav class="md-nav" aria-label="XI. DM - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-weka-lab" class="md-nav__link">
    26. Introduction to Weka
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-bank-data" class="md-nav__link">
    27. Bank Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-census-data" class="md-nav__link">
    28. Census Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-clustering" class="md-nav__link">
    29. Clustering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-lab" class="md-nav__link">
    XII. ML - LAB
  </a>
  
    <nav class="md-nav" aria-label="XII. ML - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-lab-lab-notes" class="md-nav__link">
    30. Lab Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-seminar" class="md-nav__link">
    XIII. ML - Seminar
  </a>
  
    <nav class="md-nav" aria-label="XIII. ML - Seminar">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-ai-business" class="md-nav__link">
    31. AI Solutions for real-world business
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-use-cases" class="md-nav__link">
    32. Inventory Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-marketing" class="md-nav__link">
    33. Marketing Management Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-nlp" class="md-nav__link">
    34. Natural Language Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-ethics" class="md-nav__link">
    35. AI Ethics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#" class="md-nav__link">
    1. Data Mining and Machine Learning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    2. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    3. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    4. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    5. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classifier-models" class="md-nav__link">
    IV. Classifier Models
  </a>
  
    <nav class="md-nav" aria-label="IV. Classifier Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-rule-classifier" class="md-nav__link">
    6. Rule-Based classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-instance-based" class="md-nav__link">
    7. Instance-Based Classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-bayesian-classifier" class="md-nav__link">
    8. Bayesian Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-multiclassifier" class="md-nav__link">
    9. Multi-Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-association-rule" class="md-nav__link">
    V. Association Rule
  </a>
  
    <nav class="md-nav" aria-label="V. Association Rule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-association-rule" class="md-nav__link">
    10. Association Rules
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-sequential-pattern" class="md-nav__link">
    11. Sequential Pattern
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-association-rule-outlier" class="md-nav__link">
    12. Outlier Detection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-clustering" class="md-nav__link">
    VI. Clustering
  </a>
  
    <nav class="md-nav" aria-label="VI. Clustering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-clustering-clustering" class="md-nav__link">
    13. Clustering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-machine-learning" class="md-nav__link">
    VII. Machine Learning
  </a>
  
    <nav class="md-nav" aria-label="VII. Machine Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-introduction" class="md-nav__link">
    14. Introduction to Machine Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-history" class="md-nav__link">
    15. Historical Evolution of AI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-acquisition-and-processing" class="md-nav__link">
    VIII. Data Acquisition and Processing
  </a>
  
    <nav class="md-nav" aria-label="VIII. Data Acquisition and Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-acquisition" class="md-nav__link">
    16. Data Acquisition and Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-types" class="md-nav__link">
    17. Data Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-preparation" class="md-nav__link">
    18. Data Preparation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-model" class="md-nav__link">
    IX. Model
  </a>
  
    <nav class="md-nav" aria-label="IX. Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-model" class="md-nav__link">
    19. Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-pattern" class="md-nav__link">
    20. Pattern Recognition
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classification" class="md-nav__link">
    X. Classification
  </a>
  
    <nav class="md-nav" aria-label="X. Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-classification" class="md-nav__link">
    21. Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-metrics" class="md-nav__link">
    22. Metrics
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-deep-learning" class="md-nav__link">
    23. Deep Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-neural-network" class="md-nav__link">
    24. Neural Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-classification-machine-vs-deep" class="md-nav__link">
    25. Machine Learning vs Deep Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-dm-lab" class="md-nav__link">
    XI. DM - LAB
  </a>
  
    <nav class="md-nav" aria-label="XI. DM - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-weka-lab" class="md-nav__link">
    26. Introduction to Weka
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-bank-data" class="md-nav__link">
    27. Bank Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-census-data" class="md-nav__link">
    28. Census Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-clustering" class="md-nav__link">
    29. Clustering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-lab" class="md-nav__link">
    XII. ML - LAB
  </a>
  
    <nav class="md-nav" aria-label="XII. ML - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-lab-lab-notes" class="md-nav__link">
    30. Lab Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-seminar" class="md-nav__link">
    XIII. ML - Seminar
  </a>
  
    <nav class="md-nav" aria-label="XIII. ML - Seminar">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-ai-business" class="md-nav__link">
    31. AI Solutions for real-world business
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-use-cases" class="md-nav__link">
    32. Inventory Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-marketing" class="md-nav__link">
    33. Marketing Management Optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-nlp" class="md-nav__link">
    34. Natural Language Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-seminar-ethics" class="md-nav__link">
    35. AI Ethics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
  
                  


<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
    <span style="float: right"><a href="https://timvink.github.io/mkdocs-print-site-plugin/">mkdocs-print-site-plugin</a></span>
</p>
<p>
    This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.
</p>
<p>
    See also <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-PDF.html">export to PDF</a> and <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-HTML.html">export to standalone HTML</a>.
</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Index</h1>
                </nav>
            </div>
        </section>
        <section class="print-page" id="index"><h1 id="index-data-mining-and-machine-learning">Data Mining and Machine Learning</h1>
<h2 id="index-modules">Modules:</h2>
<ul>
<li>
<p><strong>Data Mining</strong>: Prof. Golfarelli - 36h</p>
<ul>
<li>introduction to data mining</li>
<li>Knowledge discovery process</li>
<li>Understanding and preparing data</li>
<li>Data mining techniques</li>
<li>Data understanding and validation</li>
<li>Weka software</li>
<li>Case studies analysis</li>
</ul>
</li>
<li>
<p><strong>Machine Learning</strong>: Prof. Guido Borghi - 18h</p>
<ul>
<li>Introduction to AI</li>
<li>Machine Learning and Deep Learning</li>
<li>Data acquisition and Processing</li>
<li>Model Training</li>
<li>Metrics</li>
<li>LIBRARIES:<ul>
<li>Scikit-learn (ML)</li>
<li>Tensofrflow (DL)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="index-assessment-method-exam">Assessment Method - EXAM</h2>
<p>The exam consists in an oral exam on all the subjects (80%) of the course and an elaborate (20% - agreed with the teacher).</p>
<p>The <strong>elaborate</strong> must be carried out in the <strong>Machine Learning</strong> module, choosing between:</p>
<ul>
<li>Study and algorithm among those in the literature</li>
<li>Analysis of a data set with mining techniques</li>
</ul>
<p>There are no fixed dates for the exam, it can be defined with teachers along the whole academic year. 
The two modules must be discussed within <em>15 days</em>.</p></section>
                        <h1 class='nav-section-title' id='section-data-mining'>
                            Data Mining <a class='headerlink' href='#section-data-mining' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-introduction-data-mining"><h1 id="data-mining-introduction-data-mining-data-mining">Data Mining</h1>
<p>The amount of data stored on computer is constantly increasing, coming from:</p>
<ul>
<li>IoT data</li>
<li>Social data</li>
<li>Data on purchases</li>
<li>Banking and credit card transaction</li>
</ul>
<p>The first step is to collect data in a data set. This step can be automated through artificial intelligence increasing the analytical power.</p>
<p>From on side, data is more and more and on the other side, hardware becomes more powerful and cheaper each day.</p>
<p>At the same time, managers are more and more willing to rely on data analysis for their business decisions.
The information resource is a precious asset to overcoming competitors. </p>
<h2 id="data-mining-introduction-data-mining-artificial-intelligence-machine-learning-and-data-mining">Artificial Intelligence, Machine Learning and Data Mining</h2>
<p>Although strongly interrelates, the term machine learning is formally distinct from the term Data Mining which indicates the computational process of pattern discovery in large datasets using machine learning methods, artificial intelligence, statistics and databases. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-definition">Data Mining - definition</h2>
<p>Complex extraction of implicit, previously unknown and potentially useful data from the information.
Exploration and analysis, using automated and semi-automatic systems, of large amounts of data in order to find significant patterns through statistics. </p>
<p>We do not just need to find results, but we need results to be USEFUL. </p>
<h2 id="data-mining-introduction-data-mining-analytics">Analytics</h2>
<p>Analytics refers to software used to discover, understand and share relevant pattern in data.
Analytics are based on the concurrent use of statistics, machine learning and operational research techniques, often exploiting visualization techniques. </p>
<p><img alt="" src="data-mining/introduction/BI.jpg" /></p>
<p>Prescriptive systems generate much value but it is extremely complex. Companies should start simple, adopting simple descriptive analytics solutions, and then move on. 
It is risky to skip intermediate steps.</p>
<h2 id="data-mining-introduction-data-mining-bi-adoption-path">BI adoption path</h2>
<p>When we decide to digitalize a company, the adoption of BI solutions is incremental and rarely allows steps to be skipped.
This is because it is risky, costly and useless to adopts advanced solutions before completely exploiting simple ones. </p>
<p>The goal is to create a <strong>data-driven company</strong>, where managers are supported by data. </p>
<ul>
<li>Decisions are based on quantitative rather than qualitative knowledge.</li>
<li>Process and knowledge are an asset of the company and are not lost if managers change</li>
</ul>
<p><em>The gap between a data-driven decision and a good decision is a good manager</em></p>
<p>Adopting a data-driven mindset goes far beyond adopting a business intelligence solution and entails:</p>
<ol>
<li>Create a data culture</li>
<li>Change the mindset of managers</li>
<li>Change processes</li>
<li>Improve the quality of all the data</li>
</ol>
<p><strong>Digitalization</strong> is a journey that involves three main dimensions:</p>
<p><img alt="" src="data-mining/introduction/DT.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-pattern">Pattern</h2>
<p>A pattern is a synthetic representation rich in semantics of a set of data. It usually expresses a recurring pattern in data, but can also express an exceptional pattern.</p>
<p>A pattern must be:</p>
<ul>
<li>Valid on data with a certain degree of confidence</li>
<li>It can be understood from the syntax and semantic point of view, so that the user can interpret it</li>
<li>Previously unknown and potentially useful, so that users can take actions accordingly</li>
</ul>
<p>When we distinguish between a manual technique (DW) and an automatic technique is the creation of a small subset of data which is rich in semantics.</p>
<p>The process begins with a huge multi-dimension cube of data, then grouping and selection techniques are adopted, creating a <strong>pattern</strong>.</p>
<p><strong>Pattern types:</strong></p>
<ul>
<li>Association rules (logical implications of the dataset)</li>
<li>Classifiers (classify data according to a set of priori assigned classes)</li>
<li>Decision trees (identify the causes that lead to an event, in order of importance)</li>
<li>Clustering (group elements depending on their characteristics)</li>
<li>Time series (detection of recurring or atypical patterns in complex data sequences)</li>
</ul>
<h2 id="data-mining-introduction-data-mining-data-mining-applications">Data Mining Applications</h2>
<p><strong>Predictive Systems</strong></p>
<p>Exploit some features to predict the unknown values of other features (classification and regression).</p>
<p><strong>Descriptive Systems</strong></p>
<p>Find user-readable patterns that can be understood by human users (clustering, association rules, sequential pattern).</p>
<h2 id="data-mining-introduction-data-mining-classification-definition">Classification - Definition</h2>
<p>Given a <strong>record</strong> set, where each record is composed by a set of attributes (one of them represents the class of the record), find a model for the class attribute expressing the attribute value as a function of the remaining attributes.</p>
<p><em>Given a feature (defined at priori), define weather a user belongs to that feature</em></p>
<p>This model must work even when the record is not given. Unclassified record must be assigned to a class in the most accurate way.</p>
<p>A <strong>test set</strong> is used to determine the model accuracy.</p>
<p><img alt="" src="data-mining/introduction/test.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-classification-example">Classification example</h2>
<p><strong>Direct Marketing:</strong>
The goal is to reduce the cost of email marketing by defining the set of customers that, with the highest probability, will buy a new product.</p>
<p>Technique:</p>
<ul>
<li>Exploit the data collected during the launch of similar products<ul>
<li>We know which customers bought and which one did not</li>
<li>{<em>buy, not buy</em>} = <strong>class attribute</strong></li>
</ul>
</li>
<li>Collect all the available information about each customers</li>
<li>Use such information as an input to train the model</li>
</ul>
<p><strong>Churn Detection</strong>
Predict customers who are willing to go to a competitor.</p>
<p>Technique:</p>
<ul>
<li>Use the purchasing data of individual users to find the relevant attributes</li>
<li>Label users as {<em>loyal, not loyal</em>}</li>
<li>Find a pattern that defines loyalty</li>
</ul>
<h2 id="data-mining-introduction-data-mining-clustering-example">Clustering example</h2>
<p>Given a set of points, each featuring set of attributes, and having a similarity measure between points, find subset of points such that:
<em>points belonging to a cluster are more similar to each other than those belonging to other clusters</em></p>
<p><strong>Marketing Segmentation</strong></p>
<p>The goal is to split customers into distinct subsets to target specific marketing activities.</p>
<p>Techniques:</p>
<ul>
<li>Gather information about customer lifestyle and geographic location</li>
<li>Find clusters of similar customers</li>
<li>Measure cluster quality by verifying whether the purchasing patterns of customers belonging to the same cluster are more similar to those of distinct clusters</li>
</ul>
<h2 id="data-mining-introduction-data-mining-association-rules-example">Association Rules example</h2>
<p>Given a set of records each consisting of multiple elements belonging to a given collection.
It produces rules of dependence that predict the occurrence of one of the elements in the presence of others.</p>
<p><strong>Marketing Sales Promotion</strong>
Suppose you have discovered this association rule:
{<em>Bagels,...} -&gt; {</em>Potato chips*}</p>
<p>This information can be used to understand what actions to take to increase its sales. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-bets">Data Mining Bets</h2>
<ul>
<li>Scalability</li>
<li>Multidimensionality of data set</li>
<li>Complexity and heterogeneity of the data</li>
<li>Data quality</li>
<li>Data properties</li>
<li>Privacy keeping</li>
<li>Processing in real-time</li>
</ul>
<h2 id="data-mining-introduction-data-mining-crisp-methodology">CRISP methodology</h2>
<p>A data mining project requires a structured approach in order to choose the best algorithm.</p>
<p><strong>CRISP-DM</strong> methodology is the most used technique. It is one of the most structured proposals to define the fundamental steps of a data mining project. </p>
<p><img alt="" src="data-mining/introduction/crisp.jpg" /></p>
<p>The six stages of the life cycle are not strictly sequential, indeed, it is often necessary.</p>
<ol>
<li><strong>Business understanding</strong> (understand the application domain): understanding project goals from users' point of view, translate the user's problem into a data mining problem and define a project plan.<ul>
<li>Get an idea about the business domain and the data mining approach to adopt.</li>
</ul>
</li>
<li><strong>Data understanding</strong>: preliminary data collection aimed at identifying quality problems and conducting preliminary analysis to identify the salient characteristics.</li>
<li><strong>Data preparation</strong>: tasks needed to create the final dataset, selecting attributes and records, transforming and cleaning data.<ul>
<li>Prepare the data for ML tasks (clean, complete missing data, create new features)</li>
</ul>
</li>
<li><strong>Model creation</strong>: data mining techniques are applied to the dataset in order to identify what makes the model more accurate.</li>
<li><strong>Evaluation of model and results</strong>: the model obtained from the previous phase are analyzed to verify that they are sufficiently precise and robust to respond adequately to the user's objectives.</li>
<li><strong>Deployment</strong>: the built-in model and acquired knowledge must be made available to users.<ul>
<li>Change the software and processes to include new AI functionalities</li>
</ul>
</li>
</ol>
<p>Different classes of data mining use different algorithms so the evaluation changes accordingly. </p></section><section class="print-page" id="data-mining-introduction-customer-retention-case"><h1 id="data-mining-introduction-customer-retention-case-customer-retention">Customer Retention</h1>
<p>Customer retention, churn analysis, dropout analysis are synonyms for predictive analysis carried out by organizations and companies to avoid losing customers.</p>
<p>The idea is to create a different profile for customers who stay and customers who drop-out.</p>
<p><strong>The Gym Case Study</strong></p>
<p>They discovered that customers who did not train well, eventually drop out from the gym.
Therefore, the goal was to model customers' training sessions in order to predict those who did not train well and prevent them from dropping out.</p>
<p>Steps:</p>
<ul>
<li>Customers have s list of exercises</li>
<li>The system records the exercises (and repetition) did during the workout</li>
<li>The system matches the exercises </li>
<li>Train a classifier that is able to predict that someone is leaving the gym because he is unsatisfied<ul>
<li>The system update the profile each week</li>
<li>Four weeks without training = dropout</li>
<li>The idea of dropout needs to be defined properly (a customer who stops going to the gym in summer and comes back in summer is different from a customers who dropout and does not come back)</li>
</ul>
</li>
</ul>
<p>Practitioner who is about to leave the gym is training poorly. How can characterize the user behaviors? How long does it last?</p>
<p>Many KPIs can be adopted to assess the training session: in this case, two indicators were identified:</p>
<ol>
<li><strong>Compliance</strong> (adherence of the performed workout)</li>
<li><strong>Regularity</strong> (regularity of the training sessions with reference to the prescribed one)</li>
</ol>
<p>We still have a problem of <strong>granularity:</strong> we can assess regularity by checking steps, repetition, physical activity, muscle or body part. </p></section><h1 class='nav-section-title-end'>Ended: Data Mining</h1>
                        <h1 class='nav-section-title' id='section-data-understanding'>
                            Data Understanding <a class='headerlink' href='#section-data-understanding' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-data-understanding-data-understanding"><h1 id="data-mining-data-understanding-data-understanding-data-understanding-preparation">Data Understanding &amp; Preparation</h1>
<p>In data mining, data are composed of collections of objects described by a set of attributes (we refer to data that can be stored in a database).</p>
<p><strong>Attribute:</strong> property characteristic of an object</p>
<h2 id="data-mining-data-understanding-data-understanding-attribute-types">Attribute types</h2>
<p>In order to perform meaningful analysis, the characteristics of the attributes must be known. The <strong>attribute type</strong> tells us what properties are reflected in the value we use as a measure. </p>
<p>We can identify 4 types of attributes:</p>
<ol>
<li><strong>Nominal</strong>-qualitative: different names of value (gender, zip code, ID)</li>
<li><strong>Ordinal</strong>-qualitative: values enables us to sort objects based on the value of attribute (grade)</li>
<li><strong>Interval</strong>-quantitative: the difference between the values has a meaning, with a unit of measurement (dates, temperature)</li>
<li><strong>Ratio</strong>-quantitative: the ratio of values has meaning (age, length, amount of money)</li>
</ol>
<h2 id="data-mining-data-understanding-data-understanding-further-classifications">Further classifications</h2>
<ul>
<li>Binary, discrete and continuous<ul>
<li>Discrete: finite number of infinite countable set of values</li>
<li>Continuous: real values</li>
</ul>
</li>
</ul>
<p><em>Nominal and ordinal are typically discrete or binary, while interval and ratio attributes are continuous</em>
- Asymmetric attribute: only instances that take non-zero values are relevant
- Documents and Texts: objects of the analysis described by a vector of terms
- Transactions
    - Each record involves multiple items
    - Items come from a finite set
    - The number of items may vary from transaction to transaction
- Ordered data</p>
<h2 id="data-mining-data-understanding-data-understanding-explorative-analysis">Explorative Analysis</h2>
<p>First step in business and data understanding. It refers to the preliminary analysis of the data aimed at identify its main characteristics. </p>
<ul>
<li>It helps you choose the best tool for processing and analysis </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-statistics-overview">STATISTICS OVERVIEW</h2>
<h3 id="data-mining-data-understanding-data-understanding-frequency">Frequency</h3>
<p>The frequency of an attribute value is the percentage of times that value appears in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-mode">Mode</h3>
<p>The mode of an attribute is the value that appears most frequently in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-percentile">Percentile</h3>
<p>Given an ordinal or continuous attribute x and a  number p between 0 and 100, the p-th percentile is the value of xp of x such that p% of the observed values for x are lower than xp.</p>
<p><img alt="" src="data-mining/data-understanding/boxplot.jpg" /></p>
<p>Percentile visualization through boxplot enables the representation of a distribution of data. It can be used to compare multiple distributions when they have homogeneous magnitude.</p>
<h3 id="data-mining-data-understanding-data-understanding-mean">Mean</h3>
<p>The mean is the most common measure for locating a set of points.</p>
<ul>
<li>Subject to outliers</li>
<li>It is preferred to use tee median or a 'controlled' mean </li>
</ul>
<h3 id="data-mining-data-understanding-data-understanding-median">Median</h3>
<p>The median is the term occupying the central place if the terms are odd; if the terms are even, the median is the arithmetic mean of the two central terms.</p>
<h3 id="data-mining-data-understanding-data-understanding-range">Range</h3>
<p>Range is the difference between the minimum and maximum values taken by the attribute.</p>
<h3 id="data-mining-data-understanding-data-understanding-variance-and-standard-deviation">Variance and Standard Deviation</h3>
<p>Variance and SD are the most common measures of dispersion of a data set.</p>
<ul>
<li>Sensitive to outliers since they are quadratically related to the concept of mean</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/var-sd.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-data-quality">Data Quality</h2>
<p>The quality of the datasets profoundly affects the chances of finding meaningful patterns.
The most frequent problems that deteriorate data quality are:</p>
<ul>
<li>Noise and outliers (objects with characteristics very different from all other objects in the data set)</li>
<li>Missing values (not collecting the data is different from when the attribute is not applicable), how to handle them:<ul>
<li>Delete the objects that contain them</li>
<li>Ignore missing values during analysis</li>
<li>Manually/automatically fill the missing values<ul>
<li>ML can be applied to fill the missing values by inferring the other values of that attribute and calculate the most appropriate value </li>
</ul>
</li>
</ul>
</li>
<li>Duplicated values (it may be necessary to introduce a data cleaning step in order to identify and eliminate redundancy)</li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-dataset-preprocessing">Dataset Preprocessing</h2>
<p>Rarely the dataset has the optimal characteristics to be best processed by machine learning algorithms. It is therefore necessary to put in place a series of actions to enable the algorithms of interest to function:</p>
<ul>
<li><strong>Aggregation:</strong> combine two or more attributes into one attribute</li>
<li><strong>Sampling:</strong> main technique to select data<ul>
<li>Collecting and processing the entire dataset is too expensive and time consuming</li>
<li>Simple Random Sampling (same probability of selecting each element)</li>
<li>Stratified sampling (divides the data into multiple partitions and use simple random sampling on each partition)<ul>
<li>Before sampling a partitioning rule is applied (we inject knowledge about the domain)</li>
<li>Allow the population to be balanced</li>
<li>However, we are applying a distortion</li>
</ul>
</li>
<li>Sampling Cardinality: after choosing the sampling mode, it is necessary to fix the sample size in order to limit the loss of information</li>
</ul>
</li>
<li><strong>Dimensionality reduction:</strong> the goal is to avoid the 'curse of dimensionality', reduce the amount of time and memory used by ML algorithms, simplify data visualization and eliminate irrelevant attributes and eliminate noise on data.
Curse of dimensionality: as dimensionality increases, the data become progressively more sparse. Many clustering and classification algorithms deal with dimensionality and distances. All the elements become equi-distant from one another; the idea of selecting the right dimension to carry out analysis is crucial.</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/dimension.jpg" /></p>
<p>The curve indicates that the more we increase the number of dimensionality, the smaller the ratio is.
In the modeling phase, it is important reduce dimensionality.</p>
<p>The goal is to reduce dimensionality and carry out analysis with the highest information amount.</p>
<ul>
<li><strong>Principal Component Analysis:</strong> it is a projection method that transforms objects belonging to a p-dimensional space into a k-dimensional space in such way as to preserve maximum information in the initial dimension.</li>
<li><strong>Attribute creation:</strong> it is a way to reduce the dimensionality of data. The selection usually aims to eliminate redundant. 
We can use different attribute selection techniques:<ul>
<li>Exhaustive approaches</li>
<li>Non-exhaustive approaches</li>
<li>Feature engineering (create new features): we have raw data and we can extract useful KPIs by designing new attributes.</li>
</ul>
</li>
<li>
<p><strong>Discretization and binarization:</strong> transformation of continuos-valued attributes to discrete-valued attributes. Discretization techniques can be unsupervised (do not exploit knowledge about the class to which elements belong) or supervised (exploit knowledge about the class to which the elements belong).</p>
<ul>
<li>
<p>Unsupervised: equi-width, equi-frequency, K-means
<img alt="" src="data-mining/data-understanding/discretization.jpg" /></p>
</li>
<li>
<p>Supervised: discretization intervals are positioned to maximize the 'purity' of the intervals</p>
</li>
</ul>
</li>
</ul>
<p><strong>Entropy and Information Gain:</strong> it is the measure of uncertainty about the outcome of an experiment that can be modeled by a random variable x.
The entropy of a <strong>certain</strong> event is zero.</p>
<p>The entropy of a discretization into n intervals depends on how pure each group.</p>
<p><img alt="" src="data-mining/data-understanding/entropy.jpg" /></p>
<p><strong>Binarization:</strong> we start with a discrete attribute but we need it to be binary.</p>
<ul>
<li><strong>Attribute transformation:</strong> function that maps the entire set of values of an attribute to a new set such that each value in the starting set corresponds to a unique value in the ending set. </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-similarity-and-dissimilarity">Similarity and Dissimilarity</h2>
<p>These two concepts are central in Machine Learning, as it is important to group clusters based on similarity and dissimilarity.</p>
<p>Some techniques are stronger with long distances while sometimes, by setting the wrong distance, we will incur in problems.</p>
<ul>
<li>
<p><strong>Similarity:</strong> it is a numerical measure expressing the degree of similarity between two objects</p>
<ul>
<li>Takes values in the range [0, 1]</li>
</ul>
</li>
<li>
<p><strong>Dissimilarity (distance):</strong> it is a numerical measure expressing the degree of difference between two objects</p>
<ul>
<li>Takes values in the range [0, 1] or [0, ]</li>
</ul>
</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/similarity.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-distance">Distance</h2>
<p><img alt="" src="data-mining/data-understanding/distance.jpg" /></p>
<p><strong>Distance Properties</strong></p>
<p>Given two objects p and q and a dissimilarity measure d():</p>
<ul>
<li>d(p,q) = 0 only if p=q</li>
<li>d(p,q) = d(q,p) -&gt; <em>Symmetry</em> </li>
<li>d(p,r) + d(p,q) + d(q,r) -&gt; <em>Triangular inequality</em></li>
</ul>
<p><img alt="" src="data-mining/data-understanding/triangular.jpg" /></p>
<p><strong>Similarity Properties</strong></p>
<p>Given two objects p and q and a similarity measure s():</p>
<ul>
<li>s(p,q) = 1 only if p=q</li>
<li>s(p,q) = s(q,p) -&gt; <em>Symmetry</em></li>
</ul>
<p><strong>Binary Vector Similarities</strong></p>
<p>It is common for attributes describing an object to contain only binary values.</p>
<ul>
<li>M01 = the number of attributes where p=0 and q=1</li>
<li>M10 = the number of attributes where p=1 and q=0</li>
<li>M00 = the number of attributes where p=0 and q=0</li>
<li>M11 = the number of attributes where p=1 and q=1</li>
</ul>
<p><strong>Cosine Similarity</strong></p>
<p>Like Jaccard's index. it does not consider 00 matches, but also allows non-binary vectors to be operated on.</p>
<p><strong>Similarity with Heterogeneous Attributes</strong></p>
<p>In the presence of heterogeneous attributes, it is necessary to compute the similarities separately and then combine them so that their result belongs to the range [0, 1]</p>
<h2 id="data-mining-data-understanding-data-understanding-correlation">Correlation</h2>
<p>The correlation between pairs of objects described by attributes (binary or continuous) is a measure of the existence of a linear relationship between its attributes.</p>
<p><img alt="" src="data-mining/data-understanding/correlation.jpg" /></p>
<p><img alt="" src="data-mining/data-understanding/corr2.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Data Understanding</h1>
                        <h1 class='nav-section-title' id='section-decision-tree'>
                            Decision Tree <a class='headerlink' href='#section-decision-tree' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-decision-tree-model"><h1 id="data-mining-decision-tree-model-decision-tree">Decision Tree</h1>
<p>It is one of the most widely used classification techniques. It is simple, it can be trained with a limited number of examples, it is understandable and works well with categorical attributes.</p>
<p>The usage of this model is characterized by a set of questions (yes/no), which build the tree.
The idea is that the number of possible decision trees is exponential and we are looking for the best one (the one that creates the most accurate representation).</p>
<p>All the classification algorithms are systems that work in a multidimensional space ans try to find some regions that have the same types of object (belonging to the same class).</p>
<p><img alt="" src="data-mining/decision-tree/classification.jpg" /></p>
<h2 id="data-mining-decision-tree-model-learning-the-model">Learning the Model</h2>
<p>Many algorithms are available, but we will use <strong>C4.5</strong>.</p>
<p><strong>The Haunt's Algorithm</strong>
It is a recursive approach that progressively subdivides a set of Dt records into purely pure record sets.</p>
<p>Procedure to follow:</p>
<ol>
<li>If Dt contains records belonging to the yj class only, then it is a lea node with label <em>yj</em></li>
<li>If Dt is an empty set, then t is a leaf node to which a parent node class is assigned</li>
<li>If Dt contains records belonging to several classes, you choose an attribute and a split policy to partition the records into multiple subsets.</li>
<li>Apply recursively the current procedure to each subset</li>
</ol>
<div class="highlight"><pre><span></span><code>TreeGrowth(E,F)
    if StoppingCond(E,F) = TRUE then
        leaf = CreateNode()
        leaf.label = Classify(E) ;
        return leaf;
    else:
        root = CreateNode();
        root.test cond = FindBestSplit(E,F) ;
        let V = {V | v is a possible outcome of root.test_cond}
    for each v  V do
        E = {e | root.test cond(e)=v and e  E}
        child = TreeGrowth(E,F);
        add child as descendant of root and label edge
    end for
    end if
        return root;
    end;
</code></pre></div>
<h2 id="data-mining-decision-tree-model-characteristic-feature">Characteristic Feature</h2>
<p>Starting from the basic logic to completely define an algorithm for building decision trees, it is necessary to define:</p>
<ul>
<li>The split condition (depends on the type of attribute and on the number of splits)<ul>
<li>Nominal (N-ary split vs binary split)</li>
<li>Ordinal (partitioning should not violate the order sorting)</li>
<li>Continuous (the split condition can be expressed as a Boolean with N-ary split and as a binary comparison test with binary-split)<ul>
<li>Static (discretization takes place only once before applying the algorithm)</li>
<li>DYnamic (discretization takes place at each recursion)</li>
</ul>
</li>
</ul>
</li>
<li>The criterion defining the best split (it must allow you to determine more pure classes, using a <strong>measure of purity</strong>)<ul>
<li><img alt="" src="data-mining/decision-tree/impurity.jpg" /></li>
</ul>
</li>
<li>The criterion for interrupting splitting (AND conditions, if one applies, the splitting stops)<ul>
<li>When all its records belong to the same class</li>
<li>When all its records have similar values on all attributes</li>
<li>When the number of records in the node is below a certain threshold</li>
<li>When the selected criterion would not be statistically relevant</li>
</ul>
</li>
<li>Methods for evaluating the goodness of a decision tree</li>
</ul>
<h2 id="data-mining-decision-tree-model-metrics-for-model-evaluation">Metrics for Model Evaluation</h2>
<p><strong>Confusion Matrix</strong> evaluates the ability of a classifier based on the following indicators:</p>
<ul>
<li>TP (true positive)</li>
<li>FN (false negative)</li>
<li>FP (false positive)</li>
<li>TN (true negative)</li>
</ul>
<p><strong>Accuracy</strong> is the most widely used metric to synthesize the information of a confusion matrix</p>
<p><img alt="" src="data-mining/decision-tree/accuracy.jpg" /></p>
<ul>
<li><strong>Accuracy Limitations</strong></li>
</ul>
<p>Accuracy is not an appropriate  metric if the classes contain a very different number of records.</p>
<p><strong>Precision and Recall</strong> are two metric used in applications where the correct classification of positive class records is more important</p>
<ul>
<li><strong>Precision</strong> measures the fraction of record results actually positive among all those who were classified as such</li>
<li><strong>Recall</strong> measures the fraction of positive records correctly classified</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/precision-recall.jpg" /></p>
<p><strong>F-measure</strong> is a metric that summarizes precision and recall</p>
<p><strong>Cost-Based Evaluation</strong>
Accuracy, precision, recall and F-measure classify an instance as positive if P(+,i) &gt; P(-,i).
They assume that FN and FP have the same weight, thus they are cost-intensive, but in many domains this is not true.</p>
<p><img alt="" src="data-mining/decision-tree/cost.jpg" /></p>
<h2 id="data-mining-decision-tree-model-roc-space-receiver-operator-characteristics">ROC Space (Receiver Operator Characteristics)</h2>
<p>Roc graphs are two-dimensional graphs that depict relative tradeoffs between benefits (TP) and costs (FP) induced by a classifier. We distinguish between:</p>
<ul>
<li><strong>Probabilistic classifiers</strong> return a score that is not necessarily a <em>sensu strictu</em> probability but represents the degree to which an object is a member of one particular class rather than another one</li>
<li><strong>Discrete classifier</strong> predicts only the classes to which a test object belongs</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/ROC.jpg" /></p>
<h2 id="data-mining-decision-tree-model-classification-errors">Classification Errors</h2>
<ul>
<li><strong>Training error:</strong> mistakes that are made on the training set</li>
<li><strong>Generalization error:</strong>  errors made on the test set</li>
<li><strong>Underfitting:</strong> the model is too simple and does not allow a good classification or set training or test set</li>
<li><strong>Overfitting:</strong> the model is too complex, it allows a good classification of the training set, but a poor classification of the test set<ul>
<li>Due to noise (the boundaries of the areas are distorted)</li>
<li>Due to the reduced size of the training set</li>
</ul>
</li>
</ul>
<p><strong>How to handle overfitting</strong></p>
<ul>
<li>Pre-pruning: stop splitting before you reach a deep tree. A node can not be split further if:<ul>
<li>Nodes does not contain instances</li>
<li>All instances belong to the same class</li>
<li>All attributes have the same values</li>
</ul>
</li>
<li>Post-pruning: run all possible splits to reduce the generalization error</li>
</ul>
<p>Post-pruning is more effective but involves more computational cost. It is based on the evidence of the result of a complete tree.</p>
<h2 id="data-mining-decision-tree-model-estimate-generalization-error">Estimate Generalization Error</h2>
<p>A decision tree should minimize the error on the real data set, unfortunately during construction, only the training set is available.</p>
<p>The methods for estimating the generalization error are:</p>
<ul>
<li>Optimistic approach</li>
<li>Pessimistic approach</li>
<li>Minimum Description Length (choose the model that minimizes the cost to describe a classification)</li>
<li>Using the test set</li>
</ul>
<h2 id="data-mining-decision-tree-model-building-the-test-set">Building the Test Set</h2>
<ul>
<li><strong>Holdout:</strong> use 2/3 of training records and 1/3 for validation</li>
<li><strong>Random subsampling:</strong> repeated execution of the holdout method in which the training dataset is randomly selected</li>
<li><strong>Cross validation:</strong> partition the records into separate k subdivisions, run the training on k-1 divisions and test the reminder, repeat the test k times and calculate the average accuracy</li>
<li><strong>Bootstrap:</strong> The extracted records are replaced and records that are excluded form the validation set. This method does not create a new dataset with more information, but it can stabilize the obtained results of the available dataset.</li>
</ul>
<h2 id="data-mining-decision-tree-model-c45-j48-on-weka">C4.5 (J48 on Weka)</h2>
<p>This algorithm exploits the GainRatio approach. It manages continuous attributes by determining a split point dividing the range of values into two.
It manages data with missed values and run post pruning of the created tree.</p></section><h1 class='nav-section-title-end'>Ended: Decision Tree</h1>
                        <h1 class='nav-section-title' id='section-classifier-models'>
                            Classifier Models <a class='headerlink' href='#section-classifier-models' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-classifiers-rule-classifier"><h1 id="data-mining-classifiers-rule-classifier-rule-based-classifier">Rule-Based Classifier</h1>
<p>The basic idea is to classify records using rule sets of the type "<em>if .. then</em>". The condition used with 'if' is called the antecedent while the predicted class of each rule is called the consequent.</p>
<p>A rule has the form: (condition) -&gt; y</p>
<p>Building a model means identifying a set of rules</p>
<p><img alt="" src="data-mining/classifiers/properties.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-coverage-and-accuracy">Coverage and Accuracy</h2>
<p>We can have very accurate rules but with low coverage, which is not that relevant.
Given a dataset D and a classification rule A -&gt; y, we define:</p>
<ul>
<li><strong>Coverage</strong> as the portion of records satisfying the antecedent of the rule<ul>
<li>Coverage = |A|/|D|</li>
</ul>
</li>
<li><strong>Accuracy</strong> as the fraction that, by satisfying the antecedent, also satisfy the consequent<ul>
<li>Accuracy = |A  y|/|A|</li>
</ul>
</li>
</ul>
<p>A set of rules R us said to be <strong>mutually exclusive</strong> if no pair of rules can be activated by the same record.</p>
<p>A set of rules R has <strong>exhaustive coverage</strong> if there is one rule for each combination of attribute values.</p>
<h2 id="data-mining-classifiers-rule-classifier-properties">Properties</h2>
<ul>
<li>It is not always possible to determine an exhaustive and mutually exclusive set of rules</li>
<li>Lack of mutual exclusivity</li>
<li>Lack of exhaustiveness</li>
</ul>
<h2 id="data-mining-classifiers-rule-classifier-rule-sorting-approach">Rule Sorting Approach</h2>
<ol>
<li>Rule-based sorting (individual rules are sort according to their quality)</li>
<li>Class-based sorting (groups of rules that determine the same class appear consequently in the list)</li>
</ol>
<p><img alt="" src="data-mining/classifiers/sorting.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-sequential-covering">Sequential Covering</h2>
<p><div class="highlight"><pre><span></span><code><span class="nb">set</span> <span class="n">R</span> <span class="o">=</span> <span class="n"></span>
<span class="k">for</span> <span class="n">each</span> <span class="k">class</span> <span class="nc">y</span> <span class="err"></span> <span class="n">Y</span> <span class="mi">0</span> <span class="n">y</span> <span class="n">k</span> <span class="n">do</span>
    <span class="n">stop</span><span class="o">=</span><span class="n">FALSE</span><span class="p">;</span>
<span class="k">while</span> <span class="err">!</span><span class="n">stop</span> <span class="n">do</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">Learn</span> <span class="n">One</span> <span class="n">Rule</span><span class="p">(</span><span class="n">E</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">remove</span> <span class="kn">from</span> <span class="nn">E</span> <span class="n">training</span> <span class="n">records</span> <span class="n">that</span> <span class="n">are</span> <span class="n">covered</span> <span class="n">by</span> <span class="n">r</span>
    <span class="n">If</span> <span class="n">Quality</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">E</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="n">Threshold</span> <span class="n">then</span>
        <span class="n">stop</span><span class="o">=</span><span class="n">TRUE</span><span class="p">;</span>
    <span class="k">else</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="err"></span> <span class="n">r</span> <span class="o">//</span> <span class="n">Add</span> <span class="n">r</span> <span class="n">at</span> <span class="n">the</span> <span class="n">bottom</span> <span class="n">of</span> <span class="n">the</span> <span class="n">rule</span> <span class="nb">list</span>
<span class="n">end</span> <span class="k">while</span>
<span class="n">end</span> <span class="k">for</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="err"></span> <span class="p">{{}</span> <span class="o">-&gt;</span> <span class="n">y</span> <span class="n">k</span> <span class="p">}</span> <span class="o">//</span> <span class="n">Add</span> <span class="n">the</span> <span class="n">default</span> <span class="n">rule</span> <span class="n">at</span> <span class="n">the</span> <span class="n">bottom</span> <span class="n">of</span> <span class="n">the</span> <span class="n">rule</span> <span class="nb">list</span>
<span class="n">PostPruning</span> <span class="p">(</span><span class="n">R</span><span class="p">);</span>
</code></pre></div>
<img alt="" src="data-mining/classifiers/sequantial.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-dropping-instances-from-training-set">Dropping instances from Training Set</h2>
<p>Deleting instances from the training set serves the purpose of:</p>
<ul>
<li>Properly classified instances: to avoid generating the same rule again and again, avoid overestimating the accuracy of the next rule</li>
<li>Incorrectly classified instances: to avoid underestimating the accuracy of the next rule</li>
</ul>
<p><img alt="" src="data-mining/classifiers/dropping.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-learn-one-rule">Learn-One-Rule</h2>
<p>We want something that is general (even with a lower accuracy). The goal of the algorithm is to find a rule that covers as many possible examples and as few as possible negative examples.</p>
<p>Rule are constructed by progressively considering a new possible predicate.</p>
<ul>
<li>In order to choose which predicate to add, a criterion is needed:<ul>
<li>n = number of instances covered by the rule</li>
<li>nr = number of instances properly classified by the rule</li>
<li>k = number of classes</li>
</ul>
</li>
</ul>
<p><strong>Accuracy(r) = nr/n</strong></p>
<p>Some metrics (like the <strong>FoilGrain</strong>) supports the rule by identifying the number of positive examples covered by the rule.</p>
<p><strong>Stop Criterion</strong>: as soon as the rule is not relevant anymore, stop it.</p>
<p><strong>Rule Pruning</strong>: it aims at simplifying rules to improve rule generalization error.
It can be useful given that the construction approach is greedy.</p>
<p><em>example: remove the predicate whose removal results in the greatest improvement in error rate on the validation set</em></p>
<h2 id="data-mining-classifiers-rule-classifier-the-ripper-method">The RIPPER Method</h2>
<p>It is an approach based on sequential covering for 2-class problem and it is used to choose one of the classes as a positive class and the other as a negative class.</p>
<p>The idea is to compute the description length (cost for transmitting the data set from one user to another) and if it exceeds the threshold, we should stop.</p>
<p><img alt="" src="data-mining/classifiers/indirect.jpg" /></p></section><section class="print-page" id="data-mining-classifiers-instance-based"><h1 id="data-mining-classifiers-instance-based-instance-based-classifier">Instance-Based Classifier</h1>
<p>These classifiers do not build models but classify new records based on their similarity to the examples in the training set.</p>
<p>They are called <em>lazy-diligent learners</em> as opposed to <em>impatient  learners</em> (rule-based, decision trees).</p>
<p><img alt="" src="data-mining/classifiers/knn.jpg" /></p>
<p><strong>K-Nearest Neighbor</strong></p>
<p>K-Nearest Neighbor is a simple algorithm that stores all the available cases and classifies the new data or case based on a similarity measure.</p>
<p>It is mostly used to classify a data point based on how its neighbors are classified.</p>
<p>Requirements:</p>
<ul>
<li>A training set</li>
<li>A metric to calculate the distance between records</li>
<li>The value of k (the number of neighbors to be used)</li>
</ul>
<p>The classification process calculates the distance to the records in the training set, it identifies k nearest neighbors and uses nearest neighbor class labels to determine the class of the unknown record.</p>
<p>The choice of k is important because:</p>
<ul>
<li>If k is too small, the approach is sensitive to noise</li>
<li>If k is too large, the surround may include examples belonging to other classes</li>
</ul>
<p><em>Remember to normalize attributes in pre-processing, because to operate correctly, they should have the same scale of values.</em></p>
<p><strong>Pros of KNN:</strong></p>
<ul>
<li>Do not require the construction of a model</li>
<li>Compared with rule-based or decision tree systems, they allow the construction of nonlinear class (more flexible)</li>
</ul>
<p><strong>Cons of KNN:</strong></p>
<ul>
<li>Require a similarity or distance measure to assess closeness</li>
<li>Require a pre-processing step to normalize the range of variation of attributes</li>
<li>Class is locally determined and therefore susceptible to data noise</li>
<li>Very sensitive to the presence of irrelevant or related attributes that will distort distances between objects</li>
<li>Classification cost can be high and depends linearly on the size of the training set in the absence of appropriate index structures</li>
</ul>
<h2 id="data-mining-classifiers-instance-based-the-r-tree-index-structure">The R-Tree Index Structure</h2>
<p>R-trees are extensions of B+-trees to multi-dimensional spaces:</p>
<ul>
<li>B+-trees organize objects into a set of non-overlapping one-dimensional intervals, applying this principle recursively from the leaves of the root</li>
<li>R-trees organize objects into a set of overlapping multi-dimensional intervals, applying this principle recursively from the leaves to the root</li>
</ul>
<p><img alt="" src="data-mining/classifiers/rtree.jpg" /></p></section><section class="print-page" id="data-mining-classifiers-bayesian-classifier"><h1 id="data-mining-classifiers-bayesian-classifier-bayesian-classifier">Bayesian Classifier</h1>
<p>It is a probabilistic approach to solving classification problems.
In many applications, the relationship between attribute values and that of the class is not deterministic, due to noise data, hidden variables and difficulty in quantifying certain aspects.</p>
<ul>
<li><strong>Uncertainty about the outcome prediction</strong></li>
</ul>
<p>Bayesian classifier model probabilistic relationships between attributes and the classification attribute.</p>
<p><img alt="" src="data-mining/classifiers/stats.jpg" /></p>
<h2 id="data-mining-classifiers-bayesian-classifier-naive-bayes">Nave Bayes</h2>
<p>The main advantage of probabilistic reasoning over logical reasoning lies in the possibility of arriving at rational descriptions even when there is not enough deterministic information about how the system works.</p>
<p>This classifier is <strong>robust</strong> toward irrelevant attributes. </p>
<p>It provide optimal results if:</p>
<ul>
<li>The conditional independence condition is met</li>
<li>The probability distributions of P(X|Y) are known</li>
</ul>
<p><img alt="" src="data-mining/classifiers/naive.jpg" /></p>
<p>13.5 is the solution that minimizes the error.</p>
<h2 id="data-mining-classifiers-bayesian-classifier-probability-with-continuous-attributes">Probability with Continuous Attributes</h2>
<p>In case attribute A is continuous, it is not possible to estimate probability for each of its values.</p>
<p>We need to <strong>discretize</strong> the attribute into intervals by creating an ordinal attribute.
If too many intervals are used, the limited number of training set event per interval makes the prediction unreliable. </p>
<p>We <strong>associate</strong> the attributes with a density function and estimate the parameters of the function from the training set to estimate P(A|C).</p></section><section class="print-page" id="data-mining-classifiers-multiclassifier"><h1 id="data-mining-classifiers-multiclassifier-multi-classifier">Multi-Classifier</h1>
<p>Construct multiple base classifiers and predict the class to which a record belongs by aggregating the classification obtained.</p>
<p><strong>How to build a composite classifier</strong></p>
<ul>
<li>Changing the training set by building more training set from the given one</li>
<li>Change the attributes (random forest)</li>
<li>Changing the classes considered (translate a multi-class classification into a binary one)</li>
<li>Change the parameters of the learning algorithm</li>
</ul>
<h2 id="data-mining-classifiers-multiclassifier-error-decomposition">Error Decomposition</h2>
<p>Classifiers make mistakes in predictions, due to:</p>
<ul>
<li><strong>Bias</strong>: ability of the chosen classifier in modeling events and extending the prediction to events not in the training set</li>
<li><strong>Variance</strong>: capability of the training set in representing the actual data set</li>
<li><strong>Noise</strong>: non-determinism of the classes to be determined</li>
</ul>
<p>Different types of classifiers have inherently different capabilities in modeling the edges of regions. 
The difference between the true separation line and the average separation line represents the classifier bias.</p>
<h2 id="data-mining-classifiers-multiclassifier-bagging-variance">Bagging (variance)</h2>
<p>Bagging allows the construction of compound classifiers that associate an event with the highest rated class from the base classifiers.</p>
<p>Each classifier is constructed by <strong>bootstrapping</strong> the same training set.</p>
<p><em>bootstrapping: any test or metric that uses random sampling with replacement</em></p>
<p><img alt="" src="data-mining/classifiers/bagging.jpg" /></p>
<p>Bagging determines the behavior of a two-level decision tree.</p>
<p><img alt="" src="data-mining/classifiers/tree.jpg" /></p>
<h2 id="data-mining-classifiers-multiclassifier-random-forest">Random Forest</h2>
<p>It is a bagging method in which base classifiers are decision trees:
For each node in the decision tree, the split attribute is chosen on a random subset of features rather than on the entire set of features.</p>
<p>Random forest performs two types of bagging: one on the training set and one on the feature set.</p>
<h2 id="data-mining-classifiers-multiclassifier-boosting">Boosting</h2>
<p>An iterative approach to progressively adjust the composition of the training set in order to focus on incorrectly classified records.</p>
<ul>
<li>Initially, all N records have the same weight (1/N)</li>
<li>Unlike bagging, the weights can change at the end of the boosting round in order to increase the probability of the record being selected in the training set</li>
</ul>
<p>The final result is obtained by combining the result the predictions made by the different classifiers.</p>
<p>One of the most widely used boosting techniques is <strong>AdaBoost:</strong></p>
<p>AdaBoost complex on the most complex part of the dataset.</p>
<p><img alt="" src="data-mining/classifiers/ada.jpg" />
<img alt="" src="data-mining/classifiers/boost.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Classifier Models</h1>
                        <h1 class='nav-section-title' id='section-association-rule'>
                            Association Rule <a class='headerlink' href='#section-association-rule' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-association-rule-association-rule"><h1 id="data-mining-association-rule-association-rule-association-rule">Association Rule</h1>
<p>The idea of association rule is <strong>basket analysis</strong>.
We have a set of transactions (set of elements coming from a huge set).</p>
<p>A classic example of association rule coming from <em>data mining literature</em> is the association {Diaper} -&gt; {Beer}</p>
<h2 id="data-mining-association-rule-association-rule-applications">Applications</h2>
<ul>
<li>Marketing sales promotion (understand which products could be affected in the event that the store interrupted the sale of a specific product)</li>
<li>Arrangement of goods (to identify the products bought together by a sufficiently large number of costumers)</li>
</ul>
<p><img alt="" src="data-mining/association-rule/itemset.jpg" /></p>
<p><img alt="" src="data-mining/association-rule/ass-rule.jpg" /></p>
<p><strong>Problem Formulation</strong></p>
<p>Given a set of transactions T, you want to find all transactions such that:</p>
<ul>
<li>Support &gt;= <em>minsup</em></li>
<li>Confidence &gt;= <em>minconf</em></li>
</ul>
<p><strong>Naive Approach:</strong></p>
<ul>
<li>Lists all possible association rules</li>
<li>For each rule calculates support and confidence</li>
<li>Eliminate rules that do not meet thresholds for <em>minsup</em> and <em>minconf</em></li>
</ul>
<p><img alt="" src="data-mining/association-rule/problem.jpg" /></p>
<p>All rules are binary partitions of the same itemset: {Milk, Diaper, Beer} and rules based on the same itemset have the same support but they may have different confidence.</p>
<p>Searching association rules follow a two-steps approach:</p>
<ol>
<li>Generate <strong>frequent itemsets</strong></li>
<li>Rule generation (for each itemset, generate the rules with high confidence. Each rule is a binary partitioning of the elements in the itemset)</li>
</ol>
<h2 id="data-mining-association-rule-association-rule-frequent-itemsets-generation">Frequent Itemsets Generation</h2>
<p>Frequent itemsets can be identified following the <strong>apriori principle:</strong></p>
<p>If an itemset is frequent, then all its sub-sets must be frequent too.</p>
<ul>
<li>The support of an itemset does not exceed the support of its subsets</li>
<li>This is known as the anti-monotonic property of the support</li>
</ul>
<p>A frequent itemset is <strong>maximal</strong> if no one of its adjacent superset is frequent.
On the other hand, an itemset is <strong>closed</strong> if none if its adjacent superset has the same support.</p>
<p><strong>Closed vs Maximal itemsets</strong></p>
<p>From an efficiency point of view:</p>
<ul>
<li>They provide a more compact representation than frequent ones, which is relevant when space is an issue.</li>
<li>Only closed itemsets determine a lossless compression of frequent patterns, which contain complete information regarding the frequent itemsets but closed itemsets are fewer in number than frequent itemsets.</li>
<li>From the semantic point of view, maximal itemsets are the most complex while closed ones can be interesting if they are supported by groups with largely different support</li>
</ul>
<h2 id="data-mining-association-rule-association-rule-rule-generation">Rule Generation</h2>
<p>Given a frequent itemset L, find all the non-empty subsets f  L such that f -&gt; L - f that fulfills the minimum confidence constraint.</p>
<p>The confidence measure does not have the property of anti-monotonicity with respect to the overall associative rule.
However, it is possible to take advantage of the anti-monotonicity of the confidence with respect to the left-hand side of the rule.</p>
<h2 id="data-mining-association-rule-association-rule-interestingness">Interestingness</h2>
<p>The <strong>objective measure</strong> is to prioritize rules based on statistical criteria calculated from data.
The <strong>subjective measure</strong> is to prioritize rules based on user-defined criteria.</p>
<p>According to the latter, a patter is interesting if:</p>
<ul>
<li>It contradicts the users' expectations</li>
<li>The user is interested in performing some activities or making decisions regarding its elements</li>
</ul>
<h2 id="data-mining-association-rule-association-rule-dataset-support">Dataset Support</h2>
<p><strong>Inhomogeneous Support</strong></p>
<p>Many datasets have itemsets with very high support along with others with very limited support.
A large commerical chain sells products with price ranges from 1 to 10,000. The number of transactions that include products with low price is much higher than those with high price. However, the associations among them are of interest of the company.</p>
<p>Setting the <em>minsup</em> threshold fot these datasets can be very difficult.</p>
<p><strong>Cross-support Pattern</strong></p>
<p>A cross-support pattern is an itemset x = {i...1n} where the support ratio r(x) is lower than a threshold <em>thr</em>.</p>
<p><img alt="" src="data-mining/association-rule/cross-support.jpg" /></p>
<h2 id="data-mining-association-rule-association-rule-confidence-limits">Confidence Limits</h2>
<p>The case of cross-support patterns has shown the limits of support.</p>
<p>The confidence limitations is due to the fact that it does not consider the itemset support in the right.hand side of the rule and therefore, it does not provide a correct assessment in the case where the item groups are not stochastically independent.</p>
<p>The <strong>lift</strong> value of an association rule is the ratio of the confidence of the rule and the <strong>expected confidence</strong> of the rule.</p>
<h2 id="data-mining-association-rule-association-rule-handling-categorical-and-continuous-attributes">Handling Categorical and Continuous Attributes</h2>
<p>In its basic formulation, association rule works with binary and asymmetric variables.</p>
<p><strong>Binarization</strong> is needed to transform categorical attributes into asymmetric binary attributes by introducing a new item for every possible attribute value.</p>
<p>Association rules that include attributes with continuous values are called <strong>quantitative association rules.</strong>
Continuous attributes can be handled through several approaches:</p>
<ul>
<li>Based on discretization</li>
<li>Based on statistics</li>
<li>Without discretization </li>
</ul>
<p>The discretization poses the problem of how to fix the number and the border ranges.
The number of intervals is usually supplied by the users and can be expresses in terms of:</p>
<ul>
<li>Range of intervals (equi-distant discretization)</li>
<li>Average number of transactions per interval (equi-depth discretization)</li>
<li>Cluster number</li>
</ul>
<p>The choice of the width of the intervals affects the value of support and confidence:</p>
<ul>
<li>Too large intervals reduce confidence</li>
<li>Too narrow intervals reduce support and tend to determine replicated rules</li>
</ul>
<p><img alt="" src="data-mining/association-rule/continuous.jpg" /></p>
<p>One possible solution is to try all possible intervals (brute force).</p>
<h2 id="data-mining-association-rule-association-rule-multi-level-association-rules">Multi-level Association Rules</h2>
<p>A hierarchy of concepts composed by generalization based on the semantics of its elements.</p>
<p>The higher the level, the higher the support, leading to <strong>generic rules</strong>.</p>
<p>Hierarchies of concepts are incorporated for the following reasons:</p>
<ul>
<li>Rules at the lower levels may not have sufficient support to appear in frequent itemsets</li>
<li>Rules at lower levels may be too specific</li>
</ul>
<p>Multi-level associative rules can be handled with the algorithms already studied by extending each transaction with the parent items of items in the transaction.</p>
<p>Adding details (skimmed milk, white bread) does not add any value and increases complexity.</p>
<p><img alt="" src="data-mining/association-rule/mutli-level.jpg" /></p></section><section class="print-page" id="data-mining-association-rule-sequential-pattern"><h1 id="data-mining-association-rule-sequential-pattern-sequential-pattern">Sequential Pattern</h1>
<p>Often, temporal information is associated with transactions, allowing events concerning a specific subject to be linked together.</p>
<p>A <strong>sequence</strong> is an ordered list of elements, each of which contains a set of events (items).
Each item is associated with a specific time instant or ordinal position.</p>
<p>The length of the sequence is given by the number of elements in it.</p>
<h2 id="data-mining-association-rule-sequential-pattern-sub-sequence">Sub-sequence</h2>
<p>We have sequences (like ordered purchase list from the same customer) and <strong>sub-sequences</strong> are sequences contained in a sequence where the mapping respects the order.</p>
<p><img alt="" src="data-mining/association-rule/sequence.jpg" /></p>
<p>The support of a sub-sequence w is defined as the fraction of sequences that contain w.</p>
<p><strong>Mining Sequential Pattern</strong></p>
<p>Given a database of sequences and a minimum support threshold, <em>minsup</em> find all subsequences whose support is &gt;= <em>minsup</em></p>
<p><strong>Apriori Principle</strong> can be applied to sequential pattern mining since any sequence s that contains a particular k-sequence must contain all (k - 1) subsequences of s.</p>
<p>The steps to follow in this process, include:</p>
<ol>
<li>Run an initial scan of the sequence DB to locate all 1-sequence</li>
<li>Repeat until new frequent sequences are discovered<ul>
<li>Candidate generation: find pairs of frequent subsequences found in step k-1 to generate candidate sequences containing k items</li>
<li>Candidate pruning: eliminate k candidate sequences that contain (k-1) subsequences that are not frequent</li>
<li>Support counting: scan the DB to find the support of candidate sequences</li>
<li>Candidate elimination: eliminate candidate k-sequences whose support is actually less than <em>minsup</em></li>
</ul>
</li>
</ol>
<p><img alt="" src="data-mining/association-rule/candidate.jpg" /></p>
<p>Searching for sequential patterns is a difficult problem given the exponential number of subsequences contained in a sequence.</p>
<h2 id="data-mining-association-rule-sequential-pattern-temporal-constraints">Temporal Constraints</h2>
<p>Temporal constraints increase the expressiveness of sequential pattern by better defining their structure.</p>
<ul>
<li><strong>MaxSpan</strong> defines the maximum time interval between the first and the last sequence element</li>
<li><strong>MinGap</strong> defines the minimum gap between events belonging to two different elements</li>
<li><strong>MaxGap</strong> defines the maximum gap between events belonging to two consecutive elements</li>
</ul>
<p><strong>Sequence Mining with Temporal Constraints</strong></p>
<p>Temporal constraints impact on sequence supports as some patterns counted as frequent may not be true because some of the sequences in their support may violate a time constraint.</p>
<p>It is necessary to modify the counting technique to account for this problem.</p>
<p>The <strong>Time Window Size</strong> (ws) conversely relaxes the support basic definition as it specifies the interval within which two events occurring at different times should be considered simultaneous.</p></section><section class="print-page" id="data-mining-association-rule-outlier"><h1 id="data-mining-association-rule-outlier-outlier-detection">Outlier Detection</h1>
<p>An <strong>anomaly</strong> is a pattern in the data that does not conform to expected behavior.</p>
<p>Anomalies can be caused by different aspects:</p>
<ul>
<li>Data from different classes (an object may be different because it belongs to different class).</li>
<li>Natural variations (many phenomena can be modeled with probabilistic distributions in which there is a probability that a phenomenon with very different characteristics from others will occur).</li>
<li>Measurement errors (due to human or device errors).</li>
</ul>
<p>We can identify different types of anomalies:</p>
<ol>
<li><strong>Spot</strong> anomaly (an individual data instance is anomalous with respect to data)</li>
<li><strong>Contextual</strong> anomaly (a single instance of data is anomalous within a context)</li>
<li><strong>Collective</strong> anomaly (a set of related instances is anomalous and requires a relationship between data instances)</li>
</ol>
<h2 id="data-mining-association-rule-outlier-outliers-application">Outliers application</h2>
<p>Data from different classes, for example, can be used to identify a different purchase pattern followed by fraudsters who stole a credit card.
Also, intrusion detection can be used to monitor events occurring in a computer system and analyze them for intrusion.</p>
<p>In the healthcare sector, outliers can be useful to detect abnormal data, disease outbreaks or instrumentation errors.</p>
<p>Finally, in the industrial sector, anomalies detection can be used to identify failures and malfunctions in complex industrial systems, intrusions in security systems, suspicious events in video surveillance and abnormal energy consumption.</p>
<p>Anomaly detection follows different approaches:</p>
<ul>
<li><strong>Supervised</strong> anomaly detection, where labels are available for both normal and anomaly data.</li>
<li><strong>Semi-supervised</strong> anomaly detection, where labels are available for <em>normal</em> data.</li>
<li><strong>Unsupervised</strong> anomaly, where labels are not available and validation is complex since the real anomaly number is unknown.</li>
</ul>
<p><strong>Anomaly Detection Outputs</strong></p>
<ul>
<li><strong>Label</strong>, each test instance is assigned a label which is the outcome of classification-based approaches.</li>
<li><strong>Score</strong>, each test instance is assigned an anomaly score which allows the instance to be sorted</li>
</ul>
<p>Anomalies are rare events which make it difficult to label these with high accuracy.
<strong>Swamping</strong> is the error of labelling normal events as anomalies while <strong>masking</strong> is the error of labelling anomalous events as normal.</p></section><h1 class='nav-section-title-end'>Ended: Association Rule</h1>
                        <h1 class='nav-section-title' id='section-clustering'>
                            Clustering <a class='headerlink' href='#section-clustering' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-clustering-clustering"><h1 id="data-mining-clustering-clustering-clustering">Clustering</h1>
<p>Clustering analysis aims at finding groups of objects such that objects that belong to the same group are more similar to each other than objects belonging to different groups.</p>
<p>CLustering is NOT supervised classification (it assumes classes to be known) or segmentation (partition rule is given) or querying a database (the selection and grouping criteria are given).</p>
<h2 id="data-mining-clustering-clustering-types-of-clustering">Types of clustering</h2>
<p>We can distinguish between:</p>
<ul>
<li><strong>Partitioning clustering</strong>: a division of objects into non-overlapping subsets (clusters), in which each object belongs exactly to a cluster.</li>
<li><strong>Hierarchical clustering</strong>: a set of nested clusters organized as a hierarchical tree.</li>
<li><strong>Exclusive vs non-exclusive</strong>: in non-exclusive clustering, points can belong to multiple clusters.</li>
<li><strong>Fuzzy vs non-fuzzy</strong>: in a fuzzy clustering a point belongs to all clusters with a weight between 0 and 1.</li>
<li><strong>Partial vs complete</strong>: in a partial clustering, some points may not belong to any of the clusters.</li>
<li><strong>Heterogeneous vs homogeneous</strong>: in a heterogenous cluster, clusters can have very different sizes, shapes and densities.</li>
</ul>
<p>Similarly, we can identify different types of <strong>clusters</strong>:</p>
<ul>
<li><strong>Well-separated clusters</strong>: each point in the cluster is closer (more similar) to any other point in the cluster than any other point that does not belong to the cluster.</li>
<li><strong>Center-based clusters</strong>: a point in the cluster is closer to the <em>center</em> of the cluster, rather than to the center of each other cluster.<ul>
<li>Cluster center = <strong>centroid</strong></li>
</ul>
</li>
<li><strong>Contiguous clusters</strong> (nearest neighbor): a point in a cluster is closer to one or more other points in the cluster than to any point not in the cluster.</li>
<li><strong>Density-based clusters</strong>: a cluster is a dense region of points, which is separated by low-density regions from other regions of high density.</li>
<li><strong>Conceptual clusters</strong>: clusters with shared properties or in which the shared property derives from the whole set of points.</li>
</ul>
<h2 id="data-mining-clustering-clustering-k-means-clustering">K-means Clustering</h2>
<p><img alt="" src="data-mining/clustering/k-means.jpg" /></p>
<p>Initial centroids are often chosen randomly (clusters produced vary from one run to another).</p>
<p><img alt="" src="data-mining/clustering/sse.jpg" /></p>
<h2 id="data-mining-clustering-clustering-converge-and-optimality">Converge and Optimality</h2>
<p>There is only a finite number of ways to partition n records into k groups.
So, there is only a finite number of possible configurations in which all the centers are centroids of the points they possess.</p>
<p>If there are K real clusters, the probability  of choosing a centroid from each cluster is very limited.</p>
<p>Some solutions to this problem, include:</p>
<ul>
<li>Run the algorithm several times with different centroids.</li>
<li>Perform a sampling of the points and use a hierarchical clustering to identify k initial centroids.</li>
<li>Select more than k initial centroids and then select the ones to use from these.</li>
<li>Use post-processing techniques to eliminate the identified erroneous cluster.</li>
<li>Bisecting K-means (less affected by the problem).</li>
</ul>
<p><strong>Handling empty clusters</strong></p>
<p>The K-means algorithm can determine empty clusters if, during the assignment phase, no element is assigned to a centroid.</p>
<p>In this case, different strategies can be used to identify an alternative centroid:</p>
<ul>
<li>Choose the item that most contributes to the value of SSE (sum of squared errors).</li>
<li>Choose an item of the cluster with the highest SSE (the cluster will split into two clusters that include the closest elements).</li>
</ul>
<p><strong>Handling Outliers</strong></p>
<p>The goodness of clustering can be negatively influenced by the presence of outliers that tend to shift the cluster centroids so that to reduce the increase in the SSE they determine.</p>
<p>Outliers, if identifies, can be eliminated during <em>preprocessing.</em></p>
<p><strong>Limits of K-means</strong></p>
<p>The k-means algorithm does not achieve good results when natural clusters have:</p>
<ul>
<li>Different size (the value of SSE to the identification of centroids so as to have clusters of the same size if the clusters are not well-separates)</li>
<li>Different density (more dense clusters lead to smaller intra-cluster distances, so less dense areas require more medians to minimize the total value of SSE)</li>
<li>Non-globular shape (SSE is based on an Euclidean distance that does not take into account the shape of objects)</li>
<li>Data contains outliers</li>
</ul>
<p><strong>Possible Solutions:</strong></p>
<ul>
<li>Use a higher k value, thus identifying portions of clusters</li>
<li>The definition of natural clusters then requires a technique to bring together the identified clusters</li>
<li><strong>Elbow method</strong>: execute k-means several times with increasing values for k</li>
</ul>
<p>With the elbow method, in the beginning the error decreases but then, at some point, the curve will be flattened as we dropping some intra-clusters.</p>
<p>The natural number of clusters is located in the point in which we have the <em>elbow</em>.</p>
<p><img alt="" src="data-mining/clustering/exercise.jpg" /></p>
<h2 id="data-mining-clustering-clustering-hierarchical-clustering">Hierarchical Clustering</h2>
<p>Hierarchical clustering produces a set of nested clusters organized as a hierarchical tree (it can be visualized as a dendrogram).</p>
<p>There are two approaches to build a hierarchical clustering:</p>
<ol>
<li><strong>Agglomerative</strong> (start with the points as individual clusters and, at each step, merge the closest pair of clusters until only one cluster is left).</li>
<li><strong>Divisive</strong> (start with one, all-inclusive cluster and, at each step, split the cluster until each cluster contains an individual point).</li>
</ol>
<p>The key operation is the computation of the proximity of two clusters.
Different approaches to defining the distance between clusters distinguish the different algorithms.</p>
<p><strong>Inter-cluster Distances</strong></p>
<ul>
<li>MIN: minimum distance between two cluster points</li>
<li>MAX: maximum distance between all cluster points</li>
<li>Group Average: average distance between all the cluster points</li>
</ul>
<p><img alt="" src="data-mining/clustering/min.jpg" /></p>
<p>Using MIN links, we can have non-globular clusters.</p>
<p><img alt="" src="data-mining/clustering/max.jpg" /></p>
<p>Having the MAX distance as reference distance, we will get more globular clusters and once they are put together, they cannot be split (greedy).</p>
<p><strong>Computation Complexity</strong></p>
<p>With hierarchical clustering:</p>
<ul>
<li>O(N^2) is the space occupied by the proximity matrix when the number of points is N.</li>
<li>O(N^3), where N refers to the steps needed to build the dendrogram.<ul>
<li>At each step, the proximity matrix must be updated and read</li>
<li>Prohibitive for large datasets</li>
</ul>
</li>
</ul>
<h2 id="data-mining-clustering-clustering-dbscan">DBSCAN</h2>
<p>DBSCAN is density based approach where density refers to the number of points within a specific radius.</p>
<p><img alt="" src="data-mining/clustering/dbscan.jpg" /></p>
<p>A point is a <strong>score point</strong> if it has at least a specified number of points within Eps (points that are the interior of a cluster).</p>
<p>A <strong>border point</strong> is not a core point but is in the neighborhood of a core point.</p>
<p>A <strong>noise point</strong> is any point that is not a core point o a border point.</p>
<div class="highlight"><pre><span></span><code>DBSCAN algorithm

// Input: Dataset D, MinPts, Eps
// Output: set of cluster C

Label points in D as core, border or noise
Drop all noise points
Assign to cluster c the core points with a distance &gt; Eps from one of the other points assigned to the same cluster
Assign border points to one of the clusters the corresponding core points belong to
</code></pre></div>
<p><strong>Pros and Cons of DBSCAN:</strong></p>
<ul>
<li>Pros:<ul>
<li>Resistant to noise</li>
<li>It can generate clusters with different shapes and sizes</li>
</ul>
</li>
<li>Cons:<ul>
<li>Data with high dimensionality </li>
</ul>
</li>
</ul>
<h2 id="data-mining-clustering-clustering-cluster-validity">Cluster Validity</h2>
<p>For supervised classification techniques, there are several measures to evaluate the validity of the results based on the comparison between the known labels of the test set and those calculated by the algorithm.</p>
<p><strong>Validity Measures:</strong></p>
<p>Numerical measures that are applied to judge various aspects of cluster validity, are classified into the following three types:</p>
<ul>
<li>Internal index (used to measure the goodness of a clustering structure without respect to external information)</li>
<li>External index (used to measure the extent to which a cluster labels match externally supplied class labels)</li>
<li>Relative index (used to compare two different clusters)</li>
</ul>
<p><strong>Internal Measures:</strong></p>
<ul>
<li>Custer Cohesion (measures how closely related are objects in a cluster - SSE)</li>
<li>Cluster Separation (measure how distinct or well-separated a cluster is from other clusters)</li>
</ul>
<p><img alt="" src="data-mining/clustering/measures.jpg" /></p>
<p>Validity can be measured via <strong>correlation:</strong></p>
<ul>
<li>Two matrices are used<ul>
<li>Proximity Matrix</li>
<li>Incidence Matrix</li>
</ul>
</li>
<li>Compute the correlation between the two matrices</li>
<li>High correlation indicates that points that belong to the same clusters are close to each other</li>
<li>Not a good measure for some density based clusters</li>
<li>Correlation between the incidence matrix and the proximity matrix on the results of the k-means algorithm on two data sets</li>
</ul>
<h2 id="data-mining-clustering-clustering-cophenetic-distance">Cophenetic Distance</h2>
<p><img alt="" src="data-mining/clustering/coph.jpg" /></p>
<p>To define weather the measures obtained are good or bad, we need to define some KPIs obtained by comparing our results with the results obtained with random data.</p>
<p>We are looking for non-random patterns, so, the more atypical the result we get is, the more likely it is to represent a non-random pattern in the data.</p>
<p>The issue of interpreting the measure value is less pressing when comparing the results of two clustering.</p>
<p><img alt="" src="data-mining/clustering/hopkins.jpg" /></p>
<p><strong>External measures for clustering validation:</strong></p>
<p>External information is usually the class labels of the objects on which clustering is performed.
They allow you to measure the correspondence between the computed eticle of the cluster and the class label.</p>
<p>If class labels are available, we perform clustering to compare the results of different clustering techniques and evaluate the possibility of automatically obtaining an otherwise manual classification.</p>
<p>Two approaches are possible:</p>
<ul>
<li>Classification-oriented (evaluate the extent to which clusters contain objects belonging to the same class)</li>
<li>Similarity-oriented (they measure how often two objects to the same cluster belong to the same class)</li>
</ul></section><h1 class='nav-section-title-end'>Ended: Clustering</h1>
                        <h1 class='nav-section-title' id='section-machine-learning'>
                            Machine Learning <a class='headerlink' href='#section-machine-learning' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-introduction-introduction"><h1 id="machine-learning-introduction-introduction-introduction-to-machine-learning">Introduction to Machine Learning</h1>
<p>What are the main features of intelligence?</p>
<p><em>Intelligence is a very general mental capability that, among the other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.</em></p>
<p><strong>Artificial Intelligence</strong> is a huge set of disciplines which also includes machine learning.
With machine learning, we refer only to a <a href="https://medium.com/@terdsaksu/artificial-intelligence-machine-learning-deep-learning-a2ebd43ff1b2">small subset</a> inside artificial intelligence.</p>
<p><img alt="" src="machine-learning/introduction/ai.jpg" /></p>
<p>We, as humans, take many activities for granted that for machines would be very complex. 
Simulating human intelligence is extremely complex as our brain in an incredibly sophisticated machine, of which we still know few aspects.</p>
<h2 id="machine-learning-introduction-introduction-impact-of-ai-in-our-world">Impact of AI in our world</h2>
<p><img alt="" src="machine-learning/introduction/tractica.jpg" /></p>
<p>For both graphs, we can note the exponential trend and the variety of continents covered. </p>
<p><em>Information is the oil of 21st century, and analytics is the combustion engine</em></p>
<p>The number of requests per position as data scientist is constantly increasing as companies need to extract knowledge from data to survive.</p>
<p>The revolution introduced by AI is reflected also in companies. At least 5 of the top 10 world companies are directly related to AI.
Also, at least 2 companies are directly related to the production of chips, key elements for AI.</p>
<p><strong>Nvidia</strong></p>
<p>It is a software and fabless company that design GPUs, which nowadays are essential to:</p>
<ul>
<li>Create AI models</li>
<li>Perform High Performance Computing (HPC)</li>
</ul>
<p>Nvidia is the leading company in the sector and this is the reason why its shares has risen significantly in recent years.</p>
<h2 id="machine-learning-introduction-introduction-the-general-paradigm-of-machine-learning">The General Paradigm of Machine Learning</h2>
<p><strong>Machine Learning</strong> is a subset of the AI field that tries to develop systems able to automatically learn from specific examples (<em>training data</em>) and to generalize the knowledge on new samples (<em>testing data</em>) of the same domain.</p>
<p>From a practical point of view:</p>
<ol>
<li>We have some data which represents our application domain</li>
<li>We implement an algorithm able to learn from the data (training phase)</li>
<li>We use data to understand if the trained model has learned something -&gt; <strong>model deployment</strong></li>
</ol>
<p>The main steps for the development of intelligent systems based on ML:</p>
<p><img alt="" src="machine-learning/introduction/stps.jpg" /></p>
<p><strong>Data Acquisition</strong></p>
<p>Data is the founding element of any application related to ML. Acquiring large amounts of data is one of the main concerns for top-companies today.</p>
<p><strong>Data Processing</strong></p>
<p>All those techniques with which data are processed in order to adapt them to the best of the ML model that we plan to develop.</p>
<p><strong>Model</strong></p>
<p>This is the main core of AI systems. A model can be seen as a set of mathematical and statistical technique able to learn from a certain distribution of data provided in input and to generalize on new data.</p>
<p><strong>Prediction</strong></p>
<p>It can take many forms depending on the application developed. It is the output of the model and it is important to evaluate the effectiveness of the developed system.</p></section><section class="print-page" id="machine-learning-introduction-history"><h1 id="machine-learning-introduction-history-historical-evolution-of-ai">Historical Evolution of AI</h1>
<p>To understand why AI is so important today, we have to analyze the past.</p>
<p>In <strong>1950</strong> the enthusiasm for AI began:</p>
<ul>
<li><strong>Turing Test</strong>: <em>"Can machines think?"</em></li>
<li>1954: one of the main experiments in machine translation</li>
<li>1955: Arthur Samuel wrote a program that could play checkers very well</li>
<li>1957: Rosenblatt invented perceptrons, a type of neural network</li>
</ul>
<p><strong>First AI Winter</strong>  - promises of AI were exaggerated</p>
<p>In <strong>1980</strong> the Boom times occurred:</p>
<ul>
<li>Commercialization of new AI Expert Systems capable of reproducing human decision-making, through <em>"if-then-else"</em> rules</li>
<li>Financial planning, medical diagnosis, geological exploration, and microelectronic circuit design</li>
</ul>
<p><strong>Second AI Winter</strong> - many tasks were too complicated for engineers</p>
<p>In <strong>2012</strong> the Deep Learning revolution took place</p>
<ul>
<li>Solved mathematical problems</li>
<li>New powerful Neural Networks</li>
<li>Huge improvement with the computational power</li>
<li>Introduction of GPUs</li>
</ul>
<p>Problem with <strong>data</strong></p>
<ul>
<li>AI models need huge amount of training data</li>
<li>Currently, we are able to:<ul>
<li>Acquire a lot of data (IoT)</li>
<li>Store huge amount of data (improved storage)</li>
</ul>
</li>
</ul>
<p>Today, the question is not if we are able to collect data, but if we are able to use them.</p>
<p><img alt="" src="machine-learning/introduction/data.jpg" /></p>
<p><img alt="" src="machine-learning/introduction/future.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Machine Learning</h1>
                        <h1 class='nav-section-title' id='section-data-acquisition-and-processing'>
                            Data Acquisition and Processing <a class='headerlink' href='#section-data-acquisition-and-processing' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-data-acquisition-data-acquisition"><h1 id="machine-learning-data-acquisition-data-acquisition-data-acquisition-and-processing">Data Acquisition and Processing</h1>
<p>Data acquisition and processing are the first steps in the Machine Learning pipeline.
This is one of the most important steps for many companies, but acquiring data is a time-consuming, investment and knowledge intensive process.</p>
<p><strong>Big Data</strong>: having large amounts of data available has been one of the reasons for the strong development of machine learning.
We are able to collect large amounts of data thanks to new storing devices and process digitalization.</p>
<h2 id="machine-learning-data-acquisition-data-acquisition-data-acquisition">Data Acquisition</h2>
<p>Data acquisition is the first step in developing a machine learning system. 
We can get data mainly in two ways:</p>
<ol>
<li>By using <strong>publicly available</strong> data (quality must be checked)</li>
<li>By acquiring a new set of <strong>specific</strong> data (generate specific expertise for the company)<ul>
<li>It is not certain that public data well represent the problem we want to solve</li>
<li>We are forced to acquire data that, due to their sensitive nature, would not otherwise available (privacy issues)</li>
<li>The company we work for already has a data collection process that we can use</li>
</ul>
</li>
</ol>
<p><strong>Public Datasets</strong></p>
<p>Many Universities publicly release their datasets. There are no requirements related to profit or non-disclosure agreement.
It is a consolidated practice in the world of research to share data to test the reproducibility of the results obtained.</p>
<p><strong>Acquisition of a new dataset</strong></p>
<p>Acquiring a new dataset is usually a costly process (time and money).</p>
<ul>
<li>Program acquisition tool</li>
<li>Handle large amounts of data</li>
<li>Test to find bugs</li>
<li>New hardware</li>
</ul>
<p>It is necessary to carefully consider whether it is appropriate to acquire a new dataset.</p>
<p><em>Acquiring a new dataset does not mean acquiring only new data</em></p>
<h2 id="machine-learning-data-acquisition-data-acquisition-data-annotation">Data Annotation</h2>
<p>It is one of the most relevant aspect in the data acquisition phase.
It regards the <strong>semantic content</strong> of the data and the label depends on the problem we want to solve.</p>
<p>It can be numerical or categorical and associates a label to data.</p>
<p>Data collection without correct and timely annotation is often useless.
However, it also possible to extract knowledge from un-annotated data through <strong>clustering.</strong></p>
<p><strong>Data Annotation Process</strong></p>
<p>The data annotation process can take place in several ways:</p>
<ul>
<li><strong>Manual</strong> (long and expensive but the quality of the annotations is usually controllable and high)</li>
<li><strong>Automatic</strong> (each data is automatically annotated using specific tools)</li>
<li><strong>Third parties</strong> (all data is noted by a third party)<ul>
<li>Free of charge (free use of a platform in exchange for annotated data)</li>
<li>Paid (purchase annotation time from third parties, usually from developing countries)</li>
</ul>
</li>
</ul>
<p><strong>Closed Set</strong>: the pattern to be classified belongs to one of the known classes</p>
<p><strong>Open Set</strong>: you do not know all the possible annotations, so the pattern to be classified can belong to one of the known classes or to non of these.
You can define a threshold above which a specific pattern is assigned.</p>
<h2 id="machine-learning-data-acquisition-data-acquisition-problems-in-data-acquisition">Problems in Data Acquisition</h2>
<p>Companies usually face common problems:</p>
<ul>
<li>The business process produces huge amounts of data (it is impossible to acquire all the data due to physical limitation)</li>
<li>Sometimes companies have a lot of old data in their databases</li>
<li>In many business processes it is unclear understanding which data is possible to collect or which data is really useful for the business</li>
</ul></section><section class="print-page" id="machine-learning-data-acquisition-data-types"><h1 id="machine-learning-data-acquisition-data-types-data-types">Data Types</h1>
<p>In general, there are 4 types of data:</p>
<ol>
<li><strong>Numerical:</strong><ul>
<li>Values associated with measurable characteristics</li>
<li>Continuous (subject to ordering)</li>
<li>Representable as numerical vectors</li>
</ul>
</li>
<li><strong>Categorical:</strong><ul>
<li>Qualitative characteristics</li>
<li>Presence or absence of a characteristic</li>
<li>Sometimes subject to sorting</li>
<li>Widely used in Data Mining</li>
</ul>
</li>
<li><strong>Sequences:</strong><ul>
<li>Sequential patterns with spatial or temporal relationships</li>
<li>With a variable length</li>
<li>Position in the sequence and relationship with predecessors and successors are important</li>
</ul>
</li>
<li><strong>Structured data:</strong><ul>
<li>Outputs organized in complex structures such as trees and graphs</li>
</ul>
</li>
</ol>
<h2 id="machine-learning-data-acquisition-data-types-images">Images</h2>
<p>An image is a <strong>matrix of values</strong> in which each cell is referred as pixel and each pixel contains the value of the brightness.</p>
<p><img alt="" src="machine-learning/data-acquisition/pixel.jpg" /></p>
<p>In <strong>color images</strong>, each pixel contains 3 values that represent the color components, referred as channels.
The content of channels are related to the color space (the convention used to define colors).</p>
<ul>
<li><strong>RGB color space</strong>: 3 values indicate the value of three components (Red, Gree and Blue).</li>
</ul>
<h2 id="machine-learning-data-acquisition-data-types-image-formats">Image formats</h2>
<p><img alt="" src="machine-learning/data-acquisition/formats.jpg" /></p></section><section class="print-page" id="machine-learning-data-acquisition-data-preparation"><h1 id="machine-learning-data-acquisition-data-preparation-data-preparation">Data Preparation</h1>
<p>Once obtained data for our Machine Learning system, it is necessary to prepare them.</p>
<p>They are organized as follows:</p>
<ul>
<li><strong>Training set</strong> (data on which the model automatically learns during the learning phase)</li>
<li><strong>Validation set</strong> (part of the training set in which hyper-parameters are tuned)</li>
<li><strong>Training set</strong> (data on which the model is tested during testing phase in order to model effectiveness through qualitative and quantitative numerical measures)</li>
</ul>
<p><img alt="" src="machine-learning/data-acquisition/data-prep.jpg" /></p>
<h2 id="machine-learning-data-acquisition-data-preparation-deployment">Deployment</h2>
<p>Once the previous phases have been completed, the ML system can be released for its effective use.</p>
<p>Normally, once the model is released, it no longer goes through training and testing phases.</p>
<p><strong>Different ways to train-val-test</strong></p>
<p>We can identify alternatives approaches adopted by choice or imposed by context:</p>
<ol>
<li><strong>Batch</strong>: the training is carried out only once on a given training set.</li>
<li><strong>Incremental</strong>: following the initial training, further training sessions are possible</li>
<li><strong>Natural</strong>: this is the closest case ti the human learning process</li>
</ol>
<h2 id="machine-learning-data-acquisition-data-preparation-different-ways-of-learning">Different Ways of Learning</h2>
<p>Not all data is always annotated. Depending on whether they are annotated, we can define different types of learning:</p>
<ul>
<li>Annotated data -&gt; <strong>Supervised Learning</strong></li>
<li>Not annotated data -&gt; <strong>Unsupervised Learning</strong></li>
<li>Partially annotated data -&gt; <strong>Semi-supervised Learning</strong></li>
</ul>
<p>Specific algorithms correspond to each of these areas. Usually, the presence of annotations helps and simplifies the development of ML algorithms. The best performances are usually obtained with supervised trained algorithms.</p></section><h1 class='nav-section-title-end'>Ended: Data Acquisition and Processing</h1>
                        <h1 class='nav-section-title' id='section-model'>
                            Model <a class='headerlink' href='#section-model' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-model-model"><h1 id="machine-learning-model-model-model">Model</h1>
<p>The model is the heart of the AI in our system.
It is one of the most delicate and decisive elements of the entire process:</p>
<ul>
<li><strong>Model</strong> -&gt; mechanism with which input data are transformed in outputs</li>
</ul>
<h2 id="machine-learning-model-model-machine-learning-tasks">Machine Learning Tasks</h2>
<p>There are different tasks in ML depending on the output we want:</p>
<ul>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
</ul>
<h2 id="machine-learning-model-model-classification">Classification</h2>
<div class="highlight"><pre><span></span><code>- We have a specific input, a model (classifier) which outputs a class (pattern)
- If there are only 2 classes, we call the problem *binary* classification, while with multiple classes, we have *multi-class* classification
</code></pre></div>
<p><strong>class</strong> = data set having common properties</p>
<p>The concept of <em>label</em> and <em>semantic</em> is related to the concept of class, since it strictly depends on the working context.</p>
<p><strong>Examples of classification:</strong></p>
<ol>
<li>Spam detection<ul>
<li>Input: email texts</li>
<li>Output: yes/no (spam)</li>
</ul>
</li>
<li>Credit card fraud detection<ul>
<li>Input: list of bank operations</li>
<li>Output: yes/no (fraud)</li>
</ul>
</li>
<li>Face recognition<ul>
<li>Input: images</li>
<li>Output: identity</li>
</ul>
</li>
<li>Medical diagnosis<ul>
<li>Input: x-ray images</li>
<li>Output: benign/malignant (tumor)</li>
</ul>
</li>
</ol>
<p><img alt="" src="machine-learning/model/class.jpg" /></p>
<h2 id="machine-learning-model-model-regression">Regression</h2>
<p>Given a specific input, the model (regressor) outputs a continuous value (data -&gt; value).
You can see a regression task as a classification task with a high number of classes</p>
<p><strong>Examples of regression</strong></p>
<ol>
<li>Estimation of a person's height based on weight</li>
<li>Estimated sale prices of apartments in the real estate market</li>
<li>Risk estimation for insurance companies</li>
<li>Energy prediction produced by a photovoltaic system</li>
<li>Health costs prediction models</li>
</ol>
<p><img alt="" src="machine-learning/model/reg.jpg" /></p>
<h2 id="machine-learning-model-model-clustering">Clustering</h2>
<p>Identify groups (clusters) of data with similar characteristics, usually applied in an <strong>unsupervised</strong> learning setting (patterns are not labeled and classes are not known in advance).</p>
<p>Usually, the unsupervised nature of the problem makes it more complex than classification.</p>
<p><strong>Examples of clustering</strong></p>
<ol>
<li>Marketing (user groups)</li>
<li>Genetics (group by DNA)</li>
<li>Bioinformatics (partitioning of genes)</li>
<li>Vision (unsupervised segmentation)</li>
</ol>
<p><img alt="" src="machine-learning/model/cluster.jpg" /></p>
<h2 id="machine-learning-model-model-artificial-vision">Artificial Vision</h2>
<p>For artificial vision domain, we can identify even more specific problems</p>
<p><img alt="" src="machine-learning/model/vision.jpg" /></p></section><section class="print-page" id="machine-learning-model-pattern"><h1 id="machine-learning-model-pattern-pattern-recognition">Pattern Recognition</h1>
<p>Pattern recognition is the discipline that studies the recognition of patterns (data) even with pre-programmed algorithms (not able to learn automatically)</p>
<p>The <strong>model</strong> is a set of <em>hand-crafted instructions</em>.</p>
<p>Technique similar to the <em>Expert System</em> developed in the '80, which was a first form of artificial intelligence.
The ability of a calculator to perform calculations on large amounts of data is exploited.</p>
<p>The programmer develops a series of <strong>instructions</strong> to solve specific problem:</p>
<ul>
<li>These instructions are typically based on <em>if-then-else</em> statements</li>
<li>A strong priori knowledge of the problem is required</li>
</ul>
<p><strong>Problems</strong> that can be faced with explicitly programmed instructions:</p>
<ul>
<li>The conditions are stable and known a priori (constrained industrial environment)</li>
<li>There are mathematical formulas to model the problem</li>
<li>The problem must be limited in dimensionality and not too complex</li>
</ul>
<p><img alt="" src="machine-learning/model/enomaly.jpg" /></p>
<h2 id="machine-learning-model-pattern-explicitly-programmed-instructions">Explicitly Programmed Instructions</h2>
<p>General and technical considerations about pattern recognition:</p>
<ul>
<li>It can achieve a high degree of success if the a priori knowledge is adequate, dimensionality of the problem is limited and the test domain is similar to what was assumed when defining the instructions</li>
<li>The developed solution will inevitably be <strong>specific</strong></li>
<li>There is not a real learning phase</li>
<li>It is possible to understand why the developed system fails in classification</li>
<li>If the problem becomes complicated, the programming time increase</li>
<li>The code risks becoming unmanageable due to:<ul>
<li>High complexity and number of innested statements</li>
<li>Length of code</li>
<li>Too specific functions</li>
</ul>
</li>
</ul>
<p><strong>Limits of programmed instructions</strong></p>
<p><img alt="" src="machine-learning/model/limit.jpg" /></p>
<p>Solving these problems with instructions is very complex. The level of generalization of the proposed solution would be very limited.</p>
<p>It is necessary, when needed, to address the problems with other paradigms.</p></section><h1 class='nav-section-title-end'>Ended: Model</h1>
                        <h1 class='nav-section-title' id='section-classification'>
                            Classification <a class='headerlink' href='#section-classification' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-classification-classification"><h1 id="machine-learning-classification-classification-classification-with-machine-learning">Classification with Machine Learning</h1>
<p><em>How can machine learn?</em></p>
<p>The key element is <strong>data</strong>: machine can learn from data.
This is the reason why data is so relevant today.</p>
<p>Currently, we can program machines that imitate this way of learning of humans.
Humans learn in many different ways and we only imitate just one with machines.</p>
<p><em>Learning from data</em> is similar to humans that learn to paly a musical instrument:</p>
<ul>
<li>Observe how a chord is created (<strong>annotated data</strong>)</li>
<li>Repeat the chord (<strong>iterative learning process</strong>)</li>
<li>Feedback (<strong>loss function</strong>)</li>
</ul>
<p>From a practical point of view, these are required steps:</p>
<ol>
<li>We get the annotated data</li>
<li>We pre-process data (make them suitable for the algorithm)</li>
<li>We iteratively train a classifier</li>
<li>We measure the performance of the implemented solution</li>
</ol>
<p>Machine Learning refers to the discipline that aims to develop systems able to automatically learn from (training) data and to generalize the knowledge on new (testing) data.</p>
<p>A machine learning model makes predictions without being explicitly programmed to do so.</p>
<p>Thanks to machine learning we can avoid complex operations of writing predefined instructions to solve a specific problem.</p>
<h2 id="machine-learning-classification-classification-support-vector-machines-svms">Support Vector Machines (SVMs)</h2>
<p>It is a <strong>supervised</strong> learning method used for classification, regression and outliers' detection.</p>
<p>It is effective with high dimensional inputs and still effective in cases where number of dimensions is greater than the number of samples.</p>
<p><img alt="" src="machine-learning/classification/svm.jpg" /></p>
<p>We have a point with two dimensions (x, y) and two patterns (orange and blue).</p>
<p>SVM identifies a pattern (<strong>hyperplanes</strong>) that divides the cluster in two groups.</p>
<p>Hyperplanes are decision surface, there can be infinite possible solutions but SVM finds the optimal one.</p>
<p><strong>Support vectors:</strong> data points that lie closest to the decision surface</p>
<ul>
<li>Data points most difficult to classify</li>
<li>Directly influence the optimum location of the decision surface</li>
<li>They are the element of the training set that would change the position of the dividing hyperplane if removed</li>
<li>SVMs maximize the margin between support vectors</li>
<li>The decision functions is fully specified by a subset of training samples, the support vectors</li>
<li>This becomes a quadratic programming problem that is easy to solve by standard methods</li>
</ul>
<p><em>What if patterns are not linearly separable?</em></p>
<p>The idea is to still obtain a linear separation by mapping the data to a higher dimensional space.
The mapping procedure is realized through a <strong>kernel function.</strong></p>
<p>If we have more than two classes, we can adopt two solutions:</p>
<ul>
<li><strong>One-Against-One:</strong> classifiers trained on all possible class couples</li>
<li><strong>One-Against-All:</strong> one SVM trained for each class (the SVM that has the better margin decides the final class)</li>
</ul>
<h2 id="machine-learning-classification-classification-linear-and-non-linear-kernel">Linear and Non-linear Kernel</h2>
<ul>
<li>If the dimensionality of the space is very high, linear SVM is generally used</li>
<li>Fow low dimensionality, the primary choice is non-linear SVM with RBF kernel</li>
<li>For medium dimensionality both types are generally tried</li>
</ul>
<p>Remember, the hyperparameter are calibrated on a separate validation set, or through cross validation.</p>
<h2 id="machine-learning-classification-classification-decision-trees">Decision Trees</h2>
<p>Tree-like model to perform the classification.
They are commonly used in operational research, specifically in decision analysis.</p>
<p><img alt="" src="machine-learning/classification/trees.jpg" /></p>
<p>If we add a class, we need to add another decision <strong>node</strong>.</p>
<p><strong>Decision Tree Training</strong></p>
<p>The <strong>root node</strong>:</p>
<ul>
<li>We want a decision that makes a good split (separating classes as much as possible)</li>
<li>Quantify a good split by using a <strong>measure</strong> (Gini index, entropy ..)</li>
<li>Different possible algorithms that recursively evaluate different features and use at each node the feature that best splits the data</li>
</ul>
<p>The <strong>second node</strong>:</p>
<ul>
<li>Let's go the left branch</li>
<li>We use only data that belong to the left branch</li>
<li>We do the same thing we did in the root node</li>
<li>We apply this procedure to all the other nodes</li>
</ul>
<p>We stop the training when the selected measure is not further increased after some iterations.</p>
<p><strong>Ensemble Methods</strong></p>
<p>A multi-classifier is an approach where several classifiers are used together, wither in parallel or in cascade.</p>
<p>It has been shown the use of combinations of classifiers can strongly improve performance.
The combination is effective only when individual classifiers are <strong>independent</strong>.
Unfortunately, it is very difficult to have real independence between classifiers.</p>
<p>Two approaches:</p>
<ul>
<li><strong>Bagging</strong>: I train different classification algorithms on different portions of the training set</li>
<li><strong>Boosting</strong>: I train different algorithms on incorrectly classified patterns</li>
</ul>
<p>How to merge decisions of the individual classifiers:</p>
<ol>
<li>
<p><strong>Decision level</strong></p>
<ul>
<li>Majority vote rule (each classifier vote for a class and the pattern is assigned to the highest rated class)</li>
<li>Borda count (each classifier produces a ranking of the classes, the rankings are converted into scores and the class with the highest final score is the one chosen)</li>
</ul>
</li>
<li>
<p><strong>Confidence Level</strong></p>
<ul>
<li>Each classifier outputs a confidence value, and these values are merged</li>
<li>Weighted sum (the sum of the confidence values is performed by weighting the different classifiers according to their degree of skill)</li>
<li>The sum is often preferable to the product as it is more robust (in the product it is sufficient that a single classifier indicates zero confidence to bring the confidence of the multi classifier to zero)</li>
</ul>
</li>
</ol>
<p><strong>Random Forest</strong> - based on Bagging</p>
<p>The single classifier on which random forest is based is the <strong>decision tree</strong> (hundreds or thousands of DT).</p>
<p>In random forests, we have two types of bagging:</p>
<ul>
<li><strong>Data Bagging</strong> (RF repeatedly selects a random sample with replacement of the training set and fits trees to these samples)</li>
<li><strong>Feature bagging</strong> (in each decision node, the choice of the best feature on which to partition is not made on the entire set of <em>d</em> feature)</li>
</ul>
<p>The final decision is taken upon the <strong>majority vote rule</strong>.</p>
<p><strong>Adaboost</strong> - based on boosting</p>
<p>Several weak classifiers are combined together to obtain a strong classifier.
Differently from bagging, there is an incremental learning phase, at each step a weak classifier is added.</p>
<p><img alt="" src="machine-learning/classification/adaboost.jpg" /></p>
<h1 id="machine-learning-classification-classification-feature-description">Feature Description</h1>
<p>The learning phase is complex with high-dimensionality data as images.
For instance, what if in input we have RGB images?</p>
<p><strong>Feature extraction</strong> refers to the process ot extracting features from data.
A feature is a n-dimensional vector of numerical features that represent (in a discriminative way) some object used as input data.</p>
<p><strong>Example of features:</strong></p>
<p>Object: geometric shape</p>
<ul>
<li>Data: array of values</li>
<li>Features: subset of coordinates or a new value that we can compute from coordinates</li>
</ul>
<p>Object: image</p>
<ul>
<li>Data: matrix of values</li>
<li>Features: subset of pixels or a new value that we can compute from pixels</li>
</ul>
<h2 id="machine-learning-classification-classification-histogram-of-oriented-gradients-hog">Histogram of Oriented Gradients (HOG)</h2>
<p>A visual feature descriptor that can describe the <strong>shape</strong> of an object.
HOG provides the edge direction:</p>
<ul>
<li>The whole image is divided into smaller regions</li>
<li>For each region, the edge directions are calculated<ul>
<li><strong>Edge</strong>: curves at which the brightness changes sharply</li>
<li><strong>Direction</strong>: angle and magnitude of edges</li>
</ul>
</li>
</ul>
<p><img alt="" src="machine-learning/classification/hog.jpg" /></p>
<h2 id="machine-learning-classification-classification-local-binary-pattern-lbp">Local Binary Pattern (LBP)</h2>
<p>A visual feature descriptor that can describe the texture of the surface (visual surface appearance).</p>
<p><img alt="" src="machine-learning/classification/lbp.jpg" /></p></section><section class="print-page" id="machine-learning-classification-metrics"><h1 id="machine-learning-classification-metrics-metrics">Metrics</h1>
<p>The <strong>prediction</strong> of the system is an extremely important step.
It allows you to calculate the performance of the system through metrics, to understand if the system responds correctly with respect to what was designed and desired.</p>
<p>The computation of metrics is also linked to the achievement of certain contractual obligations.</p>
<h2 id="machine-learning-classification-metrics-system-performance-with-classification">System Performance with Classification</h2>
<p>Generally, it is preferred to use a measure linked directly to the semantics of the problem.
The <strong>metric</strong> examines the prediction of the model and the label provided in input (GroundTruth and prediction).</p>
<div class="highlight"><pre><span></span><code>Accuracy = # pattern correctly classified / total patterns
</code></pre></div>
<h2 id="machine-learning-classification-metrics-confusion-matrix">Confusion Matrix</h2>
<p>The confusion matrix is very useful in multi-class classification problems to understand how errors are distributed.</p>
<ul>
<li>Rows: classes of GT</li>
<li>Columns: predicted classes</li>
</ul>
<p>A <em>cell (r,c)</em> shows the percentage in which the system predicts class c for a ground truth class r.</p>
<p>Ideally, the matrix should be diagonal and high values (off diagonal) indicate concentrations of errors.</p>
<p><img alt="" src="machine-learning/classification/confusion.jpg" /></p>
<p>In the example, the class "0" is often confused with "6".</p></section><section class="print-page" id="machine-learning-classification-deep-learning"><h1 id="machine-learning-classification-deep-learning-classification-with-deep-learning">Classification with Deep Learning</h1>
<p>Deep learning is a discipline, similar to ML, that allows you to avoid the problematic phase of feature extraction with high-dimensional input.</p>
<p>Is is based on <strong>neural networks</strong> (NNs) classifier.
The key idea is to imitate, as far as possible, he nature (neurons in the human brain).</p>
<p><img alt="" src="machine-learning/classification/neural.jpg" /></p>
<p>The first Artificial Neuron has been introduced in 1943 by McCulloch and Pitts.
In artificial neuron, there are:</p>
<ul>
<li>Inputs (digital numbers)</li>
<li>Inputs are weighted (not all inputs are equally important)</li>
<li>Inputs are merged with a sum function (plus a <em>bias</em>)</li>
<li>An <strong>activation function</strong> is used to generate the final output</li>
</ul>
<h2 id="machine-learning-classification-deep-learning-activation-function">Activation Function</h2>
<p>The activation function defines the output of that node given an input or set of inputs.
They are a sort of switch of the artificial neuron; they output a small value for small inputs and a larger value if its inputs exceed a threshold.</p>
<p><strong>Liner vs Non-linear problems</strong></p>
<p>A single artificial neuron can solve only <strong>linear</strong> problems.</p>
<p>The solution is to use more ANs organized on different layers (<strong>Multi Layer Perceptron</strong>).
It is not easy as it introduces several mathematical problems, besides, we greatly improve the computational load.</p>
<h2 id="machine-learning-classification-deep-learning-artificial-neural-networks">Artificial Neural Networks</h2>
<p>Groups of artificial neurons are organized in different <strong>layers</strong>: neural networks.</p>
<p>Typically, they present:</p>
<ul>
<li>An input layer (input of the network)</li>
<li>An output layer (output of the network)</li>
<li>One or more <strong>hidden layers</strong></li>
</ul>
<p>Each neuron is <strong>fully connected</strong> with those of the next level.
Again, we try to imitate the hierarchical nature of our neurons:</p>
<ul>
<li>We have only ten levels between the retina and the actuator muscles.</li>
<li>Otherwise, we would be too slow to react to stimuli.</li>
</ul>
<p><strong>ANN Typologies</strong></p>
<ul>
<li><strong>Feedforward</strong> (FF): the connections connect the neurons if one level with the neurons of the next level.<ul>
<li>Connections to the same level and backward connections are not allowed.</li>
<li>It is by far the most used type of network.</li>
</ul>
</li>
<li><strong>Recurrent</strong>: feedback connections are expected (towards neurons of the same level but also backward).<ul>
<li>More suitable for the management of sequences because they have a short-term memory effect</li>
</ul>
</li>
</ul>
<p><strong>Neural Networks Training</strong></p>
<p>General considerations about NN layers:</p>
<ul>
<li>Greater number of hidden layers (and neurons) -&gt; better performance</li>
<li>Greater number of hidden layers (and neurons) -&gt; need for more training data</li>
<li>Greater number of hidden layers (and neurons) -&gt; greater computational load</li>
</ul>
<p><em>How is it possible to train a neural network?</em></p>
<p>Training a neural network is extremely complicated, but we can use specific frameworks:</p>
<ul>
<li>PyTorch</li>
<li>TensorFlow</li>
<li>Mxnet</li>
</ul>
<p><img alt="" src="machine-learning/classification/train.jpg" /></p>
<p>Inside the neural network, we can change the weights applied to each input.</p>
<p>Training a NN means <strong>minimizing</strong> the loss function.
The cost function:</p>
<ul>
<li>It is a mathematical formulation of the learning goal</li>
<li>It measures the error between the prediction and the ground truth</li>
<li>Presents the performance in the form of a single real number</li>
</ul>
<p><img alt="" src="machine-learning/classification/loss.jpg" /></p>
<p><em>How to minimize the loss?</em> -&gt; adjusting the weights and the bias of every neuron</p>
<p>We can change weights and bias following <strong>gradients</strong> (the derivative of a function measures the sensitivity to change of the function value with respect to a change in its argument).</p>
<p>We can minimize the loss function with a gradient descent approach that adjust weights in the following manner:</p>
<p><img alt="" src="machine-learning/classification/weight.jpg" /></p>
<p><img alt="" src="machine-learning/classification/dnn-train.jpg" /></p></section><section class="print-page" id="machine-learning-classification-neural-network"><h1 id="machine-learning-classification-neural-network-convolutional-neural-networks">Convolutional Neural Networks</h1>
<p>CNNs are particular Neural Networks specifically designed to process images.
Instead of using a flat structure, it arranges neurons on three dimensions.</p>
<p><img alt="" src="machine-learning/classification/cnn.jpg" /></p>
<p>How is it possible to connect a 3D input with a <strong>kernel</strong>?</p>
<p>To do so, we need a mathematical operation called <strong>convolution</strong>.
The convolution is the core building block of convolutional neural networks.</p>
<p>Each kernel is convolved with the input volume thus producing a 2D feature map.
With this process, one feature map for each kernel is produced.</p>
<p>We usually have more than one kernel as output is usually larger than 1.
The output volume is then made up by stacking together all activation maps produced on the top of the other.</p>
<p><img alt="" src="machine-learning/classification/convo.jpg" /></p>
<h2 id="machine-learning-classification-neural-network-cnn-architecture">CNN Architecture</h2>
<p>In multi-layers architecture we have a flat structure with different layers.</p>
<p>With CNN, we have a bunch of layers stacked one on the top of the other:</p>
<p><img alt="" src="machine-learning/classification/cnn-arch.jpg" /></p>
<p>Differently from HOG, CNN automatically learn how to extract features, so there is no need to specify parameters.</p>
<p>Convolutional layers learn to extract various types of visual information in a hierarchical manner:</p>
<ul>
<li>In the layers close to the input, CNNs learn filters to extract 'simple' visual information.</li>
<li>In the layers placed in depth, the filters extract semantically complex visual information.</li>
</ul>
<p>The interesting thing is that this mechanism seems similar to what happens in our <strong>brain</strong>, where the visual cortex processes information by different layers.</p>
<h2 id="machine-learning-classification-neural-network-pooling-layer">Pooling Layer</h2>
<p>Pooling layers spatially subsample the input volume (reduce the input size).
They are widely used for a number of reasons:</p>
<ul>
<li>Gain robustness to exact location of the features</li>
<li>Reduce computational cost</li>
<li>Help preventing over-fitting</li>
<li>Increase receptive field of following layers</li>
</ul>
<h2 id="machine-learning-classification-neural-network-other-layers">Other Layers</h2>
<p><strong>Activation layer</strong>: activation function used with neural networks.</p>
<p><strong>Flatten layer</strong>: usually exploited to connect the 3D feature extractor to the 1D classifier (like MLP wih <em>unrolled</em> img).</p>
<p>It is possible to build a personalized architecture but it is very complex so, we will use already implemented ones:</p>
<ul>
<li>AlexNet</li>
<li>ResNet</li>
<li>VGG</li>
</ul>
<h2 id="machine-learning-classification-neural-network-train-cnns">Train CNNs</h2>
<p>CNNs can be trained in different ways:</p>
<p><img alt="" src="machine-learning/classification/train-cnn.jpg" /></p></section><section class="print-page" id="machine-learning-classification-machine-vs-deep"><h1 id="machine-learning-classification-machine-vs-deep-machine-learning-vs-deep-learning">Machine Learning vs Deep Learning</h1>
<p><img alt="" src="machine-learning/classification/machinevsdepp.jpg" /></p>
<p>The superiority of DL approaches compared to other ML algorithms manifests itself when large (<strong>huge</strong>) quantities of <strong>training data</strong> are available.</p>
<p>The training of neural networks requires a specialized hardware:</p>
<ul>
<li>Before starting a project with DL, you need to ask if the company / lab has the necessary hardware</li>
<li>Having one or more GPU available is a fundamental factor today (GPU are essential for parallelizing calculations)</li>
<li>The deeper a network is, the more computational load is introduced</li>
</ul>
<h2 id="machine-learning-classification-machine-vs-deep-hardware-purchase-for-dl">Hardware purchase for DL</h2>
<p>With <strong>in-house solutions</strong>, the company buys the necessary hardware and is the direct owner:</p>
<ul>
<li>Pros:<ul>
<li>Extreme freedom of use of hardware</li>
<li>In the long run, it tends to have lower costs</li>
</ul>
</li>
<li>Cons:<ul>
<li>Hardware maintenance is required (specialized technicians)</li>
<li>Hardware ages quickly</li>
<li>For large number of GPUs -&gt; specific server rooms (with high energy consumption)</li>
<li>The GPU market is quite expensive and volatile</li>
</ul>
</li>
</ul>
<p>With <strong>external solutions</strong>, the hardware is rented through the PaaS paradigm (Cloud).</p>
<ul>
<li>Pros:<ul>
<li>Hardware maintenance is not required</li>
<li>No investment over time is required for hardware upgrades</li>
<li>Dedicated server rooms are not required, energy consumption is not borne by the company</li>
</ul>
</li>
<li>Cons:<ul>
<li>In the long run, it tends to have higher costs</li>
<li>Vendor lock-in</li>
<li>We do not really know who the owner of the data is</li>
<li>Privacy issues</li>
</ul>
</li>
</ul>
<p><img alt="" src="machine-learning/classification/ai.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Classification</h1>
                        <h1 class='nav-section-title' id='section-dm-lab'>
                            DM - LAB <a class='headerlink' href='#section-dm-lab' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="data-mining-weka-lab-weka-lab"><h1 id="data-mining-weka-lab-weka-lab-weka">Weka</h1>
<p>Weka is an open-source software for Data Mining and Machine Learning written in Java, distributed under the GNU public license.</p>
<p>It includes four applications:</p>
<ol>
<li><strong>Explorer</strong> - we will use explorer</li>
<li><strong>Experimenter</strong></li>
<li><strong>Knowledge Flow</strong></li>
<li><strong>SimpleCLI</strong></li>
</ol>
<p>The main file format used in Weka  is <strong>ARFF</strong> (attribute-relation file format), which is a comma-separated value format.</p>
<p>Weka files store relations, attributes and values.</p>
<div class="highlight"><pre><span></span><code><span class="nd">@attribute</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="n">numeric</span><span class="w"></span>
<span class="nd">@attribute</span><span class="w"> </span><span class="n">sex</span><span class="w"> </span><span class="p">{</span><span class="n">female</span><span class="p">,</span><span class="w"> </span><span class="n">male</span><span class="p">}</span><span class="w"></span>
<span class="nd">@attribute</span><span class="w"> </span><span class="n">cholesterol</span><span class="w"> </span><span class="n">numeric</span><span class="w"></span>

<span class="nd">@data</span><span class="w"></span>
<span class="mi">63</span><span class="p">,</span><span class="w"> </span><span class="n">male</span><span class="p">,</span><span class="mi">233</span><span class="w"></span>
<span class="mi">67</span><span class="p">,</span><span class="n">male</span><span class="p">,</span><span class="mi">286</span><span class="w"></span>
</code></pre></div></section><section class="print-page" id="data-mining-weka-lab-bank-data"><h1 id="data-mining-weka-lab-bank-data-bank-data">Bank Data</h1>
<p>Attributes:</p>
<p><img alt="" src="data-mining/weka-lab/bank.jpg" /></p>
<p><strong>PEP class</strong> (Personal Equity Plan)</p>
<h2 id="data-mining-weka-lab-bank-data-pre-processing-bank-data">Pre-Processing Bank Data</h2>
<ol>
<li>Load the files and save it in an ARFF format </li>
<li>Carry out a visual analysis of the dataset</li>
<li>Drop the ID attribute </li>
</ol>
<p>Which attribute is more relevant for our analysis?</p>
<ul>
<li>Sex (no relevant difference)</li>
<li>Age (not as relevant as income, but there is a trend)</li>
<li>Married (relevant)</li>
<li>
<p>Children (linear correlation, the first column does not respect the trend)</p>
</li>
<li>
<p>Income (normal distribution)</p>
<ul>
<li>The higher the income, the higher the probability to buy PEP</li>
</ul>
<p><img alt="" src="data-mining/weka-lab/income.jpg" /></p>
</li>
</ul>
<h2 id="data-mining-weka-lab-bank-data-visualize-the-plot-matrix">Visualize the Plot Matrix</h2>
<p><img alt="" src="data-mining/weka-lab/children.jpg" /></p>
<p>The higher the number of children, the lower the income as children cost a lot of money.</p>
<p>People without children are not interested in PEP as they do not need to think about the future. </p>
<h2 id="data-mining-weka-lab-bank-data-classify">Classify</h2>
<p>Use the following algorithms and evaluate the result:</p>
<ul>
<li>J48</li>
<li>J48 (without post-pruning)</li>
<li>Jrip</li>
<li>IBk (with k=1 and k=5)</li>
</ul>
<p><img alt="" src="data-mining/weka-lab/j48.jpg" /></p>
<p>The main variables are <strong>children</strong> and <strong>income</strong> (closest to the root).</p>
<p>KNN with k = 1 using the training set has an accuracy of 100% because the closest point to me is me (if given), so we drop it.</p>
<p>With IBk we are using a <em>distance function</em> so numbers should be discretized. </p>
<p>Also, irrelevant and replicated attributes can create distortion in the result. 
- In this case, irrelevant attributes are most likely (sex, car)</p>
<p><img alt="" src="data-mining/weka-lab/knn.jpg" /></p>
<p>We can drop the irrelevant attributes to increase accuracy.</p></section><section class="print-page" id="data-mining-weka-lab-census-data"><h1 id="data-mining-weka-lab-census-data-census-data">Census Data</h1>
<p>Identify relevant attributes:</p>
<ul>
<li>Capital gain/loss are not relevant because many attributes are zero</li>
<li>Age (we expect it to be linear but the majority of &gt;50k is in the range of 40 years)</li>
<li>Work class is unbalanced (the majority of people works in private), coverage is very low</li>
<li>Education number </li>
<li>Marital status (interesting, married is different)</li>
<li>Occupation (interesting)</li>
</ul>
<p>Education and education-num are perfectly correlated (it is duplicated).</p>
<p><img alt="" src="data-mining/weka-lab/census.jpg" /></p>
<p>We can enhance accuracy by replacing missing values:</p>
<p>Preprocess - filter - unsupervised - attribute - replacemissingvalues</p>
<h2 id="data-mining-weka-lab-census-data-apriori-algorithm">Apriori Algorithm</h2>
<p>We apply discretization and perform a manual analysis of the data in order to identify any correlation between pairs of attributes.</p>
<p><img alt="" src="data-mining/weka-lab/apriori.jpg" /></p></section><section class="print-page" id="data-mining-weka-lab-clustering"><h1 id="data-mining-weka-lab-clustering-clustering-with-weka">Clustering with Weka</h1>
<p>During the pre-processing part, the first thing to to do is to <strong>normalize</strong> the dataset (unsupervised filter) because clustering algorithms need a distance measure.</p>
<p>Then, we need to select the K-Means parameters:</p>
<ul>
<li><strong>DisplayStDev</strong>: shows the standard deviation of the distances of individual points from the cluster center. The measure is reported separately for each attribute.</li>
<li><strong>Distance Function</strong>: distance function used in the calculation</li>
<li><strong>Maxiteration</strong>: maximum number of iterations to achieve convergence</li>
<li><strong>NumCluster</strong>: value of k</li>
<li><strong>Seed</strong>: random value for choosing the initial</li>
</ul>
<p><img alt="" src="data-mining/weka-lab/clust-weka.jpg" /></p>
<p>We set the number of cluster at 3:</p>
<p><img alt="" src="data-mining/weka-lab/plot.jpg" /></p>
<p>If we try with two clusters, the squared error is high (12.34), while with 4 clusters, the squared error is about 5.</p>
<p><img alt="" src="data-mining/weka-lab/class.jpg" /></p>
<p>By running the analysis with <em>class to cluster evaluation</em>, clusters are created based on their size.</p>
<p>We can improving the model by running the system in a two-dimensional space, selecting the two clusters with the lowest standard deviation.</p>
<h2 id="data-mining-weka-lab-clustering-food-nutrient-dataset">Food Nutrient Dataset</h2>
<p>The dataset is composed by 25 foods with their nutritional information based on the following KPIs:</p>
<ul>
<li>Energy</li>
<li>Protein</li>
<li>Fat</li>
<li>Calcium</li>
<li>Iron</li>
</ul>
<p>The goal is to distinguish, based on the data, which ones are <strong>meat</strong> and which ones are <strong>fish</strong>.</p>
<p>We run the cluster analysis using k-means:</p>
<p><img alt="" src="data-mining/weka-lab/food1.jpg" /></p>
<p>With two clusters, we can easily distinguish which cluster regards meat and which one is for fish.</p>
<p>With three clusters, C0 remains unchanged while characterization between C1 and C2 is relevant only for proteins.</p></section><h1 class='nav-section-title-end'>Ended: DM - LAB</h1>
                        <h1 class='nav-section-title' id='section-ml-lab'>
                            ML - LAB <a class='headerlink' href='#section-ml-lab' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-lab-lab-notes"><h1 id="machine-learning-lab-lab-notes-laboratory">Laboratory</h1>
<ul>
<li>Programing done in Python</li>
<li>We will use <strong>Colab</strong> as a programming tool</li>
<li>We can use any IDE (Visual Studio, PyCharm)</li>
</ul>
<h2 id="machine-learning-lab-lab-notes-guide">Guide</h2>
<ul>
<li>Do not execute code without understanding it</li>
<li>It is necessary to <em>play</em> a lot with the code to become familiar</li>
<li><strong>Bugs</strong> can be very informative</li>
</ul>
<h2 id="machine-learning-lab-lab-notes-colab">Colab</h2>
<ul>
<li>Free Google service</li>
<li>Easy to use, the environment provided already has most of the resources installed</li>
<li>The code runs in the browser (VM)</li>
<li>Each assigned virtual machine has a variable hardware equipment<ul>
<li>CPU and RAM available</li>
<li>GPU resources</li>
</ul>
</li>
</ul></section><h1 class='nav-section-title-end'>Ended: ML - LAB</h1>
                        <h1 class='nav-section-title' id='section-ml-seminar'>
                            ML - Seminar <a class='headerlink' href='#section-ml-seminar' title='Permanent link'></a>
                        </h1>
                        <section class="print-page" id="machine-learning-seminar-ai-business"><h1 id="machine-learning-seminar-ai-business-ai-solutions-for-real-world-business">AI Solutions for Real-World Business</h1>
<p>Presented by: <strong>Cosimo Fiorino</strong> - Head of Data Science</p>
<p><em>c.fiorini@ammagamma.com</em></p>
<p>The team of <em>Ammagamma</em> is various, composed not only by engineers, but also philosophers and designers.</p>
<p>Their <strong>vision</strong> is to develop a society aware of the potentialities, implications and impacts of technology.</p>
<p>As a <strong>mission</strong>, they offer the best choice instruments through the development of artificial intelligence innovative solutions.</p>
<p>They master cutting-edge technology to make company processes easier, curating the whole life cycle.</p>
<p><strong>AI solutions implemented:</strong></p>
<ol>
<li>Scheduling and optimized planning of production and organizational processes</li>
<li>API for the forecast of future trends and historic series</li>
<li>Data enrichment for the optimization of advanced analytics solutions</li>
<li>Warehouse forecasting and replenishment for the predictive management of the warehouse</li>
<li>Georouting API engine for the optimized planning of complex and bound scenarios</li>
<li>Intelligent document processing platform enabling classification and extraction of the information</li>
</ol>
<h2 id="machine-learning-seminar-ai-business-shared-knowledge-in-the-ai-world">Shared Knowledge in the AI World</h2>
<p>They develop educational projects in the intersection between the culture of data and humanistic thought, to lead people to the discovery of new growth frontiers.</p>
<p>Also, they devise new innovation paths that create a long-term value for the community and the territory.</p>
<p><em>We use numbers to transform chaotic systems into organized, intuitive and verifiable models</em></p>
<ul>
<li><strong>Descriptive Algorithms:</strong> description of the variables that characterize a phenomenon to create its mathematical model (clustering, market basket analysis, BI systems)</li>
<li><strong>Predictive Algorithms:</strong> Information analysis to predict and control future behaviors of the phenomena (forecasting, regression, classification)</li>
<li><strong>Prescriptive ALgorithms:</strong> Development of prescriptive formulas to support and guide the process management (optimization, optimal control, next best action)</li>
</ul>
<h2 id="machine-learning-seminar-ai-business-ai-projects-development">AI Projects Development</h2>
<ol>
<li>Define the objectives and the context</li>
<li>Feasibility analysis and solution design</li>
<li>Proof od concept</li>
<li>Pilot</li>
<li>Deploy</li>
<li>Scale up</li>
<li>Post go-live and monitoring</li>
</ol>
<p><strong>What can go wrong?</strong></p>
<p>DATA:</p>
<ul>
<li>Quality</li>
<li>Null values</li>
<li>Legal problems</li>
<li>Security</li>
<li>Dimension</li>
</ul>
<p>COMPLEXITY AND BUSINESS CONSTRAINTS:</p>
<p>Constraints related to third parties in the business.</p>
<p>ADOPTION:</p>
<p>In the post deployment phase, the innovation department develop innovative solutions and then the department will not use it because they are skeptical. </p>
<h2 id="machine-learning-seminar-ai-business-design-thinking">Design Thinking</h2>
<p>An approach to innovation that studies the adoption of a creative view to solve complex problems.</p>
<p><img alt="" src="machine-learning/seminar/mlops.jpg" /></p>
<p>Make sure that when you build a model you will be able to reproduce it in the future and maintain it properly.</p>
<h2 id="machine-learning-seminar-ai-business-data-ethic">Data Ethic</h2>
<p><img alt="" src="machine-learning/seminar/ethic.jpg" /></p></section><section class="print-page" id="machine-learning-seminar-use-cases"><h1 id="machine-learning-seminar-use-cases-demand-forecasting-inventory-optimization">Demand Forecasting &amp; Inventory Optimization</h1>
<p>In the food processing sector for the supply process of canteens, restaurants and indirect channels, it is extremely important to estimate the food products that will be sold to ease the order of raw materials necessary for production.</p>
<p>The <strong>target</strong> is to improve demand forecasting accuracy, reduce total stock level in the warehouse and reduce the total amount of wasted food.</p>
<p><strong>Solutions:</strong></p>
<ul>
<li>Demand forecasting algorithms adaptable to market movements and new products quickly</li>
<li>Optimization models which, according to the predicted demand, suggest the best list of products </li>
</ul></section><section class="print-page" id="machine-learning-seminar-marketing"><h1 id="machine-learning-seminar-marketing-marketing-management-optimization">Marketing Management Optimization</h1>
<p>Difficulty in managing and synchronizing the multiple campaigns aimed at costumer up / cross selling, retention and / or acquisition.</p>
<p>The approach, typical of product-centric strategies and dictated by objectives that are imposed on individual campaigns without considering the synergistic interaction between generating excessive push communication on the customer.</p>
<p><strong>Recommender Systems</strong></p>
<p>The goal is to find the perfect match between customers and products in order to suggest the right product at the right moment.</p>
<p><strong>Item-based filtering</strong></p>
<p>I suggest you product x because it is similar to the product you bought.
The implementation needs a vectorial representation of each product in order to evaluate their similarity.</p>
<p><strong>Collaborative filtering</strong></p>
<p>I suggest you product x because people similar to you have chosen it.
The implementation needs a vectorial representation of each customer in order to evaluate their similarity.</p>
<p><strong>Popularity filtering</strong></p>
<p>I suggest you product x because it is popular right now.</p>
<p><strong>Serendipity effect</strong></p>
<p>Serendipity is an unplanned fortunate discovery.</p>
<p><strong>Hybrid filtering</strong></p>
<p>When I have a lot of information I can use a combination of the preceding approaches.</p>
<h2 id="machine-learning-seminar-marketing-dynamic-recommender-systems">Dynamic Recommender Systems</h2>
<p>Recommendation is not only about the right product, but also the right moment to suggest it.</p></section><section class="print-page" id="machine-learning-seminar-nlp"><h1 id="machine-learning-seminar-nlp-natural-language-processing">Natural Language Processing</h1>
<p>The main concept behind NLP is embedding: exposing natural language in a vector in multidimensional place, maintaining the semantics.</p>
<p>The management of appraisal documents as part of the due diligence processes for non performing loans involves various resources in the back office department who are respnsible for visually checking the appraisal documents of judicial auctions.</p></section><section class="print-page" id="machine-learning-seminar-ethics"><h1 id="machine-learning-seminar-ethics-ai-ethics">AI Ethics</h1>
<p><em>Gabriele Graffieti</em> - CV Algorithm Engineer at Ambarella</p>
<p>Case Study:</p>
<p>You work in a large company which receives thousands of CVs daily.
The openings are many and different from each other. Of course, skimming through CVs requires a lot of time and effort.</p>
<p>Good candidates can be erroneously discarded in this preliminary phase.</p>
<p>The idea is to develop an AI system that analyzes the CVs continuously.</p>
<p><strong>Solution</strong></p>
<p>Use the CVs of the current employees as ground truth data, in order to select candidates similar to the people that already work in the company.</p>
<p><strong>Results</strong></p>
<p>The selected people are very good candidates and the system performs better than our HRs in selecting good candidates.</p>
<p><em>Are we happy about this system?</em></p>
<p>This system was used by Amazon and it showed bias against women.</p>
<p><strong>Issues</strong></p>
<p>This problem is not easily detectable in the first place.
On other hand, if we remove all the gender info from the CV, the model is still able to infer on that information.</p>
<p><em>Are you sure about your data?</em></p>
<ul>
<li>Have you checked the labels?</li>
<li>Do you know how the data is labeled?</li>
<li>Do you know who labeled the data?</li>
</ul>
<p>Some tools like <strong>amazon mechanical turk</strong> can be used to label data but, some issues can be found with this tool:
For example, emotion recognition can be labelled wrongly due to different cultures.</p>
<p>This aspect is particularly relevant with high risk AI applications, like:</p>
<ul>
<li>Diagnosis applications</li>
<li>Control of critical infrastructure</li>
<li>Law enforcement</li>
<li>Scoring</li>
<li>Hiring</li>
</ul>
<p>Many people say that critical decisions should be taken by humans, however, humans are not perfect.</p>
<p><em>In the US, the best day to have a trial is Monday after a victory of the local football team</em></p>
<p>Human decision making is highly affected by mood, personal concerns, stress, level of sleep, affinity with the assessed person, stereotypes and so on.</p>
<p><strong>What about human-AI collaboration?</strong></p>
<p>It seems the perfect solution but humans can be biased by the fact they that believe that AI is always right so they tend to trust it.</p>
<p>Also, after some time, humans unconsciously trust AI and they no longer be able to spot errors.</p>
<p>On the other hand, what if AI is right but the human overcome the decision?
And what if AI is wrong but it is so powerful that it can convince humans?</p>
<p><em>Are we sure that we are completely free of biases?</em></p>
<p>Right now, we have no answer to all these questions, because software development is not considered to be economically valuable.</p>
<p>In order to design safe AI systems, we should follow these steps:</p>
<ol>
<li>Alignment </li>
<li>Robustness</li>
<li>Corrigibility </li>
</ol>
<p>Some countermeasures to adopt would be to use explainable models (deep learning, CNN and neural network are not explainable).</p></section><h1 class='nav-section-title-end'>Ended: ML - Seminar</h1></div>





                

  

              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
<script id="__config" type="application/json">{"base": "/", "features": [], "search": "assets/javascripts/workers/search.16e2a7d4.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    

<script src="search/search_index.js"></script>


    
      <script src="assets/javascripts/bundle.d6c3db9e.min.js"></script>
      
        <script src="js/print-site.js"></script>
      
        <script src="js/arithmatex.config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>