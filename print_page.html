
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.4.2">
    
    
      
        <title>Print as PDF - CescaNeri/DataMining-MachineLearning</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.69437709.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="css/print-site.css">
    
      <link rel="stylesheet" href="css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-data-mining" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="CescaNeri/DataMining-MachineLearning" class="md-header__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CescaNeri/DataMining-MachineLearning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print as PDF
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="CescaNeri/DataMining-MachineLearning" class="md-nav__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    CescaNeri/DataMining-MachineLearning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Data Mining
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Mining" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Data Mining
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/data-mining.html" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/customer-retention-case.html" class="md-nav__link">
        Customer Retention Case
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Data Understanding
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Understanding" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Data Understanding
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/data-understanding/data-understanding.html" class="md-nav__link">
        Data Understanding
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Decision Tree
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Decision Tree" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Decision Tree
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/decision-tree/model.html" class="md-nav__link">
        Decision Tree Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Print as PDF
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="print_page.html" class="md-nav__link md-nav__link--active">
        Print as PDF
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    2. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    3. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    4. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    2. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    3. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    4. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
  
                


<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
    <span style="float: right"><a href="https://timvink.github.io/mkdocs-print-site-plugin/">mkdocs-print-site-plugin</a></span>
</p>
<p>
    This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.
</p>
<p>
    See also <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-PDF.html">export to PDF</a> and <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-HTML.html">export to standalone HTML</a>.
</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Index</h1>
                </nav>
            </div>
        </section>
        
                        <h1 class='nav-section-title' id='section-data-mining'>
                            Data Mining <a class='headerlink' href='#section-data-mining' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-introduction-data-mining"><h1 id="data-mining-introduction-data-mining-data-mining">Data Mining</h1>
<p>The amount of data stored on computer is constantly increasing, coming from:</p>
<ul>
<li>IoT data</li>
<li>Social data</li>
<li>Data on purchases</li>
<li>Banking and credit card transaction</li>
</ul>
<p>The first step is to collect data in a data set. This step can be automated through artificial intelligence increasing the analytical power.</p>
<p>From on side, data is more and more and on the other side, hardware becomes more powerful and cheaper each day.</p>
<p>At the same time, managers are more and more willing to rely on data analysis for their business decisions.
The information resource is a precious asset to overcoming competitors. </p>
<h2 id="data-mining-introduction-data-mining-artificial-intelligence-machine-learning-and-data-mining">Artificial Intelligence, Machine Learning and Data Mining</h2>
<p>Although strongly interrelates, the term machine learning is formally distinct from the term Data Mining which indicates the computational process of pattern discovery in large datasets using machine learning methods, artificial intelligence, statistics and databases. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-definition">Data Mining - definition</h2>
<p>Complex extraction of implicit, previously unknown and potentially useful data from the information.
Exploration and analysis, using automated and semi-automatic systems, of large amounts of data in order to find significant patterns through statistics. </p>
<p>We do not just need to find results, but we need results to be USEFUL. </p>
<h2 id="data-mining-introduction-data-mining-analytics">Analytics</h2>
<p>ANalytics refers to software used to discover, understand and share relevant pattern in data.
Analytics are based on the concurrent use of statistics, machine learning and operational research techniques, often exploiting visualization techniques. </p>
<p><img alt="" src="data-mining/introduction/BI.jpg" /></p>
<p>Prescriptive systems generale much value but it is extremely complex. Companies should start simple, adopting simple descriptive analytics solutions, and then move on. 
It is risky to skip intermediate steps.</p>
<h2 id="data-mining-introduction-data-mining-bi-adoption-path">BI adoption path</h2>
<p>When we decide to digitalize a company, the adoption of BI solutions is incremental and rarely allows steps to be skipped.
This is because it is risky, costly and useless to adopts advanced solutions before completely exploiting simple ones. </p>
<p>The goal is to create a <strong>data-driven company</strong>, where managers are supported by data. </p>
<ul>
<li>Decisions are based on quantitative rather than qualitative knowledge.</li>
<li>Process and knowledge are an asset of the company and are not lost if managers change</li>
</ul>
<p><em>The gap between a data-driven decision and a good decision is a good manager</em></p>
<p>Adopting a data-driven mindset goes far beyond adopting a business intelligence solution and entails:</p>
<ol>
<li>Create a data culture</li>
<li>Change the mindset of managers</li>
<li>Change processes</li>
<li>Improve the quality of all the data</li>
</ol>
<p><strong>Digitalization</strong> is a journey that involves three main dimensions:</p>
<p><img alt="" src="data-mining/introduction/DT.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-pattern">Pattern</h2>
<p>A pattern is a synthetic representation rich in semantics of a set of data. It usually expresses a recurring pattern in data, but can also express an exceptional pattern.</p>
<p>A pattern must be:</p>
<ul>
<li>Valid on data with a certain degree of confidence</li>
<li>It can be understood from the syntax and semantic point of view, so that the user can interpret it</li>
<li>Previously unknown and potentially useful, so that users can take actions accordingly</li>
</ul>
<p>When we distinguish between a manual technique (DW) and an automatic technique is the creation of a small subset of data which is rich in semantics.</p>
<p>The process begins with a huge multi-dimension cube of data, then grouping and selection techniques are adopted, creating a <strong>pattern</strong>.</p>
<p><strong>Pattern types:</strong></p>
<ul>
<li>Association rules (logical implications of the dataset)</li>
<li>Classifiers (classify data according to a set of priori assigned classes)</li>
<li>Decision trees (identify the causes that lead to an event, in order of importance)</li>
<li>Clustering (group elements depending on their characteristics)</li>
<li>Time series (detection of recurring or atypical patterns in complex data sequences)</li>
</ul>
<h2 id="data-mining-introduction-data-mining-data-mining-applications">Data Mining Applications</h2>
<p><strong>Predictive Systems</strong>
Exploit some features to predict the unknown values of other features (classification and regression).</p>
<p><strong>Descriptive Systems</strong>
Find user-readable patterns that can be understood by human users (clustering, association rules, sequential patter).</p>
<h2 id="data-mining-introduction-data-mining-classification-definition">Classification - Definition</h2>
<p>Given a <strong>record</strong> set, where each record is composed by a set of attributes (one of them represents the class of the record), find a model for the class attribute expressing the attribute value as a function of the remaining attributes.</p>
<p><em>Given a feature (defined at priori), define weather a user belongs to that feature</em></p>
<p>This model must work even when the record is not given. Unclassified record must be assigned to a class in the most accurate way.</p>
<p>A <strong>test set</strong> is used to determine the model accuracy.</p>
<p><img alt="" src="data-mining/introduction/test.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-classification-example">Classification example</h2>
<p><strong>Direct Marketing:</strong>
The goal is to reduce the cost of email marketing by defining the set of customers that, with the highest probability, will buy a new product.</p>
<p>Technique:</p>
<ul>
<li>Exploit the data collected during the launch of similar products<ul>
<li>We know which customers bought and which one did not</li>
<li>{<em>buy, not buy</em>} = <strong>class attribute</strong></li>
</ul>
</li>
<li>Collect all the available information about each customers</li>
<li>Use such information as an input to train the model</li>
</ul>
<p><strong>Churn Detection</strong>
Predict customers who are willing to go to a competitor.</p>
<p>Technique:</p>
<ul>
<li>Use the purchasing data of individual users to find the relevant attributes</li>
<li>Label users as {<em>loyal, not loyal</em>}</li>
<li>Find a pattern that defines loyalty</li>
</ul>
<h2 id="data-mining-introduction-data-mining-clustering-example">Clustering example</h2>
<p>Given a set of points, each featuring set of attributes, and having a similarity measure between points, find subset of points such that:
<em>points belonging to a cluster are more similar to each other than those belonging to other clusters</em></p>
<p><strong>Marketing Segmentation</strong>
The goal is to spit customers into distinct subsets to target specific marketing activities.</p>
<p>Techniques:</p>
<ul>
<li>Gather information about customer lifestyle and geographic location</li>
<li>Find clusters of similar customers</li>
<li>Measure cluster quality by verifying whether the purchasing patterns of customers belonging to the same cluster are more similar to those of distinct clusters</li>
</ul>
<h2 id="data-mining-introduction-data-mining-association-rules-example">Association Rules example</h2>
<p>Given a set of records each consisting of multiple elements belonging to a given collection.
It produces rules of dependence that predict the occurrence of one of the elements in the presence of others.</p>
<p><strong>Marketing Sales Promotion</strong>
Suppose you have discovered this association rule:
{<em>Bagels,...} -&gt; {</em>Potato chips*}</p>
<p>This information can be used to understand what actions to take to increase its sales. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-bets">Data Mining Bets</h2>
<ul>
<li>Scalability</li>
<li>Multidimensionality of data set</li>
<li>Complexity and heterogeneity of the data</li>
<li>Data quality</li>
<li>Data properties</li>
<li>Privacy keeping</li>
<li>Processing in real-time</li>
</ul>
<h2 id="data-mining-introduction-data-mining-crisp-methodology">CRISP methodology</h2>
<p>A data mining project requires a structured approach in order to choose the best algorithm.</p>
<p><strong>CRISP-DM</strong> methodology is the most used technique. It is one of the most structures proposals to define the fundamental steps of a data mining project. </p>
<p><img alt="" src="data-mining/introduction/crisp.jpg" /></p>
<p>The six stages of the life cycle are not strictly sequential, indeed, it is often necessary.</p>
<ol>
<li><strong>Business understanding</strong> (understand the application domain): understanding project goals from users' point of view, translate the user's problem into a data mining problem and define a project plan.<ul>
<li>Get an idea about the business domain and the data mining approach to adopt.</li>
</ul>
</li>
<li><strong>Data understanding</strong>: preliminary data collection aimed at identifying quality problems and conducting preliminary analysis to identify the salient characteristics.</li>
<li><strong>Data preparation</strong>: tasks needed to create the final dataset, selecting attributes and records, transforming and cleaning data.<ul>
<li>Prepare the data for ML tasks (clean, complete missing data, create new features)</li>
</ul>
</li>
<li><strong>Model creation</strong>: data mining techniques are applied to the dataset in order to identify what makes the model more accurate.</li>
<li><strong>Evaluation of model and results</strong>: the model obtained from the previous phase are analyzed to verify that they are sufficiently precise and robust to respond adequately to the user's objectives.</li>
<li><strong>Deployment</strong>: the built-in model and acquired knowledge must be made available to users.<ul>
<li>Change the software and processes to include new AI functionalities</li>
</ul>
</li>
</ol>
<p>Different classes of data mining use different algorithms so the evaluation changes accordingly. </p></section><section class="print-page" id="data-mining-introduction-customer-retention-case"><h1 id="data-mining-introduction-customer-retention-case-customer-retention">Customer Retention</h1>
<p>Customer retention, churn analysis, dropout analysis are synonyms for predictive analysis carried out by organizations and companies to avoid losing customers.</p>
<p>The idea is to create a different profile for customers who stay and customers who drop-out.</p>
<p><strong>The Gym Case Study</strong></p>
<p>They discovered that customers who did not train well, eventually drop out from the gym.
Therefore, the goal was to model customers' training sessions in order to predict those who did not train well and prevent them from dropping out.</p>
<p>Steps:</p>
<ul>
<li>Customers have s list of exercises</li>
<li>The system records the exercises (and repetition) did during the workout</li>
<li>The system matches the exercises </li>
<li>Train a classifier that is able to predict that someone is leaving the gym because he is unsatisfied<ul>
<li>The system update the profile each week</li>
<li>Four weeks without training = dropout</li>
<li>The idea of dropout needs to be defined properly (a customer who stops going to the gym in summer and comes back in summer is different from a customers who dropout and does not come back)</li>
</ul>
</li>
</ul>
<p>Practitioner who is about to leave the gym is training poorly. How can characterize the user behaviors? How long does it last?</p>
<p>Many KPIs can be adopted to assess the training session: in this case, two indicators were identified:</p>
<ol>
<li><strong>Compliance</strong> (adherence of the performed workout)</li>
<li><strong>Regularity</strong> (regularity of the training sessions with reference to the prescribed one)</li>
</ol>
<p>We still have a problem of <strong>granularity:</strong> we can assess regularity by checking steps, repetition, physical activity, muscle or body part. </p></section><h1 class='nav-section-title-end'>Ended: Data Mining</h1>
                        <h1 class='nav-section-title' id='section-data-understanding'>
                            Data Understanding <a class='headerlink' href='#section-data-understanding' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-data-understanding-data-understanding"><h1 id="data-mining-data-understanding-data-understanding-data-understanding-preparation">Data Understanding &amp; Preparation</h1>
<p>In data mining, data are composed of collections of objects described by a set of attributes (we refer to data that can be stored in a database).</p>
<p><strong>Attribute:</strong> property characteristic of an object</p>
<h2 id="data-mining-data-understanding-data-understanding-attribute-types">Attribute types</h2>
<p>In order to perform meaningful analysis, the characteristics of the attributes must be known. The <strong>attribute type</strong> tells us what properties are reflected in the value we use as a measure. </p>
<p>We can identify 4 types of attributes:</p>
<ol>
<li><strong>Nominal</strong>-qualitative: different names of value (gender, zip code, ID)</li>
<li><strong>Ordinal</strong>-qualitative: values enables us to sort objects based on the value of attribute (grade)</li>
<li><strong>Interval</strong>-quantitative: the difference between the values has a meaning, with a unit of measurement (dates, temperature)</li>
<li><strong>Ratio</strong>-quantitative: the ratio of values has meaning (age, length, amount of money)</li>
</ol>
<h2 id="data-mining-data-understanding-data-understanding-further-classifications">Further classifications</h2>
<ul>
<li>Binary, discrete and continuous<ul>
<li>Discrete: finite number of infinite countable set of values</li>
<li>Continuous: real values
<em>Nominal and ordinal are typically discrete or binary, while interval and ratio attributes are continuous</em></li>
</ul>
</li>
<li>Asymmetric attribute: only instances that take non-zero values are relevant</li>
<li>Documents and Texts: objects of the analysis described by a vector of terms</li>
<li>Transactions<ul>
<li>Each record involves multiple items</li>
<li>Items come from a finite set</li>
<li>The number of items may vary from transaction to transaction</li>
</ul>
</li>
<li>Ordered data</li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-explorative-analysis">Explorative Analysis</h2>
<p>First step in business and ata understanding. It refers to the preliminary analysis of the data aimed at identify its main characteristics. </p>
<ul>
<li>It helps you choose the best tool for processing and analysis </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-statistics-overview">STATISTICS OVERVIEW</h2>
<h3 id="data-mining-data-understanding-data-understanding-frequency">Frequency</h3>
<p>The frequency of an attribute value is the percentage of times that value appears in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-mode">Mode</h3>
<p>The mode of an attribute is the value that appears most frequently in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-percentile">Percentile</h3>
<p>Given an ordinal or continuous attribute x and a  number p between 0 and 100, the p-th percentile is the value of xp of x such that p% of the observed values for x are lower than xp.</p>
<p><img alt="" src="data-mining/data-understanding/boxplot.jpg" /></p>
<p>Percentile visualization through boxplot enables the representation of a distribution of data. It can be used to compare multiple distributions when they have homogeneous magnitude.</p>
<h3 id="data-mining-data-understanding-data-understanding-mean">Mean</h3>
<p>The mean is the most common measure for locating a set of points.</p>
<ul>
<li>Subject to outliers</li>
<li>It is preferred to use tee median or a 'controlled' mean </li>
</ul>
<h3 id="data-mining-data-understanding-data-understanding-median">Median</h3>
<p>The median is the term occupying the central place if the terms are odd; if the terms are even, the median is the arithmetic mean of the two central terms.</p>
<h3 id="data-mining-data-understanding-data-understanding-range">Range</h3>
<p>Range is the difference between the minimum and maximum values taken by the attribute.</p>
<h3 id="data-mining-data-understanding-data-understanding-variance-and-standard-deviation">Variance and Standard Deviation</h3>
<p>Variance and SD are the most common measures of dispersion of a data set.</p>
<ul>
<li>Sensitive to outliers since they are quadratically related to the concept of mean</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/var-sd.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-data-quality">Data Quality</h2>
<p>The quality of the datasets profoundly affects the chances of finding meaningful patterns.
The most frequent problems that deteriorate data quality are:</p>
<ul>
<li>Noise and outliers (objects with characteristics very different from all other objects in the data set)</li>
<li>Missing values (not collecting the data is different from when the attribute is not applicable), how to handle them:<ul>
<li>Delete the objects that contain them</li>
<li>Ignore missing values during analysis</li>
<li>Manually/automatically fill the missing values<ul>
<li>ML can be applied to fill the missing values by inferring the other values of that attribute and calculate the most appropriate value </li>
</ul>
</li>
</ul>
</li>
<li>Duplicated values (it may be necessary to introduce a data cleaning step in order to identify and eliminate redundancy)</li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-dataset-preprocessing">Dataset Preprocessing</h2>
<p>Rarely the dataset has the optimal characteristics to be best processed by machine learning algorithms. It is therefore necessary to put in place a series of actions to enable the algorithms of interest to function:</p>
<ul>
<li><strong>Aggregation:</strong> combine two or more attributes into one attribute</li>
<li><strong>Sampling:</strong> main technique to select data<ul>
<li>Collecting and processing the entire dataset is too expensive and time consuming</li>
<li>Simple Random Sampling (same probability of selecting each element)</li>
<li>Stratified sampling (divides the data into multiple partitions and use simple random sampling on each partition)<ul>
<li>Before sampling a partitioning rule is applied (we inject knowledge about the domain)</li>
<li>Allow the population to be balanced</li>
<li>However, we are applying a distortion</li>
</ul>
</li>
<li>Sampling Cardinality: after choosing the sampling mode, it is necessary to fix the sample size in order to limit the loss of information</li>
</ul>
</li>
<li><strong>Dimensionality reduction:</strong> the goal is to avoid the 'curse of dimensionality', reduce the amount of time and memory used by ML algorithms, simplify data visualization and eliminate irrelevant attributes and eliminate noise on data.
Curse of dimensionality: as dimensionality increases, the data become progressively more sparse. Many clustering and classification algorithms deal with dimensionality and distances. All the elements become equi-distant from one another; the idea of selecting the right dimension to carry out analysis is crucial.</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/dimension.jpg" /></p>
<p>The curve indicates that the more we increase the number of dimensionality, the smaller the ratio is.
In the modeling phase, it is important reduce dimensionality.</p>
<p>The goal is to reduce dimensionality and carry out analysis with the highest information amount.</p>
<ul>
<li><strong>Principal Component Analysis:</strong> it is a projection method that transforms objects belonging to a p-dimensional space into a k-dimensional space in such way as to preserve maximum information in the initial dimension.</li>
<li><strong>Attribute creation:</strong> it is a way to reduce the dimensionality of data. The selection usually aims to eliminate redundant. 
We can use different attribute selection techniques:<ul>
<li>Exhaustive approaches</li>
<li>Non-exhaustive approaches</li>
<li>Feature engineering (create new features): we have raw data and we can extract useful KPIs by designing new attributes.</li>
</ul>
</li>
<li>
<p><strong>Discretization and binarization:</strong> transformation of continuos-valued attributes to discrete-valued attributes. Discretization techniques can be unsupervised (do not exploit knowledge about the class to which elements belong) or supervised (exploit knowledge about the class to which the elements belong).</p>
<ul>
<li>
<p>Unsupervised: equi-width, equi-frequency, K-means
<img alt="" src="data-mining/data-understanding/discretization.jpg" /></p>
</li>
<li>
<p>Supervised: discretization intervals are positioned to maximize the 'purity' of the intervals</p>
</li>
</ul>
</li>
</ul>
<p><strong>Entropy and Information Gain:</strong> it is the measure of uncertainty about the outcome of an experiment that can be modeled by a random variable x.
The entropy of a <strong>certain</strong> event is zero.</p>
<p>The entropy of a discretization into n intervals depends on how pure each group.</p>
<p><img alt="" src="data-mining/data-understanding/entropy.jpg" /></p>
<p><strong>Binarization:</strong> we start with a discrete attribute but we need it to be binary.</p>
<ul>
<li><strong>Attribute transformation:</strong> function that maps the entire set of values of an attribute to a new set such that each value in the starting set corresponds to a unique value in the ending set. </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-similarity-and-dissimilarity">Similarity and Dissimilarity</h2>
<p>These two concepts are central in Machine Learning, as it is important to group clusters based on similarity and dissimilarity.</p>
<p>Some techniques are stronger with long distances while sometimes, by setting the wrong distance, we will incur in problems.</p>
<ul>
<li>
<p><strong>Similarity:</strong> it is a numerical measure expressing the degree of similarity between two objects</p>
<ul>
<li>Takes values in the range [0, 1]</li>
</ul>
</li>
<li>
<p><strong>Dissimilarity (distance):</strong> it is a numerical measure expressing the degree of difference between two objects</p>
<ul>
<li>Takes values in the range [0, 1] or [0, ∞]</li>
</ul>
</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/similarity.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-distance">Distance</h2>
<p><img alt="" src="data-mining/data-understanding/distance.jpg" /></p>
<p><strong>Distance Properties</strong>
Given two objects p and q and a dissimilarity measure d():</p>
<ul>
<li>d(p,q) = 0 only if p=q</li>
<li>d(p,q) = d(q,p) -&gt; <em>Symmetry</em> </li>
<li>d(p,r) + d(p,q) + d(q,r) -&gt; <em>Triangular inequality</em></li>
</ul>
<p><img alt="" src="data-mining/data-understanding/triangular.jpg" /></p>
<p><strong>Similarity Properties</strong>
Given two objects p and q and a similarity measure s():</p>
<ul>
<li>s(p,q) = 1 only if p=q</li>
<li>s(p,q) = s(q,p) -&gt; <em>Symmetry</em></li>
</ul>
<p><strong>Binary Vector Similarities</strong>
It is common for attributes describing an object to contain only binary values.</p>
<ul>
<li>M01 = the number of attributes where p=0 and q=1</li>
<li>M10 = the number of attributes where p=1 and q=0</li>
<li>M00 = the number of attributes where p=0 and q=0</li>
<li>M11 = the number of attributes where p=1 and q=1</li>
</ul>
<p><strong>Cosine Similarity</strong>
Like Jaccard's index. it does not consider 00 matches, but also allows non-binary vectors to be operated on.</p>
<p><strong>Similarity with Heterogeneous Attributes</strong>
In the presence of heterogeneous attributes, it is necessary to compute the similarities separately and then combine them so that their result belongs to the range [0, 1]</p>
<h2 id="data-mining-data-understanding-data-understanding-correlation">Correlation</h2>
<p>The correlation between pairs of objects described by attributes (binary or continuous) is a measure of the existence of a linear relationship between its attributes.</p>
<p><img alt="" src="data-mining/data-understanding/correlation.jpg" /></p>
<p><img alt="" src="data-mining/data-understanding/corr2.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Data Understanding</h1>
                        <h1 class='nav-section-title' id='section-decision-tree'>
                            Decision Tree <a class='headerlink' href='#section-decision-tree' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-decision-tree-model"><h1 id="data-mining-decision-tree-model-decision-tree">Decision Tree</h1>
<p>It is one of the most widely used classification techniques. It is simple, it can be trained with a limited number of examples, it is understandable and works well with categorical attributes.</p>
<p>The usage of this model is characterized by a set of questions (yes/no), which build the tree.
The idea is that the number of possible decision trees is exponential and we are looking for the best one (the one that creates the most accurate representation).</p>
<p>All the classification algorithms are systems that work in a multidimensional space ans try to find some regions that have the same types of object (belonging to the same class).</p>
<p><img alt="" src="data-mining/decision-tree/classification.jpg" /></p>
<h2 id="data-mining-decision-tree-model-learning-the-model">Learning the Model</h2>
<p>Many algorithms are available, but we will use <strong>C4.5</strong>.</p>
<p><strong>The Haunt's Algorithm</strong>
It is a recursive approach that progressively subdivides a set of Dt records into purely pure record sets.</p>
<p>Procedure to follow:</p>
<ol>
<li>If Dt contains records belonging to the yj class only, then it is a lea node with label <em>yj</em></li>
<li>If Dt is an empty set, then t is a leaf node to which a parent node class is assigned</li>
<li>If Dt contains records belonging to several classes, you choose an attribute and a split policy to partition the records into multiple subsets.</li>
<li>Apply recursively the current procedure to each subset</li>
</ol>
<div class="highlight"><pre><span></span><code>TreeGrowth(E,F)
    if StoppingCond(E,F) = TRUE then
        leaf = CreateNode()
        leaf.label = Classify(E) ;
        return leaf;
    else:
        root = CreateNode();
        root.test cond = FindBestSplit(E,F) ;
        let V = {V | v is a possible outcome of root.test_cond}
    for each v ∈ V do
        E = {e | root.test cond(e)=v and e ∈ E}
        child = TreeGrowth(E,F);
        add child as descendant of root and label edge
    end for
    end if
        return root;
    end;
</code></pre></div>
<h2 id="data-mining-decision-tree-model-characteristic-feature">Characteristic Feature</h2>
<p>Starting from the basic logic to completely define an algorithm for building decision trees, it is necessary to define:</p>
<ul>
<li>The split condition (depends on the type of attribute and on the number of splits)<ul>
<li>Nominal (N-ary split vs binary split)</li>
<li>Ordinal (partitioning should not violate the order sorting)</li>
<li>Continuous (the split condition can be expressed as a Boolean with N-ary split and as a binary comparison test with binary-split)<ul>
<li>Static (discretization takes place only once before applying the algorithm)</li>
<li>DYnamic (discretization takes place at each recursion)</li>
</ul>
</li>
</ul>
</li>
<li>The criterion defining the best split (it must allow you to determine more pure classes, using a <strong>measure of purity</strong>)<ul>
<li><img alt="" src="data-mining/decision-tree/impurity.jpg" /></li>
</ul>
</li>
<li>The criterion for interrupting splitting (AND conditions, if one applies, the splitting stops)<ul>
<li>When all its records belong to the same class</li>
<li>When all its records have similar values on all attributes</li>
<li>When the number of records in the node is below a certain threshold</li>
<li>When the selected criterion would not be statistically relevant</li>
</ul>
</li>
<li>Methods for evaluating the goodness of a decision tree</li>
</ul>
<h2 id="data-mining-decision-tree-model-metrics-for-model-evaluation">Metrics for Model Evaluation</h2>
<p><strong>Confusion Matrix</strong> evaluates the ability of a classifier based on the following indicators:</p>
<ul>
<li>TP (true positive)</li>
<li>FN (false negative)</li>
<li>FP (false positive)</li>
<li>TN (true negative)</li>
</ul>
<p><strong>Accuracy</strong> is the most widely used metric to synthesize the information of a confusion matrix</p>
<p><img alt="" src="data-mining/decision-tree/accuracy.jpg" /></p>
<ul>
<li><strong>Accuracy Limitations</strong></li>
</ul>
<p>Accuracy is not an appropriate  metric if the classes contain a very different number of records.</p>
<p><strong>Precision and Recall</strong> are two metric used in applications where the correct classification of positive class records is more important</p>
<ul>
<li><strong>Precision</strong> measures the fraction of record results actually among all those who were classified as such</li>
<li><strong>Recall</strong> measures the fraction of positive records correctly classified</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/precision-recall.jpg" /></p>
<p><strong>F-measure</strong> is a metric that summarizes precision and recall</p>
<p><strong>Cost-Based Evaluation</strong>
Accuracy, precision, recall and F-measure classify an instance as positive if P(+,i) &gt; P(-,i).
They assume that FN and FP have the same weight, thus they are cost-intensive, but in many domains this is not true.</p>
<p><img alt="" src="data-mining/decision-tree/cost.jpg" /></p>
<h2 id="data-mining-decision-tree-model-roc-space-receiver-operator-characteristics">ROC Space (Receiver Operator Characteristics)</h2>
<p>Roc graohs are two-dimensional graphs that depict relative tradeoffs between benefits (TP) and costs (FP) induced by a classifier. We distinguish between:</p>
<ul>
<li><strong>Probabilistic classifiers</strong> return a score that is not necessarily a <em>sensu strictu</em> probability but represents the degree to which an object is a member of one particular class rather than another one
-<strong>Discrete classifier</strong> predicts only the classes to which a test object belongs</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/ROC.jpg" /></p>
<h2 id="data-mining-decision-tree-model-classification-errors">Classification Errors</h2>
<ul>
<li><strong>Training error:</strong> mistakes that are made on the training set</li>
<li><strong>Generalization error:</strong>  errors made on the test set</li>
<li><strong>Underfitting:</strong> the model is too simple and does not allow a good classification or set training or test set</li>
<li><strong>Overfitting:</strong> the model is too complex, it allows a good classification of the training set, but a poor classification of the test set<ul>
<li>Due to noise (the boundaries of the areas are distorted)</li>
<li>Due to the reduced size of the training set</li>
</ul>
</li>
</ul>
<p><strong>How to handle overfitting</strong></p>
<ul>
<li>Pre-pruning: stop splitting before you reach a deep tree. A node can be split further if:<ul>
<li>Nodes does not contain instances</li>
<li>All instances belong to the same class</li>
<li>All attributes have the same values</li>
</ul>
</li>
<li>Post-pruning: run all possible splits to reduce the generalization error</li>
</ul>
<p>Post-pruning is more effective but involves more computational cost. It is based on the evidence of the result of a complete tree.</p>
<h2 id="data-mining-decision-tree-model-estimate-generalization-error">Estimate Generalization Error</h2>
<p>A decision tree should minimize the error on the real data set, unfortunately during construction, only the training set is available.</p>
<p>The methods for estimating the generalization error are:</p>
<ul>
<li>Optimistic approach</li>
<li>Pessimistic approach</li>
<li>Minimum Description Length (choose the model that minimizes the cost to describe a classification)</li>
<li>Using the test set</li>
</ul>
<h2 id="data-mining-decision-tree-model-building-the-test-set">Building the Test Set</h2>
<ul>
<li><strong>Holdout:</strong> use 2/3 of training records and 1/3 for validation</li>
<li><strong>Random subsampling:</strong> repeated execution of the holdout method in which the training dataset is randomly selected</li>
<li><strong>Cross validation:</strong> partition the records into separate k subdivisions, run the training on k-1 divisions and test the reminder, repeat the test k times and calculate the average accuracy</li>
<li><strong>Bootstrap:</strong> The extracted records are replaced and records that are excluded form the validation set. This method does not create a new dataset with more information, but it can stabilize the obtained results of the available dataset.</li>
</ul>
<h2 id="data-mining-decision-tree-model-c45-j48-on-weka">C4.5 (J48 on Weka)</h2>
<p>This algorithm exploits the GainRatio approach. It manages continuous attributes by determining a split point dividing the range of values into two.
It manages data with missed values and run post pruning of the created tree.</p></section><h1 class='nav-section-title-end'>Ended: Decision Tree</h1></div>




              

  

            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
<script id="__config" type="application/json">{"base": "/", "features": [], "search": "assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    

<script src="https://unpkg.com/iframe-worker/polyfill"></script>
<script src="search/search_index.js"></script>


    
      <script src="assets/javascripts/bundle.9c69f0bc.min.js"></script>
      
        <script src="js/print-site.js"></script>
      
        <script src="js/arithmatex.config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>