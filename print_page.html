
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.4.2">
    
    
      
        <title>Print as PDF - CescaNeri/DataMining-MachineLearning</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.69437709.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.cbb835fc.min.css">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="css/print-site-enum-headings1.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings2.css">
    
      <link rel="stylesheet" href="css/print-site-enum-headings3.css">
    
      <link rel="stylesheet" href="css/print-site.css">
    
      <link rel="stylesheet" href="css/print-site-material.css">
    
    <script>__md_scope=new URL("/",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  
        <script type="text/javascript">
        document.addEventListener('DOMContentLoaded', function () {
            remove_material_navigation();remove_mkdocs_theme_navigation();generate_toc();
        })
        </script>
        </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#section-data-mining" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="CescaNeri/DataMining-MachineLearning" class="md-header__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            CescaNeri/DataMining-MachineLearning
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Print as PDF
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue-grey" data-md-color-accent="blue-grey"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="CescaNeri/DataMining-MachineLearning" class="md-nav__button md-logo" aria-label="CescaNeri/DataMining-MachineLearning" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12Z"/></svg>

    </a>
    CescaNeri/DataMining-MachineLearning
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/CescaNeri/DataMining-MachineLearning" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_1">
          Data Mining
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Mining" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Data Mining
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/data-mining.html" class="md-nav__link">
        Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/introduction/customer-retention-case.html" class="md-nav__link">
        Customer Retention Case
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Data Understanding
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Understanding" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Data Understanding
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/data-understanding/data-understanding.html" class="md-nav__link">
        Data Understanding
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Decision Tree
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Decision Tree" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Decision Tree
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/decision-tree/model.html" class="md-nav__link">
        Decision Tree Model
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Classifier Models
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Classifier Models" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Classifier Models
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/rule-classifier.html" class="md-nav__link">
        Rule-Based classifier Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/instance-based.html" class="md-nav__link">
        Instance-Based Classifier Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/bayesian-classifier.html" class="md-nav__link">
        Bayesian Classifier
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/classifiers/multiclassifier.html" class="md-nav__link">
        Multi-Classifier
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Machine Learning" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/introduction/introduction.html" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/introduction/history.html" class="md-nav__link">
        Historical Evolution of AI
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          Data Acquisition and Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Data Acquisition and Processing" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Data Acquisition and Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-acquisition.html" class="md-nav__link">
        Data Acquisition and Processing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-types.html" class="md-nav__link">
        Data Types
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/data-acquisition/data-preparation.html" class="md-nav__link">
        Data Preparation
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" type="checkbox" id="__nav_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_7">
          Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Model" data-md-level="1">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/model/model.html" class="md-nav__link">
        Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/model/pattern.html" class="md-nav__link">
        Pattern Recognition
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          DM - LAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="DM - LAB" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          DM - LAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/weka-lab.html" class="md-nav__link">
        Introduction to Weka
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/bank-data.html" class="md-nav__link">
        Bank Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="data-mining/weka-lab/census-data.html" class="md-nav__link">
        Census Data
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_9" type="checkbox" id="__nav_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_9">
          ML - LAB
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="ML - LAB" data-md-level="1">
        <label class="md-nav__title" for="__nav_9">
          <span class="md-nav__icon md-icon"></span>
          ML - LAB
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="machine-learning/LAB/lab-notes.html" class="md-nav__link">
        Lab Notes
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Print as PDF
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="print_page.html" class="md-nav__link md-nav__link--active">
        Print as PDF
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    2. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    3. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    4. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classifier-models" class="md-nav__link">
    IV. Classifier Models
  </a>
  
    <nav class="md-nav" aria-label="IV. Classifier Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-rule-classifier" class="md-nav__link">
    5. Rule-Based classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-instance-based" class="md-nav__link">
    6. Instance-Based Classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-bayesian-classifier" class="md-nav__link">
    7. Bayesian Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-multiclassifier" class="md-nav__link">
    8. Multi-Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-machine-learning" class="md-nav__link">
    V. Machine Learning
  </a>
  
    <nav class="md-nav" aria-label="V. Machine Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-introduction" class="md-nav__link">
    9. Introduction to Machine Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-history" class="md-nav__link">
    10. Historical Evolution of AI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-acquisition-and-processing" class="md-nav__link">
    VI. Data Acquisition and Processing
  </a>
  
    <nav class="md-nav" aria-label="VI. Data Acquisition and Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-acquisition" class="md-nav__link">
    11. Data Acquisition and Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-types" class="md-nav__link">
    12. Data Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-preparation" class="md-nav__link">
    13. Data Preparation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-model" class="md-nav__link">
    VII. Model
  </a>
  
    <nav class="md-nav" aria-label="VII. Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-model" class="md-nav__link">
    14. Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-pattern" class="md-nav__link">
    15. Pattern Recognition
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-dm-lab" class="md-nav__link">
    VIII. DM - LAB
  </a>
  
    <nav class="md-nav" aria-label="VIII. DM - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-weka-lab" class="md-nav__link">
    16. Introduction to Weka
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-bank-data" class="md-nav__link">
    17. Bank Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-census-data" class="md-nav__link">
    18. Census Data
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-lab" class="md-nav__link">
    IX. ML - LAB
  </a>
  
    <nav class="md-nav" aria-label="IX. ML - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-lab-lab-notes" class="md-nav__link">
    19. Lab Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#section-data-mining" class="md-nav__link">
    I. Data Mining
  </a>
  
    <nav class="md-nav" aria-label="I. Data Mining">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-data-mining" class="md-nav__link">
    1. Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-introduction-customer-retention-case" class="md-nav__link">
    2. Customer Retention Case
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-understanding" class="md-nav__link">
    II. Data Understanding
  </a>
  
    <nav class="md-nav" aria-label="II. Data Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-data-understanding-data-understanding" class="md-nav__link">
    3. Data Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-decision-tree" class="md-nav__link">
    III. Decision Tree
  </a>
  
    <nav class="md-nav" aria-label="III. Decision Tree">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-decision-tree-model" class="md-nav__link">
    4. Decision Tree Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-classifier-models" class="md-nav__link">
    IV. Classifier Models
  </a>
  
    <nav class="md-nav" aria-label="IV. Classifier Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-rule-classifier" class="md-nav__link">
    5. Rule-Based classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-instance-based" class="md-nav__link">
    6. Instance-Based Classifier Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-bayesian-classifier" class="md-nav__link">
    7. Bayesian Classifier
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-classifiers-multiclassifier" class="md-nav__link">
    8. Multi-Classifier
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-machine-learning" class="md-nav__link">
    V. Machine Learning
  </a>
  
    <nav class="md-nav" aria-label="V. Machine Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-introduction" class="md-nav__link">
    9. Introduction to Machine Learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-introduction-history" class="md-nav__link">
    10. Historical Evolution of AI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-data-acquisition-and-processing" class="md-nav__link">
    VI. Data Acquisition and Processing
  </a>
  
    <nav class="md-nav" aria-label="VI. Data Acquisition and Processing">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-acquisition" class="md-nav__link">
    11. Data Acquisition and Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-types" class="md-nav__link">
    12. Data Types
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-data-acquisition-data-preparation" class="md-nav__link">
    13. Data Preparation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-model" class="md-nav__link">
    VII. Model
  </a>
  
    <nav class="md-nav" aria-label="VII. Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-model" class="md-nav__link">
    14. Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#machine-learning-model-pattern" class="md-nav__link">
    15. Pattern Recognition
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-dm-lab" class="md-nav__link">
    VIII. DM - LAB
  </a>
  
    <nav class="md-nav" aria-label="VIII. DM - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-weka-lab" class="md-nav__link">
    16. Introduction to Weka
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-bank-data" class="md-nav__link">
    17. Bank Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-mining-weka-lab-census-data" class="md-nav__link">
    18. Census Data
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#section-ml-lab" class="md-nav__link">
    IX. ML - LAB
  </a>
  
    <nav class="md-nav" aria-label="IX. ML - LAB">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#machine-learning-lab-lab-notes" class="md-nav__link">
    19. Lab Notes
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
  
                


<div id="print-site-page" class="print-site-enumerate-headings print-site-enumerate-figures">
        <div id="print-site-banner">
            <p>
    <em>This box will disappear when printing</em>
    <span style="float: right"><a href="https://timvink.github.io/mkdocs-print-site-plugin/">mkdocs-print-site-plugin</a></span>
</p>
<p>
    This page has combined all site pages into one. You can export to PDF using <b>File > Print > Save as PDF</b>.
</p>
<p>
    See also <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-PDF.html">export to PDF</a> and <a href="https://timvink.github.io/mkdocs-print-site-plugin/how-to/export-HTML.html">export to standalone HTML</a>.
</p>
        </div>
        
        <section class="print-page">
            <div id="print-page-toc" data-toc-depth="3">
                <nav role='navigation' class='print-page-toc-nav'>
                <h1 class='print-page-toc-title'>Index</h1>
                </nav>
            </div>
        </section>
        
                        <h1 class='nav-section-title' id='section-data-mining'>
                            Data Mining <a class='headerlink' href='#section-data-mining' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-introduction-data-mining"><h1 id="data-mining-introduction-data-mining-data-mining">Data Mining</h1>
<p>The amount of data stored on computer is constantly increasing, coming from:</p>
<ul>
<li>IoT data</li>
<li>Social data</li>
<li>Data on purchases</li>
<li>Banking and credit card transaction</li>
</ul>
<p>The first step is to collect data in a data set. This step can be automated through artificial intelligence increasing the analytical power.</p>
<p>From on side, data is more and more and on the other side, hardware becomes more powerful and cheaper each day.</p>
<p>At the same time, managers are more and more willing to rely on data analysis for their business decisions.
The information resource is a precious asset to overcoming competitors. </p>
<h2 id="data-mining-introduction-data-mining-artificial-intelligence-machine-learning-and-data-mining">Artificial Intelligence, Machine Learning and Data Mining</h2>
<p>Although strongly interrelates, the term machine learning is formally distinct from the term Data Mining which indicates the computational process of pattern discovery in large datasets using machine learning methods, artificial intelligence, statistics and databases. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-definition">Data Mining - definition</h2>
<p>Complex extraction of implicit, previously unknown and potentially useful data from the information.
Exploration and analysis, using automated and semi-automatic systems, of large amounts of data in order to find significant patterns through statistics. </p>
<p>We do not just need to find results, but we need results to be USEFUL. </p>
<h2 id="data-mining-introduction-data-mining-analytics">Analytics</h2>
<p>ANalytics refers to software used to discover, understand and share relevant pattern in data.
Analytics are based on the concurrent use of statistics, machine learning and operational research techniques, often exploiting visualization techniques. </p>
<p><img alt="" src="data-mining/introduction/BI.jpg" /></p>
<p>Prescriptive systems generale much value but it is extremely complex. Companies should start simple, adopting simple descriptive analytics solutions, and then move on. 
It is risky to skip intermediate steps.</p>
<h2 id="data-mining-introduction-data-mining-bi-adoption-path">BI adoption path</h2>
<p>When we decide to digitalize a company, the adoption of BI solutions is incremental and rarely allows steps to be skipped.
This is because it is risky, costly and useless to adopts advanced solutions before completely exploiting simple ones. </p>
<p>The goal is to create a <strong>data-driven company</strong>, where managers are supported by data. </p>
<ul>
<li>Decisions are based on quantitative rather than qualitative knowledge.</li>
<li>Process and knowledge are an asset of the company and are not lost if managers change</li>
</ul>
<p><em>The gap between a data-driven decision and a good decision is a good manager</em></p>
<p>Adopting a data-driven mindset goes far beyond adopting a business intelligence solution and entails:</p>
<ol>
<li>Create a data culture</li>
<li>Change the mindset of managers</li>
<li>Change processes</li>
<li>Improve the quality of all the data</li>
</ol>
<p><strong>Digitalization</strong> is a journey that involves three main dimensions:</p>
<p><img alt="" src="data-mining/introduction/DT.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-pattern">Pattern</h2>
<p>A pattern is a synthetic representation rich in semantics of a set of data. It usually expresses a recurring pattern in data, but can also express an exceptional pattern.</p>
<p>A pattern must be:</p>
<ul>
<li>Valid on data with a certain degree of confidence</li>
<li>It can be understood from the syntax and semantic point of view, so that the user can interpret it</li>
<li>Previously unknown and potentially useful, so that users can take actions accordingly</li>
</ul>
<p>When we distinguish between a manual technique (DW) and an automatic technique is the creation of a small subset of data which is rich in semantics.</p>
<p>The process begins with a huge multi-dimension cube of data, then grouping and selection techniques are adopted, creating a <strong>pattern</strong>.</p>
<p><strong>Pattern types:</strong></p>
<ul>
<li>Association rules (logical implications of the dataset)</li>
<li>Classifiers (classify data according to a set of priori assigned classes)</li>
<li>Decision trees (identify the causes that lead to an event, in order of importance)</li>
<li>Clustering (group elements depending on their characteristics)</li>
<li>Time series (detection of recurring or atypical patterns in complex data sequences)</li>
</ul>
<h2 id="data-mining-introduction-data-mining-data-mining-applications">Data Mining Applications</h2>
<p><strong>Predictive Systems</strong>
Exploit some features to predict the unknown values of other features (classification and regression).</p>
<p><strong>Descriptive Systems</strong>
Find user-readable patterns that can be understood by human users (clustering, association rules, sequential patter).</p>
<h2 id="data-mining-introduction-data-mining-classification-definition">Classification - Definition</h2>
<p>Given a <strong>record</strong> set, where each record is composed by a set of attributes (one of them represents the class of the record), find a model for the class attribute expressing the attribute value as a function of the remaining attributes.</p>
<p><em>Given a feature (defined at priori), define weather a user belongs to that feature</em></p>
<p>This model must work even when the record is not given. Unclassified record must be assigned to a class in the most accurate way.</p>
<p>A <strong>test set</strong> is used to determine the model accuracy.</p>
<p><img alt="" src="data-mining/introduction/test.jpg" /></p>
<h2 id="data-mining-introduction-data-mining-classification-example">Classification example</h2>
<p><strong>Direct Marketing:</strong>
The goal is to reduce the cost of email marketing by defining the set of customers that, with the highest probability, will buy a new product.</p>
<p>Technique:</p>
<ul>
<li>Exploit the data collected during the launch of similar products<ul>
<li>We know which customers bought and which one did not</li>
<li>{<em>buy, not buy</em>} = <strong>class attribute</strong></li>
</ul>
</li>
<li>Collect all the available information about each customers</li>
<li>Use such information as an input to train the model</li>
</ul>
<p><strong>Churn Detection</strong>
Predict customers who are willing to go to a competitor.</p>
<p>Technique:</p>
<ul>
<li>Use the purchasing data of individual users to find the relevant attributes</li>
<li>Label users as {<em>loyal, not loyal</em>}</li>
<li>Find a pattern that defines loyalty</li>
</ul>
<h2 id="data-mining-introduction-data-mining-clustering-example">Clustering example</h2>
<p>Given a set of points, each featuring set of attributes, and having a similarity measure between points, find subset of points such that:
<em>points belonging to a cluster are more similar to each other than those belonging to other clusters</em></p>
<p><strong>Marketing Segmentation</strong>
The goal is to spit customers into distinct subsets to target specific marketing activities.</p>
<p>Techniques:</p>
<ul>
<li>Gather information about customer lifestyle and geographic location</li>
<li>Find clusters of similar customers</li>
<li>Measure cluster quality by verifying whether the purchasing patterns of customers belonging to the same cluster are more similar to those of distinct clusters</li>
</ul>
<h2 id="data-mining-introduction-data-mining-association-rules-example">Association Rules example</h2>
<p>Given a set of records each consisting of multiple elements belonging to a given collection.
It produces rules of dependence that predict the occurrence of one of the elements in the presence of others.</p>
<p><strong>Marketing Sales Promotion</strong>
Suppose you have discovered this association rule:
{<em>Bagels,...} -&gt; {</em>Potato chips*}</p>
<p>This information can be used to understand what actions to take to increase its sales. </p>
<h2 id="data-mining-introduction-data-mining-data-mining-bets">Data Mining Bets</h2>
<ul>
<li>Scalability</li>
<li>Multidimensionality of data set</li>
<li>Complexity and heterogeneity of the data</li>
<li>Data quality</li>
<li>Data properties</li>
<li>Privacy keeping</li>
<li>Processing in real-time</li>
</ul>
<h2 id="data-mining-introduction-data-mining-crisp-methodology">CRISP methodology</h2>
<p>A data mining project requires a structured approach in order to choose the best algorithm.</p>
<p><strong>CRISP-DM</strong> methodology is the most used technique. It is one of the most structures proposals to define the fundamental steps of a data mining project. </p>
<p><img alt="" src="data-mining/introduction/crisp.jpg" /></p>
<p>The six stages of the life cycle are not strictly sequential, indeed, it is often necessary.</p>
<ol>
<li><strong>Business understanding</strong> (understand the application domain): understanding project goals from users' point of view, translate the user's problem into a data mining problem and define a project plan.<ul>
<li>Get an idea about the business domain and the data mining approach to adopt.</li>
</ul>
</li>
<li><strong>Data understanding</strong>: preliminary data collection aimed at identifying quality problems and conducting preliminary analysis to identify the salient characteristics.</li>
<li><strong>Data preparation</strong>: tasks needed to create the final dataset, selecting attributes and records, transforming and cleaning data.<ul>
<li>Prepare the data for ML tasks (clean, complete missing data, create new features)</li>
</ul>
</li>
<li><strong>Model creation</strong>: data mining techniques are applied to the dataset in order to identify what makes the model more accurate.</li>
<li><strong>Evaluation of model and results</strong>: the model obtained from the previous phase are analyzed to verify that they are sufficiently precise and robust to respond adequately to the user's objectives.</li>
<li><strong>Deployment</strong>: the built-in model and acquired knowledge must be made available to users.<ul>
<li>Change the software and processes to include new AI functionalities</li>
</ul>
</li>
</ol>
<p>Different classes of data mining use different algorithms so the evaluation changes accordingly. </p></section><section class="print-page" id="data-mining-introduction-customer-retention-case"><h1 id="data-mining-introduction-customer-retention-case-customer-retention">Customer Retention</h1>
<p>Customer retention, churn analysis, dropout analysis are synonyms for predictive analysis carried out by organizations and companies to avoid losing customers.</p>
<p>The idea is to create a different profile for customers who stay and customers who drop-out.</p>
<p><strong>The Gym Case Study</strong></p>
<p>They discovered that customers who did not train well, eventually drop out from the gym.
Therefore, the goal was to model customers' training sessions in order to predict those who did not train well and prevent them from dropping out.</p>
<p>Steps:</p>
<ul>
<li>Customers have s list of exercises</li>
<li>The system records the exercises (and repetition) did during the workout</li>
<li>The system matches the exercises </li>
<li>Train a classifier that is able to predict that someone is leaving the gym because he is unsatisfied<ul>
<li>The system update the profile each week</li>
<li>Four weeks without training = dropout</li>
<li>The idea of dropout needs to be defined properly (a customer who stops going to the gym in summer and comes back in summer is different from a customers who dropout and does not come back)</li>
</ul>
</li>
</ul>
<p>Practitioner who is about to leave the gym is training poorly. How can characterize the user behaviors? How long does it last?</p>
<p>Many KPIs can be adopted to assess the training session: in this case, two indicators were identified:</p>
<ol>
<li><strong>Compliance</strong> (adherence of the performed workout)</li>
<li><strong>Regularity</strong> (regularity of the training sessions with reference to the prescribed one)</li>
</ol>
<p>We still have a problem of <strong>granularity:</strong> we can assess regularity by checking steps, repetition, physical activity, muscle or body part. </p></section><h1 class='nav-section-title-end'>Ended: Data Mining</h1>
                        <h1 class='nav-section-title' id='section-data-understanding'>
                            Data Understanding <a class='headerlink' href='#section-data-understanding' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-data-understanding-data-understanding"><h1 id="data-mining-data-understanding-data-understanding-data-understanding-preparation">Data Understanding &amp; Preparation</h1>
<p>In data mining, data are composed of collections of objects described by a set of attributes (we refer to data that can be stored in a database).</p>
<p><strong>Attribute:</strong> property characteristic of an object</p>
<h2 id="data-mining-data-understanding-data-understanding-attribute-types">Attribute types</h2>
<p>In order to perform meaningful analysis, the characteristics of the attributes must be known. The <strong>attribute type</strong> tells us what properties are reflected in the value we use as a measure. </p>
<p>We can identify 4 types of attributes:</p>
<ol>
<li><strong>Nominal</strong>-qualitative: different names of value (gender, zip code, ID)</li>
<li><strong>Ordinal</strong>-qualitative: values enables us to sort objects based on the value of attribute (grade)</li>
<li><strong>Interval</strong>-quantitative: the difference between the values has a meaning, with a unit of measurement (dates, temperature)</li>
<li><strong>Ratio</strong>-quantitative: the ratio of values has meaning (age, length, amount of money)</li>
</ol>
<h2 id="data-mining-data-understanding-data-understanding-further-classifications">Further classifications</h2>
<ul>
<li>Binary, discrete and continuous<ul>
<li>Discrete: finite number of infinite countable set of values</li>
<li>Continuous: real values
<em>Nominal and ordinal are typically discrete or binary, while interval and ratio attributes are continuous</em></li>
</ul>
</li>
<li>Asymmetric attribute: only instances that take non-zero values are relevant</li>
<li>Documents and Texts: objects of the analysis described by a vector of terms</li>
<li>Transactions<ul>
<li>Each record involves multiple items</li>
<li>Items come from a finite set</li>
<li>The number of items may vary from transaction to transaction</li>
</ul>
</li>
<li>Ordered data</li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-explorative-analysis">Explorative Analysis</h2>
<p>First step in business and ata understanding. It refers to the preliminary analysis of the data aimed at identify its main characteristics. </p>
<ul>
<li>It helps you choose the best tool for processing and analysis </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-statistics-overview">STATISTICS OVERVIEW</h2>
<h3 id="data-mining-data-understanding-data-understanding-frequency">Frequency</h3>
<p>The frequency of an attribute value is the percentage of times that value appears in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-mode">Mode</h3>
<p>The mode of an attribute is the value that appears most frequently in the data set.</p>
<h3 id="data-mining-data-understanding-data-understanding-percentile">Percentile</h3>
<p>Given an ordinal or continuous attribute x and a  number p between 0 and 100, the p-th percentile is the value of xp of x such that p% of the observed values for x are lower than xp.</p>
<p><img alt="" src="data-mining/data-understanding/boxplot.jpg" /></p>
<p>Percentile visualization through boxplot enables the representation of a distribution of data. It can be used to compare multiple distributions when they have homogeneous magnitude.</p>
<h3 id="data-mining-data-understanding-data-understanding-mean">Mean</h3>
<p>The mean is the most common measure for locating a set of points.</p>
<ul>
<li>Subject to outliers</li>
<li>It is preferred to use tee median or a 'controlled' mean </li>
</ul>
<h3 id="data-mining-data-understanding-data-understanding-median">Median</h3>
<p>The median is the term occupying the central place if the terms are odd; if the terms are even, the median is the arithmetic mean of the two central terms.</p>
<h3 id="data-mining-data-understanding-data-understanding-range">Range</h3>
<p>Range is the difference between the minimum and maximum values taken by the attribute.</p>
<h3 id="data-mining-data-understanding-data-understanding-variance-and-standard-deviation">Variance and Standard Deviation</h3>
<p>Variance and SD are the most common measures of dispersion of a data set.</p>
<ul>
<li>Sensitive to outliers since they are quadratically related to the concept of mean</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/var-sd.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-data-quality">Data Quality</h2>
<p>The quality of the datasets profoundly affects the chances of finding meaningful patterns.
The most frequent problems that deteriorate data quality are:</p>
<ul>
<li>Noise and outliers (objects with characteristics very different from all other objects in the data set)</li>
<li>Missing values (not collecting the data is different from when the attribute is not applicable), how to handle them:<ul>
<li>Delete the objects that contain them</li>
<li>Ignore missing values during analysis</li>
<li>Manually/automatically fill the missing values<ul>
<li>ML can be applied to fill the missing values by inferring the other values of that attribute and calculate the most appropriate value </li>
</ul>
</li>
</ul>
</li>
<li>Duplicated values (it may be necessary to introduce a data cleaning step in order to identify and eliminate redundancy)</li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-dataset-preprocessing">Dataset Preprocessing</h2>
<p>Rarely the dataset has the optimal characteristics to be best processed by machine learning algorithms. It is therefore necessary to put in place a series of actions to enable the algorithms of interest to function:</p>
<ul>
<li><strong>Aggregation:</strong> combine two or more attributes into one attribute</li>
<li><strong>Sampling:</strong> main technique to select data<ul>
<li>Collecting and processing the entire dataset is too expensive and time consuming</li>
<li>Simple Random Sampling (same probability of selecting each element)</li>
<li>Stratified sampling (divides the data into multiple partitions and use simple random sampling on each partition)<ul>
<li>Before sampling a partitioning rule is applied (we inject knowledge about the domain)</li>
<li>Allow the population to be balanced</li>
<li>However, we are applying a distortion</li>
</ul>
</li>
<li>Sampling Cardinality: after choosing the sampling mode, it is necessary to fix the sample size in order to limit the loss of information</li>
</ul>
</li>
<li><strong>Dimensionality reduction:</strong> the goal is to avoid the 'curse of dimensionality', reduce the amount of time and memory used by ML algorithms, simplify data visualization and eliminate irrelevant attributes and eliminate noise on data.
Curse of dimensionality: as dimensionality increases, the data become progressively more sparse. Many clustering and classification algorithms deal with dimensionality and distances. All the elements become equi-distant from one another; the idea of selecting the right dimension to carry out analysis is crucial.</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/dimension.jpg" /></p>
<p>The curve indicates that the more we increase the number of dimensionality, the smaller the ratio is.
In the modeling phase, it is important reduce dimensionality.</p>
<p>The goal is to reduce dimensionality and carry out analysis with the highest information amount.</p>
<ul>
<li><strong>Principal Component Analysis:</strong> it is a projection method that transforms objects belonging to a p-dimensional space into a k-dimensional space in such way as to preserve maximum information in the initial dimension.</li>
<li><strong>Attribute creation:</strong> it is a way to reduce the dimensionality of data. The selection usually aims to eliminate redundant. 
We can use different attribute selection techniques:<ul>
<li>Exhaustive approaches</li>
<li>Non-exhaustive approaches</li>
<li>Feature engineering (create new features): we have raw data and we can extract useful KPIs by designing new attributes.</li>
</ul>
</li>
<li>
<p><strong>Discretization and binarization:</strong> transformation of continuos-valued attributes to discrete-valued attributes. Discretization techniques can be unsupervised (do not exploit knowledge about the class to which elements belong) or supervised (exploit knowledge about the class to which the elements belong).</p>
<ul>
<li>
<p>Unsupervised: equi-width, equi-frequency, K-means
<img alt="" src="data-mining/data-understanding/discretization.jpg" /></p>
</li>
<li>
<p>Supervised: discretization intervals are positioned to maximize the 'purity' of the intervals</p>
</li>
</ul>
</li>
</ul>
<p><strong>Entropy and Information Gain:</strong> it is the measure of uncertainty about the outcome of an experiment that can be modeled by a random variable x.
The entropy of a <strong>certain</strong> event is zero.</p>
<p>The entropy of a discretization into n intervals depends on how pure each group.</p>
<p><img alt="" src="data-mining/data-understanding/entropy.jpg" /></p>
<p><strong>Binarization:</strong> we start with a discrete attribute but we need it to be binary.</p>
<ul>
<li><strong>Attribute transformation:</strong> function that maps the entire set of values of an attribute to a new set such that each value in the starting set corresponds to a unique value in the ending set. </li>
</ul>
<h2 id="data-mining-data-understanding-data-understanding-similarity-and-dissimilarity">Similarity and Dissimilarity</h2>
<p>These two concepts are central in Machine Learning, as it is important to group clusters based on similarity and dissimilarity.</p>
<p>Some techniques are stronger with long distances while sometimes, by setting the wrong distance, we will incur in problems.</p>
<ul>
<li>
<p><strong>Similarity:</strong> it is a numerical measure expressing the degree of similarity between two objects</p>
<ul>
<li>Takes values in the range [0, 1]</li>
</ul>
</li>
<li>
<p><strong>Dissimilarity (distance):</strong> it is a numerical measure expressing the degree of difference between two objects</p>
<ul>
<li>Takes values in the range [0, 1] or [0, ∞]</li>
</ul>
</li>
</ul>
<p><img alt="" src="data-mining/data-understanding/similarity.jpg" /></p>
<h2 id="data-mining-data-understanding-data-understanding-distance">Distance</h2>
<p><img alt="" src="data-mining/data-understanding/distance.jpg" /></p>
<p><strong>Distance Properties</strong>
Given two objects p and q and a dissimilarity measure d():</p>
<ul>
<li>d(p,q) = 0 only if p=q</li>
<li>d(p,q) = d(q,p) -&gt; <em>Symmetry</em> </li>
<li>d(p,r) + d(p,q) + d(q,r) -&gt; <em>Triangular inequality</em></li>
</ul>
<p><img alt="" src="data-mining/data-understanding/triangular.jpg" /></p>
<p><strong>Similarity Properties</strong>
Given two objects p and q and a similarity measure s():</p>
<ul>
<li>s(p,q) = 1 only if p=q</li>
<li>s(p,q) = s(q,p) -&gt; <em>Symmetry</em></li>
</ul>
<p><strong>Binary Vector Similarities</strong>
It is common for attributes describing an object to contain only binary values.</p>
<ul>
<li>M01 = the number of attributes where p=0 and q=1</li>
<li>M10 = the number of attributes where p=1 and q=0</li>
<li>M00 = the number of attributes where p=0 and q=0</li>
<li>M11 = the number of attributes where p=1 and q=1</li>
</ul>
<p><strong>Cosine Similarity</strong>
Like Jaccard's index. it does not consider 00 matches, but also allows non-binary vectors to be operated on.</p>
<p><strong>Similarity with Heterogeneous Attributes</strong>
In the presence of heterogeneous attributes, it is necessary to compute the similarities separately and then combine them so that their result belongs to the range [0, 1]</p>
<h2 id="data-mining-data-understanding-data-understanding-correlation">Correlation</h2>
<p>The correlation between pairs of objects described by attributes (binary or continuous) is a measure of the existence of a linear relationship between its attributes.</p>
<p><img alt="" src="data-mining/data-understanding/correlation.jpg" /></p>
<p><img alt="" src="data-mining/data-understanding/corr2.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Data Understanding</h1>
                        <h1 class='nav-section-title' id='section-decision-tree'>
                            Decision Tree <a class='headerlink' href='#section-decision-tree' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-decision-tree-model"><h1 id="data-mining-decision-tree-model-decision-tree">Decision Tree</h1>
<p>It is one of the most widely used classification techniques. It is simple, it can be trained with a limited number of examples, it is understandable and works well with categorical attributes.</p>
<p>The usage of this model is characterized by a set of questions (yes/no), which build the tree.
The idea is that the number of possible decision trees is exponential and we are looking for the best one (the one that creates the most accurate representation).</p>
<p>All the classification algorithms are systems that work in a multidimensional space ans try to find some regions that have the same types of object (belonging to the same class).</p>
<p><img alt="" src="data-mining/decision-tree/classification.jpg" /></p>
<h2 id="data-mining-decision-tree-model-learning-the-model">Learning the Model</h2>
<p>Many algorithms are available, but we will use <strong>C4.5</strong>.</p>
<p><strong>The Haunt's Algorithm</strong>
It is a recursive approach that progressively subdivides a set of Dt records into purely pure record sets.</p>
<p>Procedure to follow:</p>
<ol>
<li>If Dt contains records belonging to the yj class only, then it is a lea node with label <em>yj</em></li>
<li>If Dt is an empty set, then t is a leaf node to which a parent node class is assigned</li>
<li>If Dt contains records belonging to several classes, you choose an attribute and a split policy to partition the records into multiple subsets.</li>
<li>Apply recursively the current procedure to each subset</li>
</ol>
<div class="highlight"><pre><span></span><code>TreeGrowth(E,F)
    if StoppingCond(E,F) = TRUE then
        leaf = CreateNode()
        leaf.label = Classify(E) ;
        return leaf;
    else:
        root = CreateNode();
        root.test cond = FindBestSplit(E,F) ;
        let V = {V | v is a possible outcome of root.test_cond}
    for each v ∈ V do
        E = {e | root.test cond(e)=v and e ∈ E}
        child = TreeGrowth(E,F);
        add child as descendant of root and label edge
    end for
    end if
        return root;
    end;
</code></pre></div>
<h2 id="data-mining-decision-tree-model-characteristic-feature">Characteristic Feature</h2>
<p>Starting from the basic logic to completely define an algorithm for building decision trees, it is necessary to define:</p>
<ul>
<li>The split condition (depends on the type of attribute and on the number of splits)<ul>
<li>Nominal (N-ary split vs binary split)</li>
<li>Ordinal (partitioning should not violate the order sorting)</li>
<li>Continuous (the split condition can be expressed as a Boolean with N-ary split and as a binary comparison test with binary-split)<ul>
<li>Static (discretization takes place only once before applying the algorithm)</li>
<li>DYnamic (discretization takes place at each recursion)</li>
</ul>
</li>
</ul>
</li>
<li>The criterion defining the best split (it must allow you to determine more pure classes, using a <strong>measure of purity</strong>)<ul>
<li><img alt="" src="data-mining/decision-tree/impurity.jpg" /></li>
</ul>
</li>
<li>The criterion for interrupting splitting (AND conditions, if one applies, the splitting stops)<ul>
<li>When all its records belong to the same class</li>
<li>When all its records have similar values on all attributes</li>
<li>When the number of records in the node is below a certain threshold</li>
<li>When the selected criterion would not be statistically relevant</li>
</ul>
</li>
<li>Methods for evaluating the goodness of a decision tree</li>
</ul>
<h2 id="data-mining-decision-tree-model-metrics-for-model-evaluation">Metrics for Model Evaluation</h2>
<p><strong>Confusion Matrix</strong> evaluates the ability of a classifier based on the following indicators:</p>
<ul>
<li>TP (true positive)</li>
<li>FN (false negative)</li>
<li>FP (false positive)</li>
<li>TN (true negative)</li>
</ul>
<p><strong>Accuracy</strong> is the most widely used metric to synthesize the information of a confusion matrix</p>
<p><img alt="" src="data-mining/decision-tree/accuracy.jpg" /></p>
<ul>
<li><strong>Accuracy Limitations</strong></li>
</ul>
<p>Accuracy is not an appropriate  metric if the classes contain a very different number of records.</p>
<p><strong>Precision and Recall</strong> are two metric used in applications where the correct classification of positive class records is more important</p>
<ul>
<li><strong>Precision</strong> measures the fraction of record results actually among all those who were classified as such</li>
<li><strong>Recall</strong> measures the fraction of positive records correctly classified</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/precision-recall.jpg" /></p>
<p><strong>F-measure</strong> is a metric that summarizes precision and recall</p>
<p><strong>Cost-Based Evaluation</strong>
Accuracy, precision, recall and F-measure classify an instance as positive if P(+,i) &gt; P(-,i).
They assume that FN and FP have the same weight, thus they are cost-intensive, but in many domains this is not true.</p>
<p><img alt="" src="data-mining/decision-tree/cost.jpg" /></p>
<h2 id="data-mining-decision-tree-model-roc-space-receiver-operator-characteristics">ROC Space (Receiver Operator Characteristics)</h2>
<p>Roc graohs are two-dimensional graphs that depict relative tradeoffs between benefits (TP) and costs (FP) induced by a classifier. We distinguish between:</p>
<ul>
<li><strong>Probabilistic classifiers</strong> return a score that is not necessarily a <em>sensu strictu</em> probability but represents the degree to which an object is a member of one particular class rather than another one
-<strong>Discrete classifier</strong> predicts only the classes to which a test object belongs</li>
</ul>
<p><img alt="" src="data-mining/decision-tree/ROC.jpg" /></p>
<h2 id="data-mining-decision-tree-model-classification-errors">Classification Errors</h2>
<ul>
<li><strong>Training error:</strong> mistakes that are made on the training set</li>
<li><strong>Generalization error:</strong>  errors made on the test set</li>
<li><strong>Underfitting:</strong> the model is too simple and does not allow a good classification or set training or test set</li>
<li><strong>Overfitting:</strong> the model is too complex, it allows a good classification of the training set, but a poor classification of the test set<ul>
<li>Due to noise (the boundaries of the areas are distorted)</li>
<li>Due to the reduced size of the training set</li>
</ul>
</li>
</ul>
<p><strong>How to handle overfitting</strong></p>
<ul>
<li>Pre-pruning: stop splitting before you reach a deep tree. A node can be split further if:<ul>
<li>Nodes does not contain instances</li>
<li>All instances belong to the same class</li>
<li>All attributes have the same values</li>
</ul>
</li>
<li>Post-pruning: run all possible splits to reduce the generalization error</li>
</ul>
<p>Post-pruning is more effective but involves more computational cost. It is based on the evidence of the result of a complete tree.</p>
<h2 id="data-mining-decision-tree-model-estimate-generalization-error">Estimate Generalization Error</h2>
<p>A decision tree should minimize the error on the real data set, unfortunately during construction, only the training set is available.</p>
<p>The methods for estimating the generalization error are:</p>
<ul>
<li>Optimistic approach</li>
<li>Pessimistic approach</li>
<li>Minimum Description Length (choose the model that minimizes the cost to describe a classification)</li>
<li>Using the test set</li>
</ul>
<h2 id="data-mining-decision-tree-model-building-the-test-set">Building the Test Set</h2>
<ul>
<li><strong>Holdout:</strong> use 2/3 of training records and 1/3 for validation</li>
<li><strong>Random subsampling:</strong> repeated execution of the holdout method in which the training dataset is randomly selected</li>
<li><strong>Cross validation:</strong> partition the records into separate k subdivisions, run the training on k-1 divisions and test the reminder, repeat the test k times and calculate the average accuracy</li>
<li><strong>Bootstrap:</strong> The extracted records are replaced and records that are excluded form the validation set. This method does not create a new dataset with more information, but it can stabilize the obtained results of the available dataset.</li>
</ul>
<h2 id="data-mining-decision-tree-model-c45-j48-on-weka">C4.5 (J48 on Weka)</h2>
<p>This algorithm exploits the GainRatio approach. It manages continuous attributes by determining a split point dividing the range of values into two.
It manages data with missed values and run post pruning of the created tree.</p></section><h1 class='nav-section-title-end'>Ended: Decision Tree</h1>
                        <h1 class='nav-section-title' id='section-classifier-models'>
                            Classifier Models <a class='headerlink' href='#section-classifier-models' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-classifiers-rule-classifier"><h1 id="data-mining-classifiers-rule-classifier-rule-based-classifier">Rule-Based Classifier</h1>
<p>The basic idea is to classify records using rule sets of the type "<em>if .. then</em>". The condition used with 'if' is called the antecedent while the predicted class of each rule is called the consequent.</p>
<p>A rule has the form: (condition) -&gt; y</p>
<p>Building a model means identifying a set of rules</p>
<p><img alt="" src="data-mining/classifiers/properties.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-coverage-and-accuracy">Coverage and Accuracy</h2>
<p>We can have very accurate rules but with low coverage, which is not that relevant.
Given a dataset D and a classification rule A -&gt; y, we define:</p>
<ul>
<li><strong>Coverage</strong> as the portion of records satisfying the antecedent of the rule<ul>
<li>Coverage = |A|/|D|</li>
</ul>
</li>
<li><strong>Accuracy</strong> as the fraction that, by satisfying the antecedent, also satisfy the consequent<ul>
<li>Accuracy = |A ∩ y|/|A|</li>
</ul>
</li>
</ul>
<p>A set of rules R us said to be <strong>mutually exclusive</strong> if no pair of rules can be activated by the same record.</p>
<p>A set of rules R has <strong>exhaustive coverage</strong> if there is one rule for each combination of attribute values.</p>
<h2 id="data-mining-classifiers-rule-classifier-properties">Properties</h2>
<ul>
<li>It is nt always possible to determine an exhaustive and mutually exclusive set of rules</li>
<li>Lack of mutual exclusivity</li>
<li>Lack of exhaustiveness</li>
</ul>
<h2 id="data-mining-classifiers-rule-classifier-rule-sorting-approach">Rule Sorting Approach</h2>
<ol>
<li>Rule-based sorting (individual rules are sort according to their quality)</li>
<li>Class-based sorting (groups of rules that determine the same class appear consequently in the list)</li>
</ol>
<p><img alt="" src="data-mining/classifiers/sorting.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-sequential-covering">Sequential Covering</h2>
<p><div class="highlight"><pre><span></span><code><span class="nb">set</span> <span class="n">R</span> <span class="o">=</span> <span class="n">Ø</span>
<span class="k">for</span> <span class="n">each</span> <span class="k">class</span> <span class="nc">y</span> <span class="err">∈</span> <span class="n">Y</span> <span class="mi">0</span> <span class="n">y</span> <span class="n">k</span> <span class="n">do</span>
    <span class="n">stop</span><span class="o">=</span><span class="n">FALSE</span><span class="p">;</span>
<span class="k">while</span> <span class="err">!</span><span class="n">stop</span> <span class="n">do</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">Learn</span> <span class="n">One</span> <span class="n">Rule</span><span class="p">(</span><span class="n">E</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    <span class="n">remove</span> <span class="kn">from</span> <span class="nn">E</span> <span class="n">training</span> <span class="n">records</span> <span class="n">that</span> <span class="n">are</span> <span class="n">covered</span> <span class="n">by</span> <span class="n">r</span>
    <span class="n">If</span> <span class="n">Quality</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">E</span> <span class="p">)</span> <span class="o">&lt;</span> <span class="n">Threshold</span> <span class="n">then</span>
        <span class="n">stop</span><span class="o">=</span><span class="n">TRUE</span><span class="p">;</span>
    <span class="k">else</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="err">∪</span> <span class="n">r</span> <span class="o">//</span> <span class="n">Add</span> <span class="n">r</span> <span class="n">at</span> <span class="n">the</span> <span class="n">bottom</span> <span class="n">of</span> <span class="n">the</span> <span class="n">rule</span> <span class="nb">list</span>
<span class="n">end</span> <span class="k">while</span>
<span class="n">end</span> <span class="k">for</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="err">∪</span> <span class="p">{{}</span> <span class="o">-&gt;</span> <span class="n">y</span> <span class="n">k</span> <span class="p">}</span> <span class="o">//</span> <span class="n">Add</span> <span class="n">the</span> <span class="n">default</span> <span class="n">rule</span> <span class="n">at</span> <span class="n">the</span> <span class="n">bottom</span> <span class="n">of</span> <span class="n">the</span> <span class="n">rule</span> <span class="nb">list</span>
<span class="n">PostPruning</span> <span class="p">(</span><span class="n">R</span><span class="p">);</span>
</code></pre></div>
<img alt="" src="data-mining/classifiers/sequantial.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-dropping-instances-from-training-set">Dropping instances from Training Set</h2>
<p>Deleting instances from the training set serves the purpose of:</p>
<ul>
<li>Properly classified instances: to avoid generating the same rule again and again, avoid overestimating the accuracy of the next rule</li>
<li>Incorrectly classified instances: to avoid underestimating the accuracy of the next rule</li>
</ul>
<p><img alt="" src="data-mining/classifiers/dropping.jpg" /></p>
<h2 id="data-mining-classifiers-rule-classifier-learn-one-rule">Learn-One-Rule</h2>
<p>We want something that is general (even with a lower accuracy). The goal of the algorithm is to find a rule that covers as many possible examples and as few as possible negative examples.</p>
<p>Rule are constructed by progressively considering a new possible predicate.</p>
<ul>
<li>In order to choose which predicate to add, a criterion is needed:<ul>
<li>n = number of instances covered by the rule</li>
<li>nr = number of instances properly classified by the rule</li>
<li>k = number of classes</li>
</ul>
</li>
</ul>
<p><strong>Accuracy(r) = nr/n</strong></p>
<p>Some metrics (like the <strong>FoilGrain</strong>) supports the rule by identifying the number of positive examples covered by the rule.</p>
<p><strong>Stop Criterion</strong>: as soon as the rule is not relevant anymore, stop it.</p>
<p><strong>Rule Pruning</strong>: it aims at simplifying tules to improve rule generalization error.
It can be useful given that the construction approach is greedy.</p>
<p><em>example: remove the predicate whose removal results in the greatest improvement in error rate on the validation set</em></p>
<h2 id="data-mining-classifiers-rule-classifier-the-ripper-method">The RIPPER Method</h2>
<p>It is an approach based on sequential covering for 2-class problem and it is used to choose one of the classes as a positive class and the other as a negative class.</p>
<p>the idea is to compute the description length (cost for transmitting the data set from one user to another) and if it exceeds the threshold, we should sthop.</p>
<p><img alt="" src="data-mining/classifiers/indirect.jpg" /></p></section><section class="print-page" id="data-mining-classifiers-instance-based"><h1 id="data-mining-classifiers-instance-based-instance-based-classifier">Instance-Based Classifier</h1>
<p>These classifiers do not build models but classify new records based on their similarity to the examples in the training set.</p>
<p>They are called <em>lazy-diligent learners</em> as opposed to <em>impatient  learners</em> (rule-based, decision trees).</p>
<p><img alt="" src="data-mining/classifiers/knn.jpg" /></p>
<p><strong>K-Nearest Neighbor</strong></p>
<p>K-Nearest Neighbor is a simple algorithm that stores all the available cases and classifies the new data or case based on a similarity measure.</p>
<p>It is mostly used to classify a data point based on how its neighbors are classified.</p>
<p>Requirements:</p>
<ul>
<li>A training set</li>
<li>A metric to calculate the distance between records</li>
<li>The value of k (the number of neighbors to be used)</li>
</ul>
<p>The classification process calculates the distance to the records in the training set, it identifies k nearest neighbors and uses nearest neighbor class labels to determine the class of the unknown record.</p>
<p>The choice of k is important because:
- If k is too small, the approach is sensitive to noise
- If k is too large, the surround may include examples belonging to other classes</p>
<p><em>Remember to normalize attributes in pre-processing, because to operate correctly, they should have the same scale of values.</em></p>
<p><strong>Pros of KNN:</strong></p>
<ul>
<li>Do not require the construction of a model</li>
<li>Compared with rule-based or decision tree systems, they allow the construction of nonlinear class (more flexible)</li>
</ul>
<p><strong>Cons of KNN:</strong></p>
<ul>
<li>Require a similarity or distance measure to assess closeness</li>
<li>Require a pre-processing step to normalize the range of variation of attributes</li>
<li>Class is locally determined and therefore susceptible to data noise</li>
<li>Very sensitive to the presence of irrelevant or related attributes that will distort distances between objects</li>
<li>Classification cost can be high and depends linearly on the size of the training set in the absence of appropriate index structures</li>
</ul>
<h2 id="data-mining-classifiers-instance-based-the-r-tree-index-structure">The R-Tree Index Structure</h2>
<p>R-trees are extensions of B+-trees to multi-dimensional spaces:</p>
<ul>
<li>B+-trees organize objects into a set of non-overlapping one-dimensional intervals, applying this principle recursively from the leaves of the root</li>
<li>R-trees organize objects into a set of overlapping multi-dimensional intervals, applying this principle recursively from the leaves to the root</li>
</ul>
<p><img alt="" src="data-mining/classifiers/rtree.jpg" /></p></section><section class="print-page" id="data-mining-classifiers-bayesian-classifier"><h1 id="data-mining-classifiers-bayesian-classifier-bayesian-classifier">Bayesian Classifier</h1>
<p>It is a probabilistic approach to solving classification problems.
In many applications, the relationship between attribute values and that of the class is not deterministic, due to noise data, hidden variables and difficulty in quantifying certain aspects.</p>
<ul>
<li><strong>Uncertainty about the outcome prediction</strong></li>
</ul>
<p>Bayesian classifier model probabilistic relationships between attributes and the classification attribute.</p>
<p><img alt="" src="data-mining/classifiers/stats.jpg" /></p>
<h2 id="data-mining-classifiers-bayesian-classifier-naive-bayes">Naïve Bayes</h2>
<p>The main advantage of probabilistic reasoning over logical reasoning lies in the possibility of arriving at rational descriptions even when there is not enough deterministic information about how the system works.</p>
<p>This classifier is <strong>robust</strong> toward irrelevant attributes. </p>
<p>It provide optimal results if:</p>
<ul>
<li>The conditional independence condition is met</li>
<li>The probability distributions of P(X|Y) are known</li>
</ul>
<p><img alt="" src="data-mining/classifiers/naive.jpg" /></p>
<p>13.5 is the solution that minimizes the error.</p>
<h2 id="data-mining-classifiers-bayesian-classifier-probability-with-continuous-attributes">Probability with Continuous Attributes</h2>
<p>In case attribute A is continuous, it is not possible to estimate probability for each of its values.</p>
<p>We need to <strong>discretize</strong> the attribute into intervals by creating an ordinal attribute.
If too many intervals are used, the limited number of training set event per interval makes the prediction unreliable. </p>
<p>We <strong>associate</strong> the attributes with a density function and estimate the parameters of the function from the training set to estimate P(A|C).</p></section><section class="print-page" id="data-mining-classifiers-multiclassifier"><h1 id="data-mining-classifiers-multiclassifier-multi-classifier">Multi-Classifier</h1>
<p>Construct multiple base classifiers and predict the class to which a record belongs by aggregating the classification obtained.</p>
<p><strong>How to build a composite classifier</strong></p>
<ul>
<li>Changing the training set by building more training set from the given one</li>
<li>Change the attributes (random forest)</li>
<li>Changing the classes considered (translate a multi-class classification into a binary one)</li>
<li>Change the parameters of the learning algorithm</li>
</ul>
<h2 id="data-mining-classifiers-multiclassifier-error-decomposition">Error Decomposition</h2>
<p>Classifiers make mistakes in predictions, due to:</p>
<ul>
<li><strong>Bias</strong>: ability of the chosen classifier in modeling events and extending the prediction to events not in the training set</li>
<li><strong>Variance</strong>: capability of the training set in representing the actual data set</li>
<li><strong>Noise</strong>: non-determinism of the classes to be determined</li>
</ul>
<p>Different types of classifiers have inherently different capabilities in modeling the edges of regions. 
The difference between the true separation line and the average separation line represents the classifier bias.</p>
<h2 id="data-mining-classifiers-multiclassifier-bagging">Bagging</h2>
<p>Bagging allows the construction of compound classifiers that associate an event with the highest rated class from the base classifiers.</p>
<p>Each classifier is constructed by <strong>bootstrapping</strong> the same training set.</p>
<p><em>bootstrapping: any test or metric that uses random sampling with replacement</em></p>
<p><img alt="" src="data-mining/classifiers/bagging.jpg" /></p>
<p>Bagging determines the behavior of a two-level decision tree.</p>
<p><img alt="" src="data-mining/classifiers/tree.jpg" /></p>
<h2 id="data-mining-classifiers-multiclassifier-random-forest">Random Forest</h2>
<p>It is a bagging method in which base classifiers are decision trees:
For each node in the decision tree, the split attribute is chosen on a random subset of features rather than on the entire set of features.</p>
<p>Random forest performs two types of bagging: one on the training set and one on the feature set.</p>
<h2 id="data-mining-classifiers-multiclassifier-boosting">Boosting</h2>
<p>An iterative approach to progressively adjust the composition of the training set in order to focus on incorrectly classified records.</p>
<ul>
<li>Initially, all N records have the same weight (1/N)</li>
<li>Unlike bagging, the weights can change at the end of the boosting round in order to increase the probability of the record being selected in the training set</li>
</ul>
<p>The final result is obtained by combining the result the predictions made by the different classifiers.</p>
<p>One of the most widely used boosting techniques is <strong>AdaBoost:</strong></p>
<p><img alt="" src="data-mining/classifiers/ada.jpg" />
<img alt="" src="data-mining/classifiers/boost.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Classifier Models</h1>
                        <h1 class='nav-section-title' id='section-machine-learning'>
                            Machine Learning <a class='headerlink' href='#section-machine-learning' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="machine-learning-introduction-introduction"><h1 id="machine-learning-introduction-introduction-introduction-to-machine-learning">Introduction to Machine Learning</h1>
<p>What are the main features of intelligence?</p>
<p><em>Intelligence is a very general mental capability that, among the other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.</em></p>
<p><strong>Artificial Intelligence</strong> is a huge set of disciplines which also includes machine learning.
With machine learning, we refer only to a <a href="https://medium.com/@terdsaksu/artificial-intelligence-machine-learning-deep-learning-a2ebd43ff1b2">small subset</a> inside artificial intelligence.</p>
<p><img alt="" src="machine-learning/introduction/ai.jpg" /></p>
<p>We, as humans, take many activities for granted that for machines would be very complex. 
Simulating human intelligence is extremely complex as our brain in an incredibly sophisticated machine, of which we still know few aspects.</p>
<h2 id="machine-learning-introduction-introduction-impact-of-ai-in-our-world">Impact of AI in our world</h2>
<p><img alt="" src="machine-learning/introduction/tractica.jpg" /></p>
<p>For both graphs, we can not the exponential trend and the variety of continents covered. </p>
<p><em>Information is the oil of 21st century, and analytics is the combustion engine</em></p>
<p>The number of requests per position as data scientist is constantly increasing as companies need to extract knowledge from data to survive.</p>
<p>The revolution introduced by AI is reflected also in companies. At least 5 of the top 10 world companies are directly related to AI.
Also, at least 2 companies are directly related to the production of chips, key elements for AI.</p>
<p><strong>Nvidia</strong></p>
<p>It is a software and fabless company that design GPUs, which nowadays are essential to:</p>
<ul>
<li>Create AI models</li>
<li>Perform High Performance Computing (HPC)</li>
</ul>
<p>Nvidia is the leading company in the sector and this is the reason why its shares has risen significantly in recent years.</p>
<h2 id="machine-learning-introduction-introduction-the-general-paradigm-of-machine-learning">The General Paradigm of Machine Learning</h2>
<p><strong>Machine Learning</strong> is a subset of the AI field that tries to develop systems able to automatically learn from specific examples (<em>training data</em>) amd to generalize the knowledge on new samples (<em>testing data</em>) of the same domain.</p>
<p>From a practical point of view:</p>
<ol>
<li>We have some data which represents our application domain</li>
<li>We implement an algorithm able to learn from the data (training phase)</li>
<li>We use data to understand if the trained model has learned something -&gt; <strong>model deployment</strong></li>
</ol>
<p>The main steps for the development of intelligent systems based on ML:</p>
<p><img alt="" src="machine-learning/introduction/stps.jpg" /></p>
<p><strong>Data Acquisition</strong></p>
<p>Data is the founding element of any application related to ML. Acquiring large amounts of data is one of the main concerns for top-companies today.</p>
<p><strong>Data Processing</strong></p>
<p>All those techniques with which data are processed in order to adapt them to the best of the ML model that we plan to develop.</p>
<p><strong>Model</strong></p>
<p>This is the main core of AI systems. A model can be seen as a set of mathematical and statistical technique able to learn from a certain distribution of data provided in input and to generalize on new data.</p>
<p><strong>Prediction</strong></p>
<p>It can take many forms depending on the application developed. It is the output of the model and it is important to evaluate the effectiveness of the developed system.</p></section><section class="print-page" id="machine-learning-introduction-history"><h1 id="machine-learning-introduction-history-historical-evolution-of-ai">Historical Evolution of AI</h1>
<p>To understand why AI is so important today, we have to analyze the past.</p>
<p>In <strong>1950</strong> the enthusiasm for AI began:</p>
<ul>
<li><strong>Turing Test</strong>: <em>"Can machines think?"</em></li>
<li>1954: one of the main experiments in machine translation</li>
<li>1955: Arthur Samuel wrote a program that could play checkers very well</li>
<li>1957: Rosenblatt invented perceptrons, a type of neural network</li>
</ul>
<p><strong>First AI Winter</strong>  - promises of AI were exaggerated</p>
<p>In <strong>1980</strong> the Boom times occurred:</p>
<ul>
<li>Commercialization of new AI Expert Systems capable of reproducing human-decision making, through <em>"if-then-else"</em> rules</li>
<li>Financial planning, medical diagnosis, geological exploration, and microelectronic circuit design</li>
</ul>
<p><strong>Second AI Winter</strong> - many tasks were too complicated for engineers</p>
<p>In <strong>2012</strong> the Deep Learning revolution took place</p>
<ul>
<li>Solved mathematical problems</li>
<li>New powerful Neural Networks</li>
<li>Huge improvement with the computational power</li>
<li>Introduction of GPUs</li>
</ul>
<p>Problem with <strong>data</strong></p>
<ul>
<li>AI models need huge amount of training data</li>
<li>Currently, we are able to:<ul>
<li>Acquire a lot of data (IoT)</li>
<li>Store huge amount of data (improved storage)</li>
</ul>
</li>
</ul>
<p>Today, the question is not if we are able to collect data, but if we are able to use them.</p>
<p><img alt="" src="machine-learning/introduction/data.jpg" /></p>
<p><img alt="" src="machine-learning/introduction/future.jpg" /></p></section><h1 class='nav-section-title-end'>Ended: Machine Learning</h1>
                        <h1 class='nav-section-title' id='section-data-acquisition-and-processing'>
                            Data Acquisition and Processing <a class='headerlink' href='#section-data-acquisition-and-processing' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="machine-learning-data-acquisition-data-acquisition"><h1 id="machine-learning-data-acquisition-data-acquisition-data-acquisition-and-processing">Data Acquisition and Processing</h1>
<p>Data acquisition and processing are the first steps in the Machine Learning pipeline.
This is one of the most important steps for many companies, but acquiring data is a time-consuming, investment and knowledge intensive process.</p>
<p><strong>Big Data</strong>: having large amounts of data available has been one of the reasons for the strong development of machine learning.
We are able to collect large amounts of data thanks to new storing devices and process digitalization.</p>
<h2 id="machine-learning-data-acquisition-data-acquisition-data-acquisition">Data Acquisition</h2>
<p>Data acquisition is the first step in developing a machine learning system. 
We can get data mainly in two ways:</p>
<ol>
<li>By using <strong>publicly available</strong> data (quality must be checked)</li>
<li>By acquiring a new set of <strong>specific</strong> data (generate specific expertise for the company)<ul>
<li>It is not certain that public data well represent the problem we want to solve</li>
<li>We are forced to acquire data that, due to their sensitive nature, would not otherwise available (privacy issues)</li>
<li>The company we work for already has a data collection process that we can use</li>
</ul>
</li>
</ol>
<p><strong>Public Datasets</strong></p>
<p>Many Universities publicly release their datasets. There are no requirements related to profit or non-disclosure agreement.
It is a consolidated practice in the world of research to share data to test the reproducibility of the results obtained.</p>
<p><strong>Acquisition of a new dataset</strong></p>
<p>Acquiring a new dataset is usually a costly process (time and money).</p>
<ul>
<li>Program acquisition tool</li>
<li>Handle large amounts of data</li>
<li>Test to find bugs</li>
<li>New hardware</li>
</ul>
<p>It is necessary to carefully consider whether it is appropriate to acquire a new dataset.</p>
<p><em>Acquiring a new dataset does not mean acquiring only new data</em></p>
<h2 id="machine-learning-data-acquisition-data-acquisition-data-annotation">Data Annotation</h2>
<p>It is one of the most relevant aspect in the data acquisition phase.
It regards the <strong>semantic content</strong> of the data and the label depends on the problem we want to solve.</p>
<p>It can be numerical or categorical and associates a label to data.</p>
<p>Data collection without correct and timely annotation is often useless.
However, it also possible to extract knowledge from un-annotated data through <strong>clustering.</strong></p>
<p><strong>Data Annotation Process</strong></p>
<p>The data annotation process can take place in several ways:</p>
<ul>
<li><strong>Manual</strong> (long and expensive but the quality of the annotations is usually controllable and high)</li>
<li><strong>Automatic</strong> (each data is automatically annotated using specific tools)</li>
<li><strong>Third parties</strong> (all data is noted by a third party)<ul>
<li>Free of charge (free use of a platform in exchange for annotated data)</li>
<li>Paid (purchase annotation time from third parties, usually from developing countries)</li>
</ul>
</li>
</ul>
<p><strong>Closed Set</strong>: the pattern to be classified belongs to one of the known classes</p>
<p><strong>Open Set</strong>: you do not know all the possible annotations, so the pattern to be classified can belong to one of the known classes or to non of these.
You can define a threshold above which a specific pattern is assigned.</p>
<h2 id="machine-learning-data-acquisition-data-acquisition-problems-in-data-acquisition">Problems in Data Acquisition</h2>
<p>Companies usually face common problems:</p>
<ul>
<li>The business process produces huge amounts of data (it is impossible to acquire all the data due to physical limitation)</li>
<li>Sometimes companies have a lot of old data in their databases</li>
<li>In many business processes it is unclear understanding which data is possible to collect or which data is really useful for the business</li>
</ul></section><section class="print-page" id="machine-learning-data-acquisition-data-types"><h1 id="machine-learning-data-acquisition-data-types-data-types">Data Types</h1>
<p>In general, there are 4 types of data:</p>
<ol>
<li><strong>Numerical:</strong><ul>
<li>Values associated with measurable characteristics</li>
<li>Continuous (subject to ordering)</li>
<li>Representable as numerical vectors</li>
</ul>
</li>
<li><strong>Categorical:</strong><ul>
<li>Qualitative characteristics</li>
<li>Presence or absence of a characteristic</li>
<li>Sometimes subject to sorting</li>
<li>Widely used in Data Mining</li>
</ul>
</li>
<li><strong>Sequences:</strong><ul>
<li>Sequential patterns with spatial or temporal relationships</li>
<li>With a variable length</li>
<li>Position in the sequence and relationship with predecessors and successors are important</li>
</ul>
</li>
<li><strong>Structured data:</strong><ul>
<li>Outputs organized in complex structures such as trees and graphs</li>
</ul>
</li>
</ol>
<h2 id="machine-learning-data-acquisition-data-types-images">Images</h2>
<p>An image is a <strong>matrix of values</strong> in which each cell is referred as pixel and each pixel contains the value of the brightness.</p>
<p><img alt="" src="machine-learning/data-acquisition/pixel.jpg" /></p>
<p>In <strong>color images</strong>, each pixel contains 3 values that represent the color components, referred as channels.
The content of channels are related to the color space (the convention used to define colors).</p>
<ul>
<li><strong>RGB color space</strong>: 3 values indicate the value of three components (Red, Gree and Blue).</li>
</ul>
<h2 id="machine-learning-data-acquisition-data-types-image-formats">Image formats</h2>
<p><img alt="" src="machine-learning/data-acquisition/formats.jpg" /></p></section><section class="print-page" id="machine-learning-data-acquisition-data-preparation"><h1 id="machine-learning-data-acquisition-data-preparation-data-preparation">Data Preparation</h1>
<p>Once obtained data for our Machine Learning system, it is necessary to prepare them.</p>
<p>They are organized as follows:</p>
<ul>
<li><strong>Training set</strong> (data on which the model automatically learns during the learning phase)</li>
<li><strong>Validation set</strong> (part of the training set in which hyper-parameters are tuned)</li>
<li><strong>Training set</strong> (data on which the model is tested during testing phase in order to model effectiveness through qualitative and quantitative numerical measures)</li>
</ul>
<p><img alt="" src="machine-learning/data-acquisition/data-prep.jpg" /></p>
<h2 id="machine-learning-data-acquisition-data-preparation-deployment">Deployment</h2>
<p>Once the previous phases have been completed, the ML system can be released for its effective use.</p>
<p>Normally, once the model is released, it no longer goes through training and testing phases.</p>
<p><strong>Different ways to train-val-test</strong></p>
<p>We can identify alternatives approaches adopted by choice or imposed by context:</p>
<ol>
<li><strong>Batch</strong>: the training is carried out only once on a given training set.</li>
<li><strong>Incremental</strong>: following the initial training, further training sessions are possible</li>
<li><strong>Natural</strong>: this is the closest case ti the human learning process</li>
</ol>
<h2 id="machine-learning-data-acquisition-data-preparation-different-ways-of-learning">Different Ways of Learning</h2>
<p>Not all data is always annotated. Depending on whether they are annotated, we can define different types of learning:</p>
<ul>
<li>Annotated data -&gt; <strong>Supervised Learning</strong></li>
<li>Not annotated data -&gt; <strong>Unsupervised Learning</strong></li>
<li>Partially annotated data -&gt; <strong>Semi-supervised Learning</strong></li>
</ul>
<p>Specific algorithms correspond to each of these areas. Usually, the presence of annotations helps and simplifies the development of ML algorithms. The best performances are usually obtained with supervised trained algorithms.</p></section><h1 class='nav-section-title-end'>Ended: Data Acquisition and Processing</h1>
                        <h1 class='nav-section-title' id='section-model'>
                            Model <a class='headerlink' href='#section-model' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="machine-learning-model-model"><h1 id="machine-learning-model-model-model">Model</h1>
<p>The model is the heart of the AI in our system.
It is one of the most delicate and decisive elements of the entire process:</p>
<ul>
<li><strong>Model</strong> -&gt; mechanism with which input data are transformed in outputs</li>
</ul>
<h2 id="machine-learning-model-model-machine-learning-tasks">Machine Learning Tasks</h2>
<p>There are different tasks in ML depending on the output we want:</p>
<ul>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
</ul>
<h2 id="machine-learning-model-model-classification">Classification</h2>
<div class="highlight"><pre><span></span><code>- We have a specific input, a model (classifier) which outputs a class (pattern)
- If there are only 2 classes, we call the problem *binary* classification, while with multiple classes, we have *multi-class* classification
</code></pre></div>
<p><strong>class</strong> = data set having common properties</p>
<p>The concept of <em>label</em> and <em>semantic</em> is related to the concept of class, since it strictly depends on the working context.</p>
<p><strong>Examples of classification:</strong></p>
<ol>
<li>Spam detection<ul>
<li>Input: email texts</li>
<li>Output: yes/no (spam)</li>
</ul>
</li>
<li>Credit card fraud detection<ul>
<li>Input: list of bank operations</li>
<li>Output: yes/no (fraud)</li>
</ul>
</li>
<li>Face recognition<ul>
<li>Input: images</li>
<li>Output: identity</li>
</ul>
</li>
<li>Medical diagnosis<ul>
<li>Input: x-ray images</li>
<li>Output: benign/malignant (tumor)</li>
</ul>
</li>
</ol>
<p><img alt="" src="machine-learning/model/class.jpg" /></p>
<h2 id="machine-learning-model-model-regression">Regression</h2>
<p>Given a specific input, the model (regressor) outputs a continuous value (data -&gt; value).
You can see a regression task as a classification task with a high number of classes</p>
<p><strong>Examples of regression</strong></p>
<ol>
<li>Estimation of a person's height based on weight</li>
<li>Estimated sale prices of apartments in the real estate market</li>
<li>Risk estimation for insurance companies</li>
<li>Energy prediction produced by a photovoltaic system</li>
<li>Health costs prediction models</li>
</ol>
<p><img alt="" src="machine-learning/model/reg.jpg" /></p>
<h2 id="machine-learning-model-model-clustering">Clustering</h2>
<p>Identify groups (clusters) of data with similar characteristics, usually applied in an <strong>unsupervised</strong> learning setting (patterns are not labeled and classes are not known in advance).</p>
<p>Usually, the unsupervised nature of the problem makes it more complex than classification.</p>
<p><strong>Examples of clustering</strong></p>
<ol>
<li>Marketing (user groups)</li>
<li>Genetics (group by DNA)</li>
<li>Bioinformatics (partitioning of genes)</li>
<li>Vision (unsupervised segmentation)</li>
</ol>
<p><img alt="" src="machine-learning/model/cluster.jpg" /></p>
<h2 id="machine-learning-model-model-artificial-vision">Artificial Vision</h2>
<p>For artificial vision domain, we can identify even more specific problems</p>
<p><img alt="" src="machine-learning/model/vision.jpg" /></p></section><section class="print-page" id="machine-learning-model-pattern"><h1 id="machine-learning-model-pattern-pattern-recognition">Pattern Recognition</h1>
<p>Pattern recognition is the discipline that studies the recognition of patterns (data) even with pre-programmed algorithms (not able to learn automatically)</p>
<p>The <strong>model</strong> is a set of <em>hand-crafted instructions</em>.</p>
<p>Technique similar to the <em>Expert System</em> developed in the '80, which was a first form of artificial intelligence.
The ability of a calculator to perform calculations on large amounts of data is exploited.</p>
<p>The programmer develops a series of <strong>instructions</strong> to solve specific problem:</p>
<ul>
<li>These instructions are typically based on <em>if-then-else</em> statements</li>
<li>A strong priori knowledge of the problem is required</li>
</ul>
<p><strong>Problems</strong> that can be faced with explicitly programmed instructions:</p>
<ul>
<li>The conditions are stable and known a priori (constrained industrial environment)</li>
<li>There are mathematical formulas to model the problem</li>
<li>The problem must be limited in dimensionality and not too complex</li>
</ul>
<p><img alt="" src="machine-learning/model/enomaly.jpg" /></p>
<h2 id="machine-learning-model-pattern-explicitly-programmed-instructions">Explicitly Programmed Instructions</h2>
<p>General and technical considerations about pattern recognition:</p>
<ul>
<li>It can achieve a high degree of success if the a priori knowledge is adequate, dimensionality of the problem is limited and the test domain is similar to what was assumed when defining the instructions</li>
<li>The developed solution will inevitably be <strong>specific</strong></li>
<li>There is not a real learning phase</li>
<li>It is possible to understand why the developed system fails in classification</li>
<li>If the problem becomes complicated, the programming time increase</li>
<li>The code risks becoming unmanageable due to:<ul>
<li>High complexity and number of innested statements</li>
<li>Length of code</li>
<li>Too specific functions</li>
</ul>
</li>
</ul>
<p><strong>Limits of programmed instructions</strong></p>
<p><img alt="" src="machine-learning/model/limit.jpg" /></p>
<p>Solving these problems with instructions is very complex. The level of generalization of the proposed solution would be very limited.</p>
<p>It is necessary, when needed, to address the problems with other paradigms.</p></section><h1 class='nav-section-title-end'>Ended: Model</h1>
                        <h1 class='nav-section-title' id='section-dm-lab'>
                            DM - LAB <a class='headerlink' href='#section-dm-lab' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="data-mining-weka-lab-weka-lab"><h1 id="data-mining-weka-lab-weka-lab-weka">Weka</h1>
<p>Weka is an open-source software for Data Mining and Machine Learning written in Java, distributed under the GNU public license.</p>
<p>It includes four applications:</p>
<ol>
<li><strong>Explorer</strong> - we will use explorer</li>
<li><strong>Experimenter</strong></li>
<li><strong>Knowledge Flow</strong></li>
<li><strong>SimpleCLI</strong></li>
</ol>
<p>The main file format used in Weka  is <strong>ARFF</strong> (attribute-relation file format), which is a comma-separated value format.</p>
<p>Weka files store relations, attributes and values.</p>
<div class="highlight"><pre><span></span><code><span class="nd">@attribute</span><span class="w"> </span><span class="n">age</span><span class="w"> </span><span class="n">numeric</span><span class="w"></span>
<span class="nd">@attribute</span><span class="w"> </span><span class="n">sex</span><span class="w"> </span><span class="p">{</span><span class="n">female</span><span class="p">,</span><span class="w"> </span><span class="n">male</span><span class="p">}</span><span class="w"></span>
<span class="nd">@attribute</span><span class="w"> </span><span class="n">cholesterol</span><span class="w"> </span><span class="n">numeric</span><span class="w"></span>

<span class="nd">@data</span><span class="w"></span>
<span class="mi">63</span><span class="p">,</span><span class="w"> </span><span class="n">male</span><span class="p">,</span><span class="mi">233</span><span class="w"></span>
<span class="mi">67</span><span class="p">,</span><span class="n">male</span><span class="p">,</span><span class="mi">286</span><span class="w"></span>
</code></pre></div></section><section class="print-page" id="data-mining-weka-lab-bank-data"><h1 id="data-mining-weka-lab-bank-data-bank-data">Bank Data</h1>
<p>Attributes:</p>
<p><img alt="" src="data-mining/weka-lab/bank.jpg" /></p>
<p><strong>PEP class</strong> (Personal Equity Plan)</p>
<h2 id="data-mining-weka-lab-bank-data-pre-processing-bank-data">Pre-Processing Bank Data</h2>
<ol>
<li>Load the files and save it in an ARFF format </li>
<li>Carry out a visual analysis of the dataset</li>
<li>Drop the ID attribute </li>
</ol>
<p>Which attribute is more relevant for our analysis?</p>
<ul>
<li>Sex (no relevant difference)</li>
<li>Age (not as relevant as income, but there is a trend)</li>
<li>Married (relevant)</li>
<li>
<p>Children (linear correlation, the first column does not respect the trend)</p>
</li>
<li>
<p>Income (normal distribution)</p>
<ul>
<li>The higher the income, the higher the probability to buy PEP</li>
</ul>
<p><img alt="" src="data-mining/weka-lab/income.jpg" /></p>
</li>
</ul>
<h2 id="data-mining-weka-lab-bank-data-visualize-the-plot-matrix">Visualize the Plot Matrix</h2>
<p><img alt="" src="data-mining/weka-lab/children.jpg" /></p>
<p>The higher the number of children, the lower the income as children cost a lot of money.</p>
<p>People without children are not interested in PEP as they do not need to think about the future. </p>
<h2 id="data-mining-weka-lab-bank-data-classify">Classify</h2>
<p>Use the following algorithms and evaluate the result:</p>
<ul>
<li>J48</li>
<li>J48 (without post-pruning)</li>
<li>Jrip</li>
<li>IBk (with k=1 and k=5)</li>
</ul>
<p><img alt="" src="data-mining/weka-lab/j48.jpg" /></p>
<p>The main variables are <strong>children</strong> and <strong>income</strong> (closest to the root).</p>
<p>KNN with k = 1 using the training set has an accuracy of 100% because the closest point to me is me (if given), so we drop it.</p>
<p>With IBk we are using a <em>distance function</em> so numbers should be discretized. </p>
<p>Also, irrelevant and replicated attributes can create distortion in the result. 
- In this case, irrelevant attributes are most likely (sex, car)</p>
<p><img alt="" src="data-mining/weka-lab/knn.jpg" /></p>
<p>We can drop the irrelevant attributes to increase accuracy.</p></section><section class="print-page" id="data-mining-weka-lab-census-data"><h1 id="data-mining-weka-lab-census-data-census-data">Census Data</h1>
<p>Identify relevant attributes:</p>
<ul>
<li>Capital gain/loss are not relevant because many attributes are zero</li>
<li>Age (we expect it to be linear but the majority of &gt;50k is in the range of 40 years)</li>
<li>Work class is unbalanced (the majority of people works in private), coverage eis very low</li>
<li>Education number </li>
<li>Marital status (interesting, married is different)</li>
<li>Occupation (interesting)</li>
</ul>
<p>Education and education-num are perfectly correlated (it is duplicated).</p></section><h1 class='nav-section-title-end'>Ended: DM - LAB</h1>
                        <h1 class='nav-section-title' id='section-ml-lab'>
                            ML - LAB <a class='headerlink' href='#section-ml-lab' title='Permanent link'>↵</a>
                        </h1>
                        <section class="print-page" id="machine-learning-lab-lab-notes"><h1 id="machine-learning-lab-lab-notes-laboratory">Laboratory</h1>
<ul>
<li>Programing done in Python</li>
<li>We will use <strong>Colab</strong> as a programming tool</li>
<li>We can use any IDE (Visual Studio, PyCharm)</li>
</ul>
<h2 id="machine-learning-lab-lab-notes-guide">Guide</h2>
<ul>
<li>Do not execute code without understanding it</li>
<li>It is necessary to <em>play</em> a lot with the code to become familiar</li>
<li><strong>Bugs</strong> can be very informative</li>
</ul>
<h2 id="machine-learning-lab-lab-notes-colab">Colab</h2>
<ul>
<li>Free Google service</li>
<li>Easy to use, the environment provided already has most of the resources installed</li>
<li>The code runs in the browser (VM)</li>
<li>Each assigned virtual machine has a variable hardware equipment<ul>
<li>CPU and RAM available</li>
<li>GPU resources</li>
</ul>
</li>
</ul></section><h1 class='nav-section-title-end'>Ended: ML - LAB</h1></div>




              

  

            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
<script id="__config" type="application/json">{"base": "/", "features": [], "search": "assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    

<script src="https://unpkg.com/iframe-worker/polyfill"></script>
<script src="search/search_index.js"></script>


    
      <script src="assets/javascripts/bundle.9c69f0bc.min.js"></script>
      
        <script src="js/print-site.js"></script>
      
        <script src="js/arithmatex.config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>